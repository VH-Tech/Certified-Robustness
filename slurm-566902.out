==========================================
SLURM_CLUSTER_NAME = paramhimalaya
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = iitmandi
SLURM_JOB_ID = 566902
SLURM_JOB_NAME = Certified-Robustness.job
SLURM_JOB_NODELIST = gpu008
SLURM_JOB_USER = ravihm.scee.iitmandi
SLURM_JOB_UID = 46161
SLURM_JOB_PARTITION = gpu
SLURM_TASK_PID = 18877
SLURM_SUBMIT_DIR = /home/ravihm.scee.iitmandi/Certified-Robustness
SLURM_CPUS_ON_NODE = 1
SLURM_NTASKS = 
SLURM_TASK_PID = 18877
==========================================
/var/share/slurm/d/job566902/slurm_script: line 11: activate: No such file or directory
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190036-bii6ceue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-waterfall-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/bii6ceue
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190036-k08qt36g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-moon-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/k08qt36g
Training 0.04501772644334634% of the parameters
starting training
Training 0.04501772644334634% of the parameters
starting training
Epoch: [0][0/196]	Time 10.048 (10.048)	Data 1.250 (1.250)	Loss 2.3292 (2.3292)	Acc@1 6.250 (6.250)Epoch: [0][0/196]	Time 9.948 (9.948)	Data 1.153 (1.153)	Loss 2.3360 (2.3360)	Acc@1 9.375 (9.375)

Epoch: [0][10/196]	Time 0.550 (1.401)	Data 0.217 (0.295)	Loss 2.3521 (2.3231)	Acc@1 6.250 (10.511)
Epoch: [0][10/196]	Time 0.558 (1.410)	Data 0.224 (0.305)	Loss 2.2580 (2.3275)	Acc@1 17.188 (8.949)
Epoch: [0][20/196]	Time 0.554 (0.997)	Data 0.221 (0.259)	Loss 2.3672 (2.3484)	Acc@1 9.375 (10.640)
Epoch: [0][20/196]	Time 0.568 (1.002)	Data 0.230 (0.265)	Loss 2.3818 (2.3552)	Acc@1 4.688 (9.077)
Epoch: [0][30/196]	Time 0.553 (0.857)	Data 0.220 (0.251)	Loss 2.3144 (2.3582)	Acc@1 14.062 (8.871)
Epoch: [0][30/196]	Time 0.563 (0.854)	Data 0.226 (0.247)	Loss 2.3642 (2.3506)	Acc@1 14.062 (10.837)
Epoch: [0][40/196]	Time 0.548 (0.783)	Data 0.214 (0.243)	Loss 2.4212 (2.3555)	Acc@1 4.688 (8.803)
Epoch: [0][40/196]	Time 0.552 (0.781)	Data 0.217 (0.240)	Loss 2.3495 (2.3469)	Acc@1 6.250 (10.671)
Epoch: [0][50/196]	Time 0.556 (0.738)	Data 0.225 (0.239)	Loss 2.3584 (2.3559)	Acc@1 15.625 (8.977)
Epoch: [0][50/196]	Time 0.560 (0.736)	Data 0.227 (0.236)	Loss 2.3573 (2.3451)	Acc@1 15.625 (10.539)
Epoch: [0][60/196]	Time 0.549 (0.707)	Data 0.216 (0.235)	Loss 2.3004 (2.3589)	Acc@1 17.188 (9.298)
Epoch: [0][60/196]	Time 0.551 (0.705)	Data 0.217 (0.233)	Loss 2.3502 (2.3461)	Acc@1 10.938 (10.835)
Epoch: [0][70/196]	Time 0.551 (0.685)	Data 0.218 (0.233)	Loss 2.2918 (2.3518)	Acc@1 9.375 (9.991)
Epoch: [0][70/196]	Time 0.549 (0.684)	Data 0.216 (0.231)	Loss 2.3953 (2.3474)	Acc@1 10.938 (10.849)
Epoch: [0][80/196]	Time 0.546 (0.669)	Data 0.214 (0.232)	Loss 2.3127 (2.3480)	Acc@1 15.625 (10.397)
Epoch: [0][80/196]	Time 0.561 (0.668)	Data 0.227 (0.230)	Loss 2.2729 (2.3433)	Acc@1 17.188 (11.015)
Epoch: [0][90/196]	Time 0.549 (0.656)	Data 0.217 (0.230)	Loss 2.3801 (2.3464)	Acc@1 4.688 (10.731)
Epoch: [0][90/196]	Time 0.552 (0.655)	Data 0.217 (0.228)	Loss 2.3457 (2.3401)	Acc@1 14.062 (11.367)
Epoch: [0][100/196]	Time 0.544 (0.645)	Data 0.214 (0.229)	Loss 2.3823 (2.3444)	Acc@1 12.500 (10.922)
Epoch: [0][100/196]	Time 0.553 (0.644)	Data 0.219 (0.227)	Loss 2.3918 (2.3391)	Acc@1 14.062 (11.726)
Epoch: [0][110/196]	Time 0.550 (0.637)	Data 0.217 (0.228)	Loss 2.3640 (2.3411)	Acc@1 14.062 (11.205)
Epoch: [0][110/196]	Time 0.550 (0.636)	Data 0.218 (0.226)	Loss 2.2923 (2.3362)	Acc@1 23.438 (11.923)
Epoch: [0][120/196]	Time 0.547 (0.630)	Data 0.215 (0.227)	Loss 2.2799 (2.3390)	Acc@1 20.312 (11.506)
Epoch: [0][120/196]	Time 0.549 (0.629)	Data 0.217 (0.226)	Loss 2.2746 (2.3359)	Acc@1 15.625 (12.035)
Epoch: [0][130/196]	Time 0.550 (0.623)	Data 0.219 (0.227)	Loss 2.3045 (2.3399)	Acc@1 18.750 (11.498)
Epoch: [0][130/196]	Time 0.549 (0.623)	Data 0.216 (0.225)	Loss 2.2928 (2.3345)	Acc@1 14.062 (12.047)
Epoch: [0][140/196]	Time 0.551 (0.618)	Data 0.217 (0.226)	Loss 2.3487 (2.3380)	Acc@1 18.750 (11.658)
Epoch: [0][140/196]	Time 0.550 (0.618)	Data 0.217 (0.224)	Loss 2.2493 (2.3354)	Acc@1 21.875 (12.023)
Epoch: [0][150/196]	Time 0.554 (0.614)	Data 0.221 (0.226)	Loss 2.3843 (2.3375)	Acc@1 17.188 (11.807)
Epoch: [0][150/196]	Time 0.550 (0.613)	Data 0.218 (0.224)	Loss 2.3969 (2.3368)	Acc@1 7.812 (11.889)
Epoch: [0][160/196]	Time 0.557 (0.610)	Data 0.223 (0.225)	Loss 2.3153 (2.3339)	Acc@1 14.062 (11.957)
Epoch: [0][160/196]	Time 0.550 (0.609)	Data 0.217 (0.223)	Loss 2.3614 (2.3353)	Acc@1 14.062 (11.966)
Epoch: [0][170/196]	Time 0.548 (0.606)	Data 0.215 (0.225)	Loss 2.2681 (2.3299)	Acc@1 14.062 (12.098)
Epoch: [0][170/196]	Time 0.552 (0.606)	Data 0.217 (0.223)	Loss 2.2632 (2.3320)	Acc@1 17.188 (12.034)
Epoch: [0][180/196]	Time 0.552 (0.603)	Data 0.219 (0.224)	Loss 2.2528 (2.3271)	Acc@1 18.750 (12.293)
Epoch: [0][180/196]	Time 0.551 (0.603)	Data 0.217 (0.223)	Loss 2.3471 (2.3304)	Acc@1 9.375 (12.112)
Epoch: [0][190/196]	Time 0.547 (0.601)	Data 0.217 (0.224)	Loss 2.2320 (2.3247)	Acc@1 20.312 (12.492)
Epoch: [0][190/196]	Time 0.551 (0.600)	Data 0.218 (0.222)	Loss 2.3057 (2.3282)	Acc@1 14.062 (12.107)
Test: [0/157]	Time 0.680 (0.680)	Data 0.399 (0.399)	Loss 2.2487 (2.2487)	Acc@1 23.438 (23.438)
Test: [0/157]	Time 0.710 (0.710)	Data 0.428 (0.428)	Loss 2.2487 (2.2487)	Acc@1 23.438 (23.438)
Test: [10/157]	Time 0.273 (0.313)	Data 0.006 (0.044)	Loss 2.2994 (2.2688)	Acc@1 20.312 (16.619)
Test: [10/157]	Time 0.269 (0.312)	Data 0.005 (0.046)	Loss 2.2994 (2.2688)	Acc@1 20.312 (16.619)
Test: [20/157]	Time 0.271 (0.292)	Data 0.007 (0.027)	Loss 2.2691 (2.2673)	Acc@1 17.188 (16.667)
Test: [20/157]	Time 0.278 (0.295)	Data 0.010 (0.026)	Loss 2.2691 (2.2673)	Acc@1 17.188 (16.667)
Test: [30/157]	Time 0.274 (0.286)	Data 0.008 (0.021)	Loss 2.2639 (2.2671)	Acc@1 17.188 (17.288)
Test: [30/157]	Time 0.274 (0.288)	Data 0.006 (0.020)	Loss 2.2639 (2.2671)	Acc@1 17.188 (17.288)
Test: [40/157]	Time 0.273 (0.283)	Data 0.008 (0.018)	Loss 2.2457 (2.2677)	Acc@1 21.875 (17.073)
Test: [40/157]	Time 0.274 (0.285)	Data 0.008 (0.017)	Loss 2.2457 (2.2677)	Acc@1 21.875 (17.073)
Test: [50/157]	Time 0.271 (0.281)	Data 0.007 (0.016)	Loss 2.3140 (2.2709)	Acc@1 18.750 (17.065)
Test: [50/157]	Time 0.276 (0.283)	Data 0.007 (0.015)	Loss 2.3140 (2.2709)	Acc@1 18.750 (17.065)
Test: [60/157]	Time 0.271 (0.280)	Data 0.008 (0.015)	Loss 2.3042 (2.2698)	Acc@1 12.500 (17.444)
Test: [60/157]	Time 0.273 (0.282)	Data 0.008 (0.014)	Loss 2.3042 (2.2698)	Acc@1 12.500 (17.444)
Test: [70/157]	Time 0.277 (0.279)	Data 0.009 (0.014)	Loss 2.2139 (2.2690)	Acc@1 18.750 (17.496)
Test: [70/157]	Time 0.276 (0.281)	Data 0.008 (0.013)	Loss 2.2139 (2.2690)	Acc@1 18.750 (17.496)
Test: [80/157]	Time 0.274 (0.278)	Data 0.010 (0.013)	Loss 2.2148 (2.2715)	Acc@1 18.750 (17.573)
Test: [80/157]	Time 0.275 (0.280)	Data 0.010 (0.013)	Loss 2.2148 (2.2715)	Acc@1 18.750 (17.573)
Test: [90/157]	Time 0.271 (0.277)	Data 0.008 (0.012)	Loss 2.3454 (2.2742)	Acc@1 7.812 (17.617)
Test: [90/157]	Time 0.276 (0.280)	Data 0.007 (0.012)	Loss 2.3454 (2.2742)	Acc@1 7.812 (17.617)
Test: [100/157]	Time 0.271 (0.277)	Data 0.007 (0.012)	Loss 2.2424 (2.2753)	Acc@1 17.188 (17.791)
Test: [100/157]	Time 0.276 (0.279)	Data 0.007 (0.012)	Loss 2.2424 (2.2753)	Acc@1 17.188 (17.791)
Test: [110/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.2081 (2.2747)	Acc@1 20.312 (17.553)
Test: [110/157]	Time 0.277 (0.279)	Data 0.009 (0.011)	Loss 2.2081 (2.2747)	Acc@1 20.312 (17.553)
Test: [120/157]	Time 0.270 (0.276)	Data 0.007 (0.011)	Loss 2.2137 (2.2749)	Acc@1 25.000 (17.497)
Test: [120/157]	Time 0.275 (0.279)	Data 0.008 (0.011)	Loss 2.2137 (2.2749)	Acc@1 25.000 (17.497)
Test: [130/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.2787 (2.2757)	Acc@1 21.875 (17.474)
Test: [130/157]	Time 0.276 (0.278)	Data 0.007 (0.011)	Loss 2.2787 (2.2757)	Acc@1 21.875 (17.474)
Test: [140/157]	Time 0.272 (0.275)	Data 0.008 (0.011)	Loss 2.2608 (2.2739)	Acc@1 14.062 (17.498)
Test: [140/157]	Time 0.273 (0.278)	Data 0.007 (0.010)	Loss 2.2608 (2.2739)	Acc@1 14.062 (17.498)
Test: [150/157]	Time 0.275 (0.275)	Data 0.008 (0.010)	Loss 2.3086 (2.2735)	Acc@1 18.750 (17.643)
Test: [150/157]	Time 0.274 (0.278)	Data 0.009 (0.010)	Loss 2.3086 (2.2735)	Acc@1 18.750 (17.643)
New Best Found: 17.67%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 17.67%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run earnest-waterfall-1 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/bii6ceue
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190036-bii6ceue/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run deft-moon-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/k08qt36g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190036-k08qt36g/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18977 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18976) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:03:50
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190419-47ucqxbu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sky-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/47ucqxbu
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190418-4cyjx8o3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-star-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/4cyjx8o3
Training 0.04501772644334634% of the parameters
starting training
Training 0.04501772644334634% of the parameters
starting training
Epoch: [0][0/196]	Time 2.245 (2.245)	Data 0.550 (0.550)	Loss 2.3292 (2.3292)	Acc@1 6.250 (6.250)
Epoch: [0][0/196]	Time 2.209 (2.209)	Data 0.507 (0.507)	Loss 2.3360 (2.3360)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.547 (0.707)	Data 0.216 (0.250)	Loss 2.3024 (2.3137)	Acc@1 7.812 (8.807)
Epoch: [0][10/196]	Time 0.552 (0.703)	Data 0.216 (0.245)	Loss 2.3218 (2.3227)	Acc@1 10.938 (9.375)
Epoch: [0][20/196]	Time 0.548 (0.633)	Data 0.217 (0.235)	Loss 2.3252 (2.3121)	Acc@1 4.688 (9.449)
Epoch: [0][20/196]	Time 0.558 (0.631)	Data 0.225 (0.233)	Loss 2.3197 (2.3146)	Acc@1 9.375 (9.673)
Epoch: [0][30/196]	Time 0.552 (0.606)	Data 0.217 (0.229)	Loss 2.3032 (2.3100)	Acc@1 9.375 (10.081)
Epoch: [0][30/196]	Time 0.556 (0.608)	Data 0.222 (0.231)	Loss 2.3033 (2.3099)	Acc@1 9.375 (9.073)
Epoch: [0][40/196]	Time 0.551 (0.595)	Data 0.218 (0.229)	Loss 2.3106 (2.3083)	Acc@1 9.375 (8.689)
Epoch: [0][40/196]	Time 0.549 (0.593)	Data 0.217 (0.226)	Loss 2.3023 (2.3084)	Acc@1 14.062 (10.061)
Epoch: [0][50/196]	Time 0.547 (0.586)	Data 0.215 (0.226)	Loss 2.2979 (2.3075)	Acc@1 9.375 (9.252)
Epoch: [0][50/196]	Time 0.552 (0.585)	Data 0.216 (0.225)	Loss 2.3028 (2.3074)	Acc@1 7.812 (10.110)
Epoch: [0][60/196]	Time 0.549 (0.579)	Data 0.217 (0.223)	Loss 2.3060 (2.3070)	Acc@1 7.812 (9.810)
Epoch: [0][60/196]	Time 0.552 (0.580)	Data 0.220 (0.225)	Loss 2.3099 (2.3070)	Acc@1 3.125 (9.144)
Epoch: [0][70/196]	Time 0.549 (0.575)	Data 0.217 (0.222)	Loss 2.2982 (2.3064)	Acc@1 6.250 (9.881)Epoch: [0][70/196]	Time 0.553 (0.576)	Data 0.221 (0.224)	Loss 2.2980 (2.3065)	Acc@1 15.625 (9.155)

Epoch: [0][80/196]	Time 0.550 (0.572)	Data 0.216 (0.222)	Loss 2.3009 (2.3060)	Acc@1 7.812 (10.012)Epoch: [0][80/196]	Time 0.550 (0.573)	Data 0.216 (0.223)	Loss 2.3033 (2.3061)	Acc@1 7.812 (9.298)

Epoch: [0][90/196]	Time 0.553 (0.570)	Data 0.219 (0.223)	Loss 2.3008 (2.3057)	Acc@1 18.750 (9.615)
Epoch: [0][90/196]	Time 0.553 (0.570)	Data 0.217 (0.221)	Loss 2.3011 (2.3055)	Acc@1 7.812 (10.216)
Epoch: [0][100/196]	Time 0.553 (0.568)	Data 0.223 (0.222)	Loss 2.3059 (2.3052)	Acc@1 6.250 (9.870)
Epoch: [0][100/196]	Time 0.554 (0.568)	Data 0.218 (0.221)	Loss 2.2921 (2.3051)	Acc@1 20.312 (10.520)
Epoch: [0][110/196]	Time 0.553 (0.567)	Data 0.220 (0.222)	Loss 2.2984 (2.3049)	Acc@1 14.062 (9.966)
Epoch: [0][110/196]	Time 0.550 (0.566)	Data 0.216 (0.220)	Loss 2.3026 (2.3049)	Acc@1 7.812 (10.529)
Epoch: [0][120/196]	Time 0.549 (0.565)	Data 0.218 (0.222)	Loss 2.3009 (2.3046)	Acc@1 12.500 (10.111)
Epoch: [0][120/196]	Time 0.551 (0.565)	Data 0.219 (0.220)	Loss 2.3008 (2.3047)	Acc@1 6.250 (10.524)
Epoch: [0][130/196]	Time 0.551 (0.564)	Data 0.219 (0.221)	Loss 2.3113 (2.3043)	Acc@1 7.812 (10.198)
Epoch: [0][130/196]	Time 0.550 (0.564)	Data 0.216 (0.220)	Loss 2.3098 (2.3045)	Acc@1 7.812 (10.615)
Epoch: [0][140/196]	Time 0.551 (0.563)	Data 0.221 (0.221)	Loss 2.3037 (2.3042)	Acc@1 9.375 (10.262)
Epoch: [0][140/196]	Time 0.553 (0.563)	Data 0.217 (0.220)	Loss 2.2986 (2.3043)	Acc@1 10.938 (10.760)
Epoch: [0][150/196]	Time 0.550 (0.562)	Data 0.216 (0.221)	Loss 2.3074 (2.3040)	Acc@1 1.562 (10.306)
Epoch: [0][150/196]	Time 0.551 (0.562)	Data 0.216 (0.220)	Loss 2.3051 (2.3042)	Acc@1 12.500 (10.544)
Epoch: [0][160/196]	Time 0.548 (0.561)	Data 0.217 (0.221)	Loss 2.3016 (2.3039)	Acc@1 10.938 (10.326)
Epoch: [0][160/196]	Time 0.555 (0.561)	Data 0.223 (0.219)	Loss 2.3007 (2.3040)	Acc@1 7.812 (10.452)
Epoch: [0][170/196]	Time 0.549 (0.561)	Data 0.216 (0.219)	Loss 2.3044 (2.3038)	Acc@1 9.375 (10.444)
Epoch: [0][170/196]	Time 0.560 (0.561)	Data 0.227 (0.221)	Loss 2.3060 (2.3037)	Acc@1 12.500 (10.371)
Epoch: [0][180/196]	Time 0.549 (0.560)	Data 0.219 (0.220)	Loss 2.3024 (2.3037)	Acc@1 7.812 (10.290)
Epoch: [0][180/196]	Time 0.547 (0.560)	Data 0.215 (0.219)	Loss 2.3041 (2.3037)	Acc@1 12.500 (10.463)
Epoch: [0][190/196]	Time 0.548 (0.560)	Data 0.216 (0.220)	Loss 2.3010 (2.3035)	Acc@1 14.062 (10.447)
Epoch: [0][190/196]	Time 0.551 (0.559)	Data 0.215 (0.219)	Loss 2.3006 (2.3036)	Acc@1 12.500 (10.496)
Test: [0/157]	Time 0.680 (0.680)	Data 0.401 (0.401)	Loss 2.3009 (2.3009)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.695 (0.695)	Data 0.417 (0.417)	Loss 2.3009 (2.3009)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.271 (0.310)	Data 0.006 (0.044)	Loss 2.3024 (2.3014)	Acc@1 7.812 (10.511)
Test: [10/157]	Time 0.271 (0.312)	Data 0.005 (0.046)	Loss 2.3024 (2.3014)	Acc@1 7.812 (10.511)
Test: [20/157]	Time 0.270 (0.293)	Data 0.007 (0.027)	Loss 2.3018 (2.3017)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.272 (0.292)	Data 0.007 (0.026)	Loss 2.3018 (2.3017)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.274 (0.286)	Data 0.007 (0.021)	Loss 2.3043 (2.3016)	Acc@1 9.375 (9.980)
Test: [30/157]	Time 0.273 (0.286)	Data 0.007 (0.020)	Loss 2.3043 (2.3016)	Acc@1 9.375 (9.980)
Test: [40/157]	Time 0.272 (0.283)	Data 0.009 (0.018)	Loss 2.3013 (2.3017)	Acc@1 12.500 (10.175)
Test: [40/157]	Time 0.273 (0.283)	Data 0.009 (0.017)	Loss 2.3013 (2.3017)	Acc@1 12.500 (10.175)
Test: [50/157]	Time 0.272 (0.281)	Data 0.008 (0.016)	Loss 2.2954 (2.3014)	Acc@1 10.938 (10.294)
Test: [50/157]	Time 0.275 (0.281)	Data 0.009 (0.015)	Loss 2.2954 (2.3014)	Acc@1 10.938 (10.294)
Test: [60/157]	Time 0.271 (0.280)	Data 0.007 (0.014)	Loss 2.2989 (2.3014)	Acc@1 7.812 (10.143)
Test: [60/157]	Time 0.272 (0.280)	Data 0.007 (0.014)	Loss 2.2989 (2.3014)	Acc@1 7.812 (10.143)
Test: [70/157]	Time 0.273 (0.279)	Data 0.007 (0.013)	Loss 2.3001 (2.3014)	Acc@1 7.812 (10.145)
Test: [70/157]	Time 0.274 (0.279)	Data 0.007 (0.013)	Loss 2.3001 (2.3014)	Acc@1 7.812 (10.145)
Test: [80/157]	Time 0.273 (0.278)	Data 0.007 (0.013)	Loss 2.3007 (2.3013)	Acc@1 10.938 (9.992)
Test: [80/157]	Time 0.271 (0.278)	Data 0.007 (0.012)	Loss 2.3007 (2.3013)	Acc@1 10.938 (9.992)
Test: [90/157]	Time 0.274 (0.277)	Data 0.009 (0.012)	Loss 2.2951 (2.3011)	Acc@1 12.500 (9.993)
Test: [90/157]	Time 0.271 (0.277)	Data 0.006 (0.012)	Loss 2.2951 (2.3011)	Acc@1 12.500 (9.993)
Test: [100/157]	Time 0.270 (0.277)	Data 0.007 (0.012)	Loss 2.3006 (2.3010)	Acc@1 12.500 (10.087)
Test: [100/157]	Time 0.271 (0.277)	Data 0.007 (0.011)	Loss 2.3006 (2.3010)	Acc@1 12.500 (10.087)
Test: [110/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.3036 (2.3010)	Acc@1 9.375 (10.234)
Test: [110/157]	Time 0.271 (0.276)	Data 0.007 (0.011)	Loss 2.3036 (2.3010)	Acc@1 9.375 (10.234)
Test: [120/157]	Time 0.275 (0.276)	Data 0.009 (0.011)	Loss 2.2940 (2.3010)	Acc@1 18.750 (10.201)
Test: [120/157]	Time 0.276 (0.276)	Data 0.009 (0.011)	Loss 2.2940 (2.3010)	Acc@1 18.750 (10.201)
Test: [130/157]	Time 0.272 (0.276)	Data 0.007 (0.010)	Loss 2.3032 (2.3010)	Acc@1 3.125 (10.126)
Test: [130/157]	Time 0.273 (0.276)	Data 0.008 (0.011)	Loss 2.3032 (2.3010)	Acc@1 3.125 (10.126)
Test: [140/157]	Time 0.272 (0.276)	Data 0.008 (0.011)	Loss 2.3000 (2.3011)	Acc@1 15.625 (10.117)
Test: [140/157]	Time 0.271 (0.276)	Data 0.007 (0.010)	Loss 2.3000 (2.3011)	Acc@1 15.625 (10.117)
Test: [150/157]	Time 0.272 (0.275)	Data 0.007 (0.010)	Loss 2.3028 (2.3011)	Acc@1 9.375 (10.110)
Test: [150/157]	Time 0.271 (0.276)	Data 0.007 (0.010)	Loss 2.3028 (2.3011)	Acc@1 9.375 (10.110)
New Best Found: 10.19%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.19%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run distinctive-star-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/4cyjx8o3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190418-4cyjx8o3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run gentle-sky-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/47ucqxbu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190419-47ucqxbu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20762 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 20763) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:07:11
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 20763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190740-9rua0s0a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-firebrand-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/9rua0s0a
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_190740-t7z4e6xn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-firefly-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/t7z4e6xn
Training 0.04501772644334634% of the parameters
starting training
Training 0.04501772644334634% of the parameters
starting training
Epoch: [0][0/196]	Time 2.118 (2.118)	Data 0.433 (0.433)	Loss 2.3292 (2.3292)	Acc@1 6.250 (6.250)
Epoch: [0][0/196]	Time 2.253 (2.253)	Data 0.472 (0.472)	Loss 2.3360 (2.3360)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.548 (0.700)	Data 0.217 (0.247)	Loss 2.3024 (2.3137)	Acc@1 7.812 (8.807)
Epoch: [0][10/196]	Time 0.549 (0.707)	Data 0.216 (0.241)	Loss 2.3218 (2.3227)	Acc@1 10.938 (9.375)
Epoch: [0][20/196]	Time 0.549 (0.629)	Data 0.217 (0.234)	Loss 2.3252 (2.3121)	Acc@1 4.688 (9.449)
Epoch: [0][20/196]	Time 0.552 (0.633)	Data 0.218 (0.230)	Loss 2.3197 (2.3146)	Acc@1 9.375 (9.673)
Epoch: [0][30/196]	Time 0.552 (0.608)	Data 0.220 (0.228)	Loss 2.3032 (2.3100)	Acc@1 9.375 (10.081)
Epoch: [0][30/196]	Time 0.558 (0.606)	Data 0.224 (0.231)	Loss 2.3033 (2.3099)	Acc@1 9.375 (9.073)
Epoch: [0][40/196]	Time 0.554 (0.593)	Data 0.220 (0.228)	Loss 2.3106 (2.3083)	Acc@1 9.375 (8.689)
Epoch: [0][40/196]	Time 0.551 (0.594)	Data 0.217 (0.226)	Loss 2.3023 (2.3084)	Acc@1 14.062 (10.061)
Epoch: [0][50/196]	Time 0.554 (0.585)	Data 0.222 (0.227)	Loss 2.2979 (2.3075)	Acc@1 9.375 (9.252)
Epoch: [0][50/196]	Time 0.553 (0.586)	Data 0.220 (0.225)	Loss 2.3028 (2.3074)	Acc@1 7.812 (10.110)
Epoch: [0][60/196]	Time 0.553 (0.579)	Data 0.221 (0.226)	Loss 2.3099 (2.3070)	Acc@1 3.125 (9.144)
Epoch: [0][60/196]	Time 0.554 (0.581)	Data 0.217 (0.224)	Loss 2.3060 (2.3070)	Acc@1 7.812 (9.810)
Epoch: [0][70/196]	Time 0.552 (0.576)	Data 0.219 (0.225)	Loss 2.2980 (2.3065)	Acc@1 15.625 (9.155)
Epoch: [0][70/196]	Time 0.555 (0.577)	Data 0.220 (0.223)	Loss 2.2982 (2.3064)	Acc@1 6.250 (9.881)
Epoch: [0][80/196]	Time 0.549 (0.573)	Data 0.219 (0.225)	Loss 2.3033 (2.3061)	Acc@1 7.812 (9.298)
Epoch: [0][80/196]	Time 0.553 (0.574)	Data 0.217 (0.223)	Loss 2.3009 (2.3060)	Acc@1 7.812 (10.012)
Epoch: [0][90/196]	Time 0.553 (0.571)	Data 0.220 (0.225)	Loss 2.3008 (2.3057)	Acc@1 18.750 (9.615)
Epoch: [0][90/196]	Time 0.555 (0.572)	Data 0.221 (0.222)	Loss 2.3011 (2.3055)	Acc@1 7.812 (10.216)
Epoch: [0][100/196]	Time 0.548 (0.569)	Data 0.216 (0.224)	Loss 2.3059 (2.3052)	Acc@1 6.250 (9.870)
Epoch: [0][100/196]	Time 0.552 (0.570)	Data 0.217 (0.222)	Loss 2.2921 (2.3051)	Acc@1 20.312 (10.520)
Epoch: [0][110/196]	Time 0.554 (0.568)	Data 0.221 (0.224)	Loss 2.2984 (2.3049)	Acc@1 14.062 (9.966)
Epoch: [0][110/196]	Time 0.553 (0.568)	Data 0.218 (0.222)	Loss 2.3026 (2.3049)	Acc@1 7.812 (10.529)
Epoch: [0][120/196]	Time 0.551 (0.567)	Data 0.220 (0.222)	Loss 2.3008 (2.3047)	Acc@1 6.250 (10.524)
Epoch: [0][120/196]	Time 0.556 (0.566)	Data 0.224 (0.224)	Loss 2.3009 (2.3046)	Acc@1 12.500 (10.111)
Epoch: [0][130/196]	Time 0.550 (0.565)	Data 0.221 (0.224)	Loss 2.3113 (2.3043)	Acc@1 7.812 (10.198)
Epoch: [0][130/196]	Time 0.557 (0.566)	Data 0.223 (0.222)	Loss 2.3098 (2.3045)	Acc@1 7.812 (10.615)
Epoch: [0][140/196]	Time 0.552 (0.565)	Data 0.220 (0.224)	Loss 2.3037 (2.3042)	Acc@1 9.375 (10.262)
Epoch: [0][140/196]	Time 0.555 (0.565)	Data 0.221 (0.221)	Loss 2.2986 (2.3043)	Acc@1 10.938 (10.760)
Epoch: [0][150/196]	Time 0.553 (0.564)	Data 0.218 (0.221)	Loss 2.3051 (2.3042)	Acc@1 12.500 (10.544)
Epoch: [0][150/196]	Time 0.553 (0.564)	Data 0.220 (0.223)	Loss 2.3074 (2.3040)	Acc@1 1.562 (10.306)
Epoch: [0][160/196]	Time 0.554 (0.564)	Data 0.219 (0.221)	Loss 2.3007 (2.3040)	Acc@1 7.812 (10.452)
Epoch: [0][160/196]	Time 0.559 (0.563)	Data 0.226 (0.223)	Loss 2.3016 (2.3039)	Acc@1 10.938 (10.326)
Epoch: [0][170/196]	Time 0.557 (0.563)	Data 0.224 (0.223)	Loss 2.3060 (2.3037)	Acc@1 12.500 (10.371)
Epoch: [0][170/196]	Time 0.555 (0.563)	Data 0.221 (0.221)	Loss 2.3044 (2.3038)	Acc@1 9.375 (10.444)
Epoch: [0][180/196]	Time 0.552 (0.562)	Data 0.220 (0.223)	Loss 2.3024 (2.3037)	Acc@1 7.812 (10.290)
Epoch: [0][180/196]	Time 0.559 (0.563)	Data 0.226 (0.221)	Loss 2.3041 (2.3037)	Acc@1 12.500 (10.463)
Epoch: [0][190/196]	Time 0.550 (0.562)	Data 0.218 (0.223)	Loss 2.3010 (2.3035)	Acc@1 14.062 (10.447)
Epoch: [0][190/196]	Time 0.553 (0.562)	Data 0.220 (0.221)	Loss 2.3006 (2.3036)	Acc@1 12.500 (10.496)
Test: [0/157]	Time 0.693 (0.693)	Data 0.411 (0.411)	Loss 2.3009 (2.3009)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.735 (0.735)	Data 0.453 (0.453)	Loss 2.3009 (2.3009)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.273 (0.313)	Data 0.006 (0.045)	Loss 2.3024 (2.3014)	Acc@1 7.812 (10.511)
Test: [10/157]	Time 0.272 (0.319)	Data 0.005 (0.049)	Loss 2.3024 (2.3014)	Acc@1 7.812 (10.511)
Test: [20/157]	Time 0.271 (0.293)	Data 0.006 (0.027)	Loss 2.3018 (2.3017)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.274 (0.297)	Data 0.007 (0.029)	Loss 2.3018 (2.3017)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.272 (0.286)	Data 0.008 (0.020)	Loss 2.3043 (2.3016)	Acc@1 9.375 (9.980)
Test: [30/157]	Time 0.273 (0.290)	Data 0.007 (0.022)	Loss 2.3043 (2.3016)	Acc@1 9.375 (9.980)
Test: [40/157]	Time 0.275 (0.283)	Data 0.009 (0.017)	Loss 2.3013 (2.3017)	Acc@1 12.500 (10.175)
Test: [40/157]	Time 0.274 (0.286)	Data 0.007 (0.018)	Loss 2.3013 (2.3017)	Acc@1 12.500 (10.175)
Test: [50/157]	Time 0.270 (0.281)	Data 0.007 (0.015)	Loss 2.2954 (2.3014)	Acc@1 10.938 (10.294)
Test: [50/157]	Time 0.272 (0.284)	Data 0.007 (0.016)	Loss 2.2954 (2.3014)	Acc@1 10.938 (10.294)
Test: [60/157]	Time 0.272 (0.279)	Data 0.007 (0.014)	Loss 2.2989 (2.3014)	Acc@1 7.812 (10.143)
Test: [60/157]	Time 0.273 (0.282)	Data 0.007 (0.015)	Loss 2.2989 (2.3014)	Acc@1 7.812 (10.143)
Test: [70/157]	Time 0.271 (0.278)	Data 0.006 (0.013)	Loss 2.3001 (2.3014)	Acc@1 7.812 (10.145)
Test: [70/157]	Time 0.292 (0.281)	Data 0.015 (0.014)	Loss 2.3001 (2.3014)	Acc@1 7.812 (10.145)
Test: [80/157]	Time 0.270 (0.277)	Data 0.007 (0.012)	Loss 2.3007 (2.3013)	Acc@1 10.938 (9.992)
Test: [80/157]	Time 0.274 (0.280)	Data 0.007 (0.013)	Loss 2.3007 (2.3013)	Acc@1 10.938 (9.992)
Test: [90/157]	Time 0.274 (0.277)	Data 0.007 (0.012)	Loss 2.2951 (2.3011)	Acc@1 12.500 (9.993)
Test: [90/157]	Time 0.275 (0.280)	Data 0.008 (0.012)	Loss 2.2951 (2.3011)	Acc@1 12.500 (9.993)
Test: [100/157]	Time 0.272 (0.276)	Data 0.008 (0.011)	Loss 2.3006 (2.3010)	Acc@1 12.500 (10.087)
Test: [100/157]	Time 0.273 (0.279)	Data 0.007 (0.012)	Loss 2.3006 (2.3010)	Acc@1 12.500 (10.087)
Test: [110/157]	Time 0.272 (0.276)	Data 0.007 (0.011)	Loss 2.3036 (2.3010)	Acc@1 9.375 (10.234)
Test: [110/157]	Time 0.276 (0.279)	Data 0.007 (0.012)	Loss 2.3036 (2.3010)	Acc@1 9.375 (10.234)
Test: [120/157]	Time 0.274 (0.276)	Data 0.007 (0.011)	Loss 2.2940 (2.3010)	Acc@1 18.750 (10.201)
Test: [120/157]	Time 0.274 (0.278)	Data 0.008 (0.011)	Loss 2.2940 (2.3010)	Acc@1 18.750 (10.201)
Test: [130/157]	Time 0.273 (0.275)	Data 0.007 (0.010)	Loss 2.3032 (2.3010)	Acc@1 3.125 (10.126)
Test: [130/157]	Time 0.273 (0.278)	Data 0.007 (0.011)	Loss 2.3032 (2.3010)	Acc@1 3.125 (10.126)
Test: [140/157]	Time 0.271 (0.275)	Data 0.007 (0.010)	Loss 2.3000 (2.3011)	Acc@1 15.625 (10.117)
Test: [140/157]	Time 0.274 (0.278)	Data 0.007 (0.011)	Loss 2.3000 (2.3011)	Acc@1 15.625 (10.117)
Test: [150/157]	Time 0.272 (0.275)	Data 0.007 (0.010)	Loss 2.3028 (2.3011)	Acc@1 9.375 (10.110)
Test: [150/157]	Time 0.273 (0.277)	Data 0.006 (0.010)	Loss 2.3028 (2.3011)	Acc@1 9.375 (10.110)
New Best Found: 10.19%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.19%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run crisp-firebrand-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/9rua0s0a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190740-9rua0s0a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run dry-firefly-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/t7z4e6xn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_190740-t7z4e6xn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22504 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 22503) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:10:34
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 22503)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191102-mhepjizl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sky-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/mhepjizl
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191102-5f0vjuwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-mountain-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/5f0vjuwe
Training 0.04501772644334634% of the parameters
starting training
Training 0.04501772644334634% of the parameters
starting training
Epoch: [0][0/196]	Time 2.168 (2.168)	Data 0.441 (0.441)	Loss 2.3360 (2.3360)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.412 (2.412)	Data 0.498 (0.498)	Loss 2.3292 (2.3292)	Acc@1 6.250 (6.250)
Epoch: [0][10/196]	Time 0.551 (0.720)	Data 0.219 (0.244)	Loss 2.2580 (2.3275)	Acc@1 17.188 (8.949)
Epoch: [0][10/196]	Time 0.549 (0.729)	Data 0.216 (0.270)	Loss 2.3521 (2.3231)	Acc@1 6.250 (10.511)
Epoch: [0][20/196]	Time 0.550 (0.644)	Data 0.217 (0.245)	Loss 2.3672 (2.3484)	Acc@1 9.375 (10.640)
Epoch: [0][20/196]	Time 0.550 (0.640)	Data 0.218 (0.232)	Loss 2.3818 (2.3552)	Acc@1 4.688 (9.077)
Epoch: [0][30/196]	Time 0.548 (0.611)	Data 0.220 (0.228)	Loss 2.3144 (2.3582)	Acc@1 14.062 (8.871)
Epoch: [0][30/196]	Time 0.549 (0.614)	Data 0.216 (0.237)	Loss 2.3642 (2.3506)	Acc@1 14.062 (10.837)
Epoch: [0][40/196]	Time 0.550 (0.596)	Data 0.218 (0.225)	Loss 2.4212 (2.3555)	Acc@1 4.688 (8.803)
Epoch: [0][40/196]	Time 0.550 (0.599)	Data 0.218 (0.232)	Loss 2.3495 (2.3469)	Acc@1 6.250 (10.671)
Epoch: [0][50/196]	Time 0.551 (0.587)	Data 0.219 (0.224)	Loss 2.3584 (2.3559)	Acc@1 15.625 (8.977)
Epoch: [0][50/196]	Time 0.549 (0.589)	Data 0.217 (0.229)	Loss 2.3573 (2.3451)	Acc@1 15.625 (10.539)
Epoch: [0][60/196]	Time 0.553 (0.581)	Data 0.222 (0.223)	Loss 2.3004 (2.3589)	Acc@1 17.188 (9.298)Epoch: [0][60/196]	Time 0.548 (0.583)	Data 0.217 (0.227)	Loss 2.3502 (2.3461)	Acc@1 10.938 (10.835)

Epoch: [0][70/196]	Time 0.546 (0.577)	Data 0.217 (0.222)	Loss 2.2918 (2.3518)	Acc@1 9.375 (9.991)
Epoch: [0][70/196]	Time 0.548 (0.578)	Data 0.217 (0.226)	Loss 2.3953 (2.3474)	Acc@1 10.938 (10.849)
Epoch: [0][80/196]	Time 0.545 (0.574)	Data 0.215 (0.222)	Loss 2.3127 (2.3480)	Acc@1 15.625 (10.397)
Epoch: [0][80/196]	Time 0.551 (0.575)	Data 0.218 (0.225)	Loss 2.2729 (2.3433)	Acc@1 17.188 (11.015)
Epoch: [0][90/196]	Time 0.546 (0.571)	Data 0.217 (0.222)	Loss 2.3801 (2.3464)	Acc@1 4.688 (10.731)
Epoch: [0][90/196]	Time 0.550 (0.572)	Data 0.217 (0.224)	Loss 2.3457 (2.3401)	Acc@1 14.062 (11.367)
Epoch: [0][100/196]	Time 0.547 (0.569)	Data 0.220 (0.221)	Loss 2.3823 (2.3444)	Acc@1 12.500 (10.922)
Epoch: [0][100/196]	Time 0.551 (0.570)	Data 0.217 (0.223)	Loss 2.3918 (2.3391)	Acc@1 14.062 (11.726)
Epoch: [0][110/196]	Time 0.554 (0.568)	Data 0.220 (0.221)	Loss 2.3640 (2.3411)	Acc@1 14.062 (11.205)
Epoch: [0][110/196]	Time 0.551 (0.568)	Data 0.217 (0.223)	Loss 2.2923 (2.3362)	Acc@1 23.438 (11.923)
Epoch: [0][120/196]	Time 0.556 (0.566)	Data 0.222 (0.221)	Loss 2.2799 (2.3390)	Acc@1 20.312 (11.506)Epoch: [0][120/196]	Time 0.550 (0.567)	Data 0.218 (0.222)	Loss 2.2746 (2.3359)	Acc@1 15.625 (12.035)

Epoch: [0][130/196]	Time 0.551 (0.565)	Data 0.220 (0.221)	Loss 2.3045 (2.3399)	Acc@1 18.750 (11.498)
Epoch: [0][130/196]	Time 0.551 (0.566)	Data 0.216 (0.222)	Loss 2.2928 (2.3345)	Acc@1 14.062 (12.047)
Epoch: [0][140/196]	Time 0.551 (0.564)	Data 0.219 (0.221)	Loss 2.3487 (2.3380)	Acc@1 18.750 (11.658)
Epoch: [0][140/196]	Time 0.552 (0.565)	Data 0.219 (0.222)	Loss 2.2493 (2.3354)	Acc@1 21.875 (12.023)
Epoch: [0][150/196]	Time 0.552 (0.563)	Data 0.220 (0.221)	Loss 2.3843 (2.3375)	Acc@1 17.188 (11.807)
Epoch: [0][150/196]	Time 0.550 (0.564)	Data 0.218 (0.221)	Loss 2.3969 (2.3368)	Acc@1 7.812 (11.889)
Epoch: [0][160/196]	Time 0.552 (0.563)	Data 0.220 (0.221)	Loss 2.3153 (2.3339)	Acc@1 14.062 (11.957)
Epoch: [0][160/196]	Time 0.552 (0.563)	Data 0.219 (0.221)	Loss 2.3614 (2.3353)	Acc@1 14.062 (11.966)
Epoch: [0][170/196]	Time 0.549 (0.562)	Data 0.220 (0.221)	Loss 2.2681 (2.3299)	Acc@1 14.062 (12.098)
Epoch: [0][170/196]	Time 0.555 (0.563)	Data 0.221 (0.221)	Loss 2.2632 (2.3320)	Acc@1 17.188 (12.034)
Epoch: [0][180/196]	Time 0.549 (0.562)	Data 0.219 (0.221)	Loss 2.2528 (2.3271)	Acc@1 18.750 (12.293)
Epoch: [0][180/196]	Time 0.551 (0.562)	Data 0.218 (0.221)	Loss 2.3471 (2.3304)	Acc@1 9.375 (12.112)
Epoch: [0][190/196]	Time 0.551 (0.561)	Data 0.221 (0.221)	Loss 2.2320 (2.3247)	Acc@1 20.312 (12.492)
Epoch: [0][190/196]	Time 0.548 (0.562)	Data 0.216 (0.221)	Loss 2.3057 (2.3282)	Acc@1 14.062 (12.107)
Test: [0/157]	Time 0.699 (0.699)	Data 0.421 (0.421)	Loss 2.2487 (2.2487)	Acc@1 23.438 (23.438)
Test: [0/157]	Time 0.722 (0.722)	Data 0.438 (0.438)	Loss 2.2487 (2.2487)	Acc@1 23.438 (23.438)
Test: [10/157]	Time 0.271 (0.313)	Data 0.005 (0.046)	Loss 2.2994 (2.2688)	Acc@1 20.312 (16.619)
Test: [10/157]	Time 0.271 (0.315)	Data 0.005 (0.046)	Loss 2.2994 (2.2688)	Acc@1 20.312 (16.619)
Test: [20/157]	Time 0.272 (0.294)	Data 0.007 (0.027)	Loss 2.2691 (2.2673)	Acc@1 17.188 (16.667)
Test: [20/157]	Time 0.274 (0.295)	Data 0.007 (0.027)	Loss 2.2691 (2.2673)	Acc@1 17.188 (16.667)
Test: [30/157]	Time 0.271 (0.287)	Data 0.007 (0.020)	Loss 2.2639 (2.2671)	Acc@1 17.188 (17.288)
Test: [30/157]	Time 0.275 (0.288)	Data 0.007 (0.021)	Loss 2.2639 (2.2671)	Acc@1 17.188 (17.288)
Test: [40/157]	Time 0.270 (0.283)	Data 0.007 (0.017)	Loss 2.2457 (2.2677)	Acc@1 21.875 (17.073)
Test: [40/157]	Time 0.274 (0.285)	Data 0.007 (0.018)	Loss 2.2457 (2.2677)	Acc@1 21.875 (17.073)
Test: [50/157]	Time 0.272 (0.281)	Data 0.007 (0.015)	Loss 2.3140 (2.2709)	Acc@1 18.750 (17.065)
Test: [50/157]	Time 0.274 (0.282)	Data 0.007 (0.015)	Loss 2.3140 (2.2709)	Acc@1 18.750 (17.065)
Test: [60/157]	Time 0.272 (0.280)	Data 0.007 (0.014)	Loss 2.3042 (2.2698)	Acc@1 12.500 (17.444)
Test: [60/157]	Time 0.274 (0.281)	Data 0.007 (0.014)	Loss 2.3042 (2.2698)	Acc@1 12.500 (17.444)
Test: [70/157]	Time 0.272 (0.279)	Data 0.007 (0.013)	Loss 2.2139 (2.2690)	Acc@1 18.750 (17.496)
Test: [70/157]	Time 0.276 (0.280)	Data 0.008 (0.013)	Loss 2.2139 (2.2690)	Acc@1 18.750 (17.496)
Test: [80/157]	Time 0.273 (0.278)	Data 0.007 (0.012)	Loss 2.2148 (2.2715)	Acc@1 18.750 (17.573)
Test: [80/157]	Time 0.272 (0.279)	Data 0.007 (0.012)	Loss 2.2148 (2.2715)	Acc@1 18.750 (17.573)
Test: [90/157]	Time 0.275 (0.277)	Data 0.008 (0.012)	Loss 2.3454 (2.2742)	Acc@1 7.812 (17.617)
Test: [90/157]	Time 0.275 (0.279)	Data 0.007 (0.012)	Loss 2.3454 (2.2742)	Acc@1 7.812 (17.617)
Test: [100/157]	Time 0.272 (0.277)	Data 0.007 (0.011)	Loss 2.2424 (2.2753)	Acc@1 17.188 (17.791)
Test: [100/157]	Time 0.276 (0.278)	Data 0.007 (0.011)	Loss 2.2424 (2.2753)	Acc@1 17.188 (17.791)
Test: [110/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.2081 (2.2747)	Acc@1 20.312 (17.553)
Test: [110/157]	Time 0.273 (0.278)	Data 0.008 (0.011)	Loss 2.2081 (2.2747)	Acc@1 20.312 (17.553)
Test: [120/157]	Time 0.271 (0.276)	Data 0.007 (0.011)	Loss 2.2137 (2.2749)	Acc@1 25.000 (17.497)
Test: [120/157]	Time 0.273 (0.278)	Data 0.007 (0.011)	Loss 2.2137 (2.2749)	Acc@1 25.000 (17.497)
Test: [130/157]	Time 0.274 (0.276)	Data 0.007 (0.010)	Loss 2.2787 (2.2757)	Acc@1 21.875 (17.474)
Test: [130/157]	Time 0.275 (0.277)	Data 0.008 (0.010)	Loss 2.2787 (2.2757)	Acc@1 21.875 (17.474)
Test: [140/157]	Time 0.273 (0.276)	Data 0.007 (0.010)	Loss 2.2608 (2.2739)	Acc@1 14.062 (17.498)
Test: [140/157]	Time 0.273 (0.277)	Data 0.007 (0.010)	Loss 2.2608 (2.2739)	Acc@1 14.062 (17.498)
Test: [150/157]	Time 0.271 (0.275)	Data 0.006 (0.010)	Loss 2.3086 (2.2735)	Acc@1 18.750 (17.643)
Test: [150/157]	Time 0.274 (0.277)	Data 0.007 (0.010)	Loss 2.3086 (2.2735)	Acc@1 18.750 (17.643)
New Best Found: 17.67%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 17.67%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run quiet-mountain-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/5f0vjuwe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191102-5f0vjuwe/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run young-sky-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter/runs/mhepjizl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191102-mhepjizl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 24233 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24232) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:13:56
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 24232)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 221, in main
    os.makedirs(args.outdir+folder+"/"+str(args.noise_sd))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/scratch/ravihm.scee.iitmandi/models/cifar10/vit/adapters_seq_bn_0.5/0.25'
Files already downloaded and verified
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 25964 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 25965) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:14:16
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 25965)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191444-vhwccsi5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sky-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/vhwccsi5
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191444-ecuewtu8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-dew-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/ecuewtu8
Training 1.0406100404478045% of the parameters
starting training
Training 1.0406100404478045% of the parameters
starting training
Epoch: [0][0/196]	Time 2.378 (2.378)	Data 0.526 (0.526)	Loss 2.3244 (2.3244)	Acc@1 6.250 (6.250)
Epoch: [0][0/196]	Time 2.328 (2.328)	Data 0.537 (0.537)	Loss 2.3340 (2.3340)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.542 (0.697)	Data 0.250 (0.272)	Loss 2.2997 (2.3116)	Acc@1 7.812 (8.807)
Epoch: [0][10/196]	Time 0.541 (0.692)	Data 0.252 (0.273)	Loss 2.3162 (2.3201)	Acc@1 7.812 (8.949)
Epoch: [0][20/196]	Time 0.529 (0.615)	Data 0.248 (0.258)	Loss 2.3211 (2.3106)	Acc@1 3.125 (8.780)
Epoch: [0][20/196]	Time 0.527 (0.613)	Data 0.246 (0.260)	Loss 2.3185 (2.3132)	Acc@1 12.500 (9.226)
Epoch: [0][30/196]	Time 0.526 (0.587)	Data 0.243 (0.254)	Loss 2.3033 (2.3084)	Acc@1 6.250 (8.569)Epoch: [0][30/196]	Time 0.530 (0.585)	Data 0.247 (0.255)	Loss 2.3026 (2.3090)	Acc@1 4.688 (9.325)

Epoch: [0][40/196]	Time 0.527 (0.573)	Data 0.244 (0.252)	Loss 2.3103 (2.3071)	Acc@1 10.938 (8.613)
Epoch: [0][40/196]	Time 0.527 (0.571)	Data 0.244 (0.253)	Loss 2.3017 (2.3075)	Acc@1 15.625 (9.413)
Epoch: [0][50/196]	Time 0.529 (0.564)	Data 0.246 (0.250)	Loss 2.2974 (2.3063)	Acc@1 9.375 (9.222)Epoch: [0][50/196]	Time 0.529 (0.563)	Data 0.245 (0.251)	Loss 2.3038 (2.3066)	Acc@1 7.812 (9.835)

Epoch: [0][60/196]	Time 0.534 (0.558)	Data 0.252 (0.250)	Loss 2.3092 (2.3059)	Acc@1 3.125 (9.119)Epoch: [0][60/196]	Time 0.529 (0.557)	Data 0.247 (0.250)	Loss 2.3068 (2.3062)	Acc@1 7.812 (9.580)

Epoch: [0][70/196]	Time 0.527 (0.554)	Data 0.245 (0.249)	Loss 2.2971 (2.3055)	Acc@1 17.188 (9.199)
Epoch: [0][70/196]	Time 0.528 (0.553)	Data 0.245 (0.250)	Loss 2.2973 (2.3057)	Acc@1 4.688 (9.595)
Epoch: [0][80/196]	Time 0.532 (0.551)	Data 0.249 (0.249)	Loss 2.3026 (2.3053)	Acc@1 4.688 (9.240)
Epoch: [0][80/196]	Time 0.529 (0.551)	Data 0.246 (0.249)	Loss 2.3003 (2.3054)	Acc@1 9.375 (9.703)
Epoch: [0][90/196]	Time 0.532 (0.548)	Data 0.247 (0.249)	Loss 2.3000 (2.3049)	Acc@1 7.812 (9.907)Epoch: [0][90/196]	Time 0.533 (0.549)	Data 0.249 (0.249)	Loss 2.3004 (2.3049)	Acc@1 17.188 (9.512)

Epoch: [0][100/196]	Time 0.535 (0.547)	Data 0.252 (0.249)	Loss 2.3064 (2.3045)	Acc@1 4.688 (9.762)Epoch: [0][100/196]	Time 0.529 (0.547)	Data 0.247 (0.249)	Loss 2.2913 (2.3045)	Acc@1 17.188 (10.272)

Epoch: [0][110/196]	Time 0.552 (0.546)	Data 0.248 (0.249)	Loss 2.2984 (2.3043)	Acc@1 14.062 (9.783)Epoch: [0][110/196]	Time 0.548 (0.545)	Data 0.245 (0.248)	Loss 2.3017 (2.3043)	Acc@1 9.375 (10.346)

Epoch: [0][120/196]	Time 0.532 (0.544)	Data 0.248 (0.249)	Loss 2.3008 (2.3039)	Acc@1 12.500 (9.956)
Epoch: [0][120/196]	Time 0.530 (0.544)	Data 0.245 (0.248)	Loss 2.3005 (2.3042)	Acc@1 6.250 (10.305)
Epoch: [0][130/196]	Time 0.532 (0.543)	Data 0.248 (0.248)	Loss 2.3101 (2.3041)	Acc@1 7.812 (10.425)
Epoch: [0][130/196]	Time 0.533 (0.543)	Data 0.249 (0.249)	Loss 2.3121 (2.3038)	Acc@1 7.812 (10.055)
Epoch: [0][140/196]	Time 0.535 (0.543)	Data 0.253 (0.249)	Loss 2.3035 (2.3036)	Acc@1 9.375 (10.117)Epoch: [0][140/196]	Time 0.530 (0.542)	Data 0.246 (0.248)	Loss 2.2979 (2.3038)	Acc@1 10.938 (10.516)

Epoch: [0][150/196]	Time 0.529 (0.542)	Data 0.245 (0.249)	Loss 2.3074 (2.3034)	Acc@1 1.562 (10.182)
Epoch: [0][150/196]	Time 0.532 (0.541)	Data 0.250 (0.248)	Loss 2.3053 (2.3037)	Acc@1 12.500 (10.296)
Epoch: [0][160/196]	Time 0.540 (0.541)	Data 0.254 (0.249)	Loss 2.3016 (2.3033)	Acc@1 10.938 (10.181)
Epoch: [0][160/196]	Time 0.536 (0.541)	Data 0.251 (0.248)	Loss 2.3009 (2.3035)	Acc@1 10.938 (10.248)
Epoch: [0][170/196]	Time 0.533 (0.541)	Data 0.253 (0.249)	Loss 2.3067 (2.3032)	Acc@1 10.938 (10.298)
Epoch: [0][170/196]	Time 0.533 (0.540)	Data 0.249 (0.248)	Loss 2.3047 (2.3034)	Acc@1 10.938 (10.261)
Epoch: [0][180/196]	Time 0.527 (0.540)	Data 0.249 (0.249)	Loss 2.3019 (2.3032)	Acc@1 9.375 (10.230)
Epoch: [0][180/196]	Time 0.530 (0.540)	Data 0.249 (0.248)	Loss 2.3037 (2.3033)	Acc@1 10.938 (10.290)
Epoch: [0][190/196]	Time 0.540 (0.540)	Data 0.262 (0.249)	Loss 2.3003 (2.3030)	Acc@1 14.062 (10.414)
Epoch: [0][190/196]	Time 0.529 (0.540)	Data 0.249 (0.248)	Loss 2.3004 (2.3032)	Acc@1 12.500 (10.357)
Test: [0/157]	Time 0.694 (0.694)	Data 0.418 (0.418)	Loss 2.2998 (2.2998)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.708 (0.708)	Data 0.433 (0.433)	Loss 2.2998 (2.2998)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.265 (0.307)	Data 0.006 (0.045)	Loss 2.3015 (2.3007)	Acc@1 7.812 (10.511)
Test: [10/157]	Time 0.269 (0.312)	Data 0.006 (0.047)	Loss 2.3015 (2.3007)	Acc@1 7.812 (10.511)
Test: [20/157]	Time 0.270 (0.288)	Data 0.008 (0.027)	Loss 2.3017 (2.3013)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.271 (0.292)	Data 0.007 (0.028)	Loss 2.3017 (2.3013)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.269 (0.282)	Data 0.009 (0.021)	Loss 2.3036 (2.3012)	Acc@1 9.375 (9.980)
Test: [30/157]	Time 0.274 (0.285)	Data 0.009 (0.021)	Loss 2.3036 (2.3012)	Acc@1 9.375 (9.980)
Test: [40/157]	Time 0.268 (0.279)	Data 0.007 (0.018)	Loss 2.3008 (2.3013)	Acc@1 12.500 (10.175)
Test: [40/157]	Time 0.271 (0.282)	Data 0.008 (0.018)	Loss 2.3008 (2.3013)	Acc@1 12.500 (10.175)
Test: [50/157]	Time 0.268 (0.277)	Data 0.008 (0.016)	Loss 2.2950 (2.3010)	Acc@1 9.375 (10.233)
Test: [50/157]	Time 0.272 (0.280)	Data 0.007 (0.016)	Loss 2.2950 (2.3010)	Acc@1 9.375 (10.233)
Test: [60/157]	Time 0.269 (0.276)	Data 0.007 (0.014)	Loss 2.2989 (2.3009)	Acc@1 7.812 (10.143)
Test: [60/157]	Time 0.271 (0.279)	Data 0.007 (0.015)	Loss 2.2989 (2.3009)	Acc@1 7.812 (10.143)
Test: [70/157]	Time 0.267 (0.275)	Data 0.007 (0.013)	Loss 2.2997 (2.3010)	Acc@1 7.812 (10.167)
Test: [70/157]	Time 0.271 (0.278)	Data 0.009 (0.014)	Loss 2.2997 (2.3010)	Acc@1 7.812 (10.167)
Test: [80/157]	Time 0.268 (0.274)	Data 0.008 (0.013)	Loss 2.3001 (2.3009)	Acc@1 10.938 (10.050)
Test: [80/157]	Time 0.273 (0.277)	Data 0.009 (0.013)	Loss 2.3001 (2.3009)	Acc@1 10.938 (10.050)
Test: [90/157]	Time 0.269 (0.273)	Data 0.008 (0.012)	Loss 2.2945 (2.3007)	Acc@1 12.500 (10.062)
Test: [90/157]	Time 0.272 (0.276)	Data 0.008 (0.012)	Loss 2.2945 (2.3007)	Acc@1 12.500 (10.062)
Test: [100/157]	Time 0.270 (0.273)	Data 0.008 (0.012)	Loss 2.3007 (2.3006)	Acc@1 12.500 (10.149)
Test: [100/157]	Time 0.272 (0.276)	Data 0.008 (0.012)	Loss 2.3007 (2.3006)	Acc@1 12.500 (10.149)
Test: [110/157]	Time 0.271 (0.273)	Data 0.008 (0.011)	Loss 2.3030 (2.3005)	Acc@1 9.375 (10.248)
Test: [110/157]	Time 0.271 (0.275)	Data 0.008 (0.012)	Loss 2.3030 (2.3005)	Acc@1 9.375 (10.248)
Test: [120/157]	Time 0.268 (0.272)	Data 0.009 (0.011)	Loss 2.2925 (2.3005)	Acc@1 18.750 (10.227)
Test: [120/157]	Time 0.271 (0.275)	Data 0.009 (0.011)	Loss 2.2925 (2.3005)	Acc@1 18.750 (10.227)
Test: [130/157]	Time 0.268 (0.272)	Data 0.008 (0.011)	Loss 2.3034 (2.3006)	Acc@1 4.688 (10.174)
Test: [130/157]	Time 0.270 (0.275)	Data 0.007 (0.011)	Loss 2.3034 (2.3006)	Acc@1 4.688 (10.174)
Test: [140/157]	Time 0.268 (0.272)	Data 0.008 (0.011)	Loss 2.2995 (2.3007)	Acc@1 15.625 (10.151)
Test: [140/157]	Time 0.271 (0.275)	Data 0.007 (0.011)	Loss 2.2995 (2.3007)	Acc@1 15.625 (10.151)
Test: [150/157]	Time 0.270 (0.272)	Data 0.008 (0.011)	Loss 2.3024 (2.3006)	Acc@1 9.375 (10.182)
Test: [150/157]	Time 0.272 (0.274)	Data 0.009 (0.011)	Loss 2.3024 (2.3006)	Acc@1 9.375 (10.182)
New Best Found: 10.26%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.26%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run stoic-dew-1 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/ecuewtu8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191444-ecuewtu8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run exalted-sky-1 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/vhwccsi5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191444-vhwccsi5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 26057 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 26058) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:17:30
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 26058)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191758-8u00my51
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-field-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/8u00my51
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_191759-14jx9tiw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-breeze-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/14jx9tiw
Training 1.0406100404478045% of the parameters
starting training
Training 1.0406100404478045% of the parameters
starting training
Epoch: [0][0/196]	Time 2.145 (2.145)	Data 0.487 (0.487)	Loss 2.3340 (2.3340)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.161 (2.161)	Data 0.439 (0.439)	Loss 2.3244 (2.3244)	Acc@1 6.250 (6.250)
Epoch: [0][10/196]	Time 0.527 (0.675)	Data 0.244 (0.263)	Loss 2.2997 (2.3116)	Acc@1 7.812 (8.807)
Epoch: [0][10/196]	Time 0.525 (0.681)	Data 0.242 (0.274)	Loss 2.3162 (2.3201)	Acc@1 7.812 (8.949)
Epoch: [0][20/196]	Time 0.536 (0.606)	Data 0.253 (0.256)	Loss 2.3211 (2.3106)	Acc@1 3.125 (8.780)
Epoch: [0][20/196]	Time 0.536 (0.609)	Data 0.250 (0.262)	Loss 2.3185 (2.3132)	Acc@1 12.500 (9.226)
Epoch: [0][30/196]	Time 0.537 (0.582)	Data 0.255 (0.254)	Loss 2.3033 (2.3084)	Acc@1 6.250 (8.569)
Epoch: [0][30/196]	Time 0.538 (0.584)	Data 0.254 (0.258)	Loss 2.3026 (2.3090)	Acc@1 4.688 (9.325)
Epoch: [0][40/196]	Time 0.545 (0.570)	Data 0.265 (0.253)	Loss 2.3103 (2.3071)	Acc@1 10.938 (8.613)
Epoch: [0][40/196]	Time 0.532 (0.571)	Data 0.248 (0.255)	Loss 2.3017 (2.3075)	Acc@1 15.625 (9.413)
Epoch: [0][50/196]	Time 0.531 (0.563)	Data 0.249 (0.254)	Loss 2.3038 (2.3066)	Acc@1 7.812 (9.835)Epoch: [0][50/196]	Time 0.532 (0.562)	Data 0.250 (0.253)	Loss 2.2974 (2.3063)	Acc@1 9.375 (9.222)

Epoch: [0][60/196]	Time 0.528 (0.558)	Data 0.244 (0.253)	Loss 2.3068 (2.3062)	Acc@1 7.812 (9.580)
Epoch: [0][60/196]	Time 0.529 (0.557)	Data 0.247 (0.252)	Loss 2.3092 (2.3059)	Acc@1 3.125 (9.119)
Epoch: [0][70/196]	Time 0.533 (0.553)	Data 0.254 (0.251)	Loss 2.2971 (2.3055)	Acc@1 17.188 (9.199)Epoch: [0][70/196]	Time 0.530 (0.554)	Data 0.250 (0.252)	Loss 2.2973 (2.3057)	Acc@1 4.688 (9.595)

Epoch: [0][80/196]	Time 0.535 (0.550)	Data 0.252 (0.251)	Loss 2.3026 (2.3053)	Acc@1 4.688 (9.240)
Epoch: [0][80/196]	Time 0.535 (0.551)	Data 0.249 (0.252)	Loss 2.3003 (2.3054)	Acc@1 9.375 (9.703)
Epoch: [0][90/196]	Time 0.532 (0.548)	Data 0.247 (0.251)	Loss 2.3004 (2.3049)	Acc@1 17.188 (9.512)Epoch: [0][90/196]	Time 0.532 (0.549)	Data 0.247 (0.252)	Loss 2.3000 (2.3049)	Acc@1 7.812 (9.907)

Epoch: [0][100/196]	Time 0.530 (0.547)	Data 0.250 (0.251)	Loss 2.2913 (2.3045)	Acc@1 17.188 (10.272)
Epoch: [0][100/196]	Time 0.534 (0.547)	Data 0.252 (0.251)	Loss 2.3064 (2.3045)	Acc@1 4.688 (9.762)
Epoch: [0][110/196]	Time 0.532 (0.545)	Data 0.250 (0.251)	Loss 2.2984 (2.3043)	Acc@1 14.062 (9.783)
Epoch: [0][110/196]	Time 0.532 (0.546)	Data 0.250 (0.251)	Loss 2.3017 (2.3043)	Acc@1 9.375 (10.346)
Epoch: [0][120/196]	Time 0.529 (0.544)	Data 0.246 (0.251)	Loss 2.3008 (2.3039)	Acc@1 12.500 (9.956)
Epoch: [0][120/196]	Time 0.533 (0.545)	Data 0.249 (0.251)	Loss 2.3005 (2.3042)	Acc@1 6.250 (10.305)
Epoch: [0][130/196]	Time 0.530 (0.544)	Data 0.249 (0.251)	Loss 2.3101 (2.3041)	Acc@1 7.812 (10.425)
Epoch: [0][130/196]	Time 0.535 (0.543)	Data 0.251 (0.250)	Loss 2.3121 (2.3038)	Acc@1 7.812 (10.055)
Epoch: [0][140/196]	Time 0.531 (0.542)	Data 0.249 (0.250)	Loss 2.3035 (2.3036)	Acc@1 9.375 (10.117)
Epoch: [0][140/196]	Time 0.535 (0.543)	Data 0.252 (0.251)	Loss 2.2979 (2.3038)	Acc@1 10.938 (10.516)
Epoch: [0][150/196]	Time 0.531 (0.541)	Data 0.249 (0.250)	Loss 2.3074 (2.3034)	Acc@1 1.562 (10.182)
Epoch: [0][150/196]	Time 0.528 (0.542)	Data 0.246 (0.250)	Loss 2.3053 (2.3037)	Acc@1 12.500 (10.296)
Epoch: [0][160/196]	Time 0.540 (0.541)	Data 0.258 (0.250)	Loss 2.3016 (2.3033)	Acc@1 10.938 (10.181)
Epoch: [0][160/196]	Time 0.538 (0.541)	Data 0.250 (0.250)	Loss 2.3009 (2.3035)	Acc@1 10.938 (10.248)
Epoch: [0][170/196]	Time 0.535 (0.541)	Data 0.252 (0.250)	Loss 2.3067 (2.3032)	Acc@1 10.938 (10.298)Epoch: [0][170/196]	Time 0.530 (0.541)	Data 0.247 (0.250)	Loss 2.3047 (2.3034)	Acc@1 10.938 (10.261)

Epoch: [0][180/196]	Time 0.532 (0.540)	Data 0.251 (0.250)	Loss 2.3019 (2.3032)	Acc@1 9.375 (10.230)
Epoch: [0][180/196]	Time 0.530 (0.540)	Data 0.246 (0.250)	Loss 2.3037 (2.3033)	Acc@1 10.938 (10.290)
Epoch: [0][190/196]	Time 0.531 (0.540)	Data 0.248 (0.250)	Loss 2.3003 (2.3030)	Acc@1 14.062 (10.414)Epoch: [0][190/196]	Time 0.532 (0.540)	Data 0.250 (0.250)	Loss 2.3004 (2.3032)	Acc@1 12.500 (10.357)

Test: [0/157]	Time 0.713 (0.713)	Data 0.438 (0.438)	Loss 2.2998 (2.2998)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.720 (0.720)	Data 0.444 (0.444)	Loss 2.2998 (2.2998)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.266 (0.311)	Data 0.005 (0.047)	Loss 2.3015 (2.3007)	Acc@1 7.812 (10.511)
Test: [10/157]	Time 0.286 (0.313)	Data 0.015 (0.048)	Loss 2.3015 (2.3007)	Acc@1 7.812 (10.511)
Test: [20/157]	Time 0.270 (0.291)	Data 0.007 (0.028)	Loss 2.3017 (2.3013)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.269 (0.292)	Data 0.007 (0.028)	Loss 2.3017 (2.3013)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.268 (0.284)	Data 0.008 (0.021)	Loss 2.3036 (2.3012)	Acc@1 9.375 (9.980)
Test: [30/157]	Time 0.269 (0.285)	Data 0.007 (0.022)	Loss 2.3036 (2.3012)	Acc@1 9.375 (9.980)
Test: [40/157]	Time 0.270 (0.280)	Data 0.007 (0.018)	Loss 2.3008 (2.3013)	Acc@1 12.500 (10.175)
Test: [40/157]	Time 0.271 (0.282)	Data 0.007 (0.018)	Loss 2.3008 (2.3013)	Acc@1 12.500 (10.175)
Test: [50/157]	Time 0.267 (0.278)	Data 0.007 (0.016)	Loss 2.2950 (2.3010)	Acc@1 9.375 (10.233)
Test: [50/157]	Time 0.271 (0.279)	Data 0.008 (0.016)	Loss 2.2950 (2.3010)	Acc@1 9.375 (10.233)
Test: [60/157]	Time 0.267 (0.276)	Data 0.007 (0.014)	Loss 2.2989 (2.3009)	Acc@1 7.812 (10.143)
Test: [60/157]	Time 0.271 (0.278)	Data 0.007 (0.015)	Loss 2.2989 (2.3009)	Acc@1 7.812 (10.143)
Test: [70/157]	Time 0.268 (0.275)	Data 0.007 (0.013)	Loss 2.2997 (2.3010)	Acc@1 7.812 (10.167)
Test: [70/157]	Time 0.267 (0.277)	Data 0.007 (0.014)	Loss 2.2997 (2.3010)	Acc@1 7.812 (10.167)
Test: [80/157]	Time 0.271 (0.274)	Data 0.006 (0.013)	Loss 2.3001 (2.3009)	Acc@1 10.938 (10.050)
Test: [80/157]	Time 0.269 (0.276)	Data 0.007 (0.013)	Loss 2.3001 (2.3009)	Acc@1 10.938 (10.050)
Test: [90/157]	Time 0.269 (0.274)	Data 0.007 (0.012)	Loss 2.2945 (2.3007)	Acc@1 12.500 (10.062)
Test: [90/157]	Time 0.269 (0.275)	Data 0.007 (0.012)	Loss 2.2945 (2.3007)	Acc@1 12.500 (10.062)
Test: [100/157]	Time 0.267 (0.273)	Data 0.006 (0.011)	Loss 2.3007 (2.3006)	Acc@1 12.500 (10.149)
Test: [100/157]	Time 0.271 (0.275)	Data 0.007 (0.012)	Loss 2.3007 (2.3006)	Acc@1 12.500 (10.149)
Test: [110/157]	Time 0.268 (0.273)	Data 0.006 (0.011)	Loss 2.3030 (2.3005)	Acc@1 9.375 (10.248)
Test: [110/157]	Time 0.271 (0.275)	Data 0.008 (0.011)	Loss 2.3030 (2.3005)	Acc@1 9.375 (10.248)
Test: [120/157]	Time 0.270 (0.272)	Data 0.008 (0.011)	Loss 2.2925 (2.3005)	Acc@1 18.750 (10.227)
Test: [120/157]	Time 0.271 (0.274)	Data 0.009 (0.011)	Loss 2.2925 (2.3005)	Acc@1 18.750 (10.227)
Test: [130/157]	Time 0.269 (0.272)	Data 0.007 (0.010)	Loss 2.3034 (2.3006)	Acc@1 4.688 (10.174)
Test: [130/157]	Time 0.271 (0.274)	Data 0.007 (0.011)	Loss 2.3034 (2.3006)	Acc@1 4.688 (10.174)
Test: [140/157]	Time 0.266 (0.272)	Data 0.006 (0.010)	Loss 2.2995 (2.3007)	Acc@1 15.625 (10.151)
Test: [140/157]	Time 0.268 (0.274)	Data 0.006 (0.011)	Loss 2.2995 (2.3007)	Acc@1 15.625 (10.151)
Test: [150/157]	Time 0.268 (0.272)	Data 0.007 (0.010)	Loss 2.3024 (2.3006)	Acc@1 9.375 (10.182)
Test: [150/157]	Time 0.270 (0.274)	Data 0.007 (0.010)	Loss 2.3024 (2.3006)	Acc@1 9.375 (10.182)
New Best Found: 10.26%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.26%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run atomic-field-3 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/8u00my51
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191758-8u00my51/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run amber-breeze-4 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/14jx9tiw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_191759-14jx9tiw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27782 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 27783) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:20:46
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 27783)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_192113-7cmo52td
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-jazz-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/7cmo52td
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_192113-k6je7fdn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-pine-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/k6je7fdn
Training 1.0406100404478045% of the parameters
starting training
Training 1.0406100404478045% of the parameters
starting training
Epoch: [0][0/196]	Time 2.073 (2.073)	Data 0.439 (0.439)	Loss 2.3340 (2.3340)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.260 (2.260)	Data 0.574 (0.574)	Loss 2.3244 (2.3244)	Acc@1 6.250 (6.250)
Epoch: [0][10/196]	Time 0.527 (0.676)	Data 0.244 (0.273)	Loss 2.3465 (2.3248)	Acc@1 12.500 (10.938)
Epoch: [0][10/196]	Time 0.530 (0.684)	Data 0.246 (0.275)	Loss 2.2688 (2.3315)	Acc@1 12.500 (9.233)
Epoch: [0][20/196]	Time 0.526 (0.609)	Data 0.247 (0.260)	Loss 2.3766 (2.3565)	Acc@1 4.688 (9.077)
Epoch: [0][20/196]	Time 0.529 (0.605)	Data 0.247 (0.260)	Loss 2.3605 (2.3488)	Acc@1 9.375 (10.789)
Epoch: [0][30/196]	Time 0.523 (0.583)	Data 0.243 (0.255)	Loss 2.3078 (2.3581)	Acc@1 14.062 (9.073)
Epoch: [0][30/196]	Time 0.524 (0.580)	Data 0.243 (0.255)	Loss 2.3546 (2.3494)	Acc@1 14.062 (10.988)
Epoch: [0][40/196]	Time 0.522 (0.567)	Data 0.245 (0.252)	Loss 2.3443 (2.3452)	Acc@1 10.938 (10.861)
Epoch: [0][40/196]	Time 0.530 (0.569)	Data 0.250 (0.253)	Loss 2.3727 (2.3542)	Acc@1 10.938 (9.070)
Epoch: [0][50/196]	Time 0.527 (0.559)	Data 0.242 (0.251)	Loss 2.3035 (2.3429)	Acc@1 15.625 (10.600)
Epoch: [0][50/196]	Time 0.532 (0.561)	Data 0.250 (0.252)	Loss 2.3482 (2.3530)	Acc@1 14.062 (9.161)
Epoch: [0][60/196]	Time 0.523 (0.556)	Data 0.243 (0.251)	Loss 2.2565 (2.3510)	Acc@1 15.625 (9.375)
Epoch: [0][60/196]	Time 0.524 (0.554)	Data 0.244 (0.250)	Loss 2.3395 (2.3396)	Acc@1 9.375 (11.194)
Epoch: [0][70/196]	Time 0.525 (0.552)	Data 0.244 (0.250)	Loss 2.1808 (2.3361)	Acc@1 12.500 (10.299)
Epoch: [0][70/196]	Time 0.532 (0.551)	Data 0.249 (0.249)	Loss 2.3598 (2.3327)	Acc@1 15.625 (11.598)
Epoch: [0][80/196]	Time 0.532 (0.548)	Data 0.250 (0.249)	Loss 2.3441 (2.3556)	Acc@1 14.062 (11.632)
Epoch: [0][80/196]	Time 0.540 (0.549)	Data 0.252 (0.250)	Loss 2.4348 (2.3580)	Acc@1 9.375 (10.243)
Epoch: [0][90/196]	Time 0.525 (0.546)	Data 0.245 (0.249)	Loss 2.3418 (2.3563)	Acc@1 14.062 (11.470)
Epoch: [0][90/196]	Time 0.531 (0.547)	Data 0.248 (0.249)	Loss 2.3752 (2.3595)	Acc@1 9.375 (10.251)
Epoch: [0][100/196]	Time 0.531 (0.545)	Data 0.249 (0.249)	Loss 2.3619 (2.3571)	Acc@1 9.375 (10.087)Epoch: [0][100/196]	Time 0.529 (0.544)	Data 0.247 (0.248)	Loss 2.3452 (2.3544)	Acc@1 10.938 (11.293)

Epoch: [0][110/196]	Time 0.523 (0.543)	Data 0.243 (0.249)	Loss 2.2810 (2.3536)	Acc@1 14.062 (9.952)
Epoch: [0][110/196]	Time 0.524 (0.543)	Data 0.242 (0.248)	Loss 2.3206 (2.3498)	Acc@1 12.500 (11.360)
Epoch: [0][120/196]	Time 0.524 (0.542)	Data 0.244 (0.248)	Loss 2.3104 (2.3508)	Acc@1 9.375 (9.698)
Epoch: [0][120/196]	Time 0.526 (0.541)	Data 0.243 (0.248)	Loss 2.3586 (2.3468)	Acc@1 1.562 (11.170)
Epoch: [0][130/196]	Time 0.523 (0.541)	Data 0.245 (0.248)	Loss 2.3198 (2.3486)	Acc@1 7.812 (9.733)
Epoch: [0][130/196]	Time 0.529 (0.540)	Data 0.247 (0.247)	Loss 2.3241 (2.3442)	Acc@1 4.688 (11.188)
Epoch: [0][140/196]	Time 0.528 (0.540)	Data 0.246 (0.248)	Loss 2.3194 (2.3459)	Acc@1 9.375 (9.774)
Epoch: [0][140/196]	Time 0.527 (0.539)	Data 0.246 (0.247)	Loss 2.2858 (2.3418)	Acc@1 15.625 (11.115)
Epoch: [0][150/196]	Time 0.535 (0.539)	Data 0.252 (0.248)	Loss 2.3240 (2.3437)	Acc@1 6.250 (9.665)
Epoch: [0][150/196]	Time 0.532 (0.539)	Data 0.247 (0.247)	Loss 2.2927 (2.3397)	Acc@1 14.062 (11.031)
Epoch: [0][160/196]	Time 0.525 (0.538)	Data 0.244 (0.247)	Loss 2.3187 (2.3379)	Acc@1 7.812 (10.879)
Epoch: [0][160/196]	Time 0.536 (0.539)	Data 0.253 (0.248)	Loss 2.3037 (2.3413)	Acc@1 7.812 (9.715)
Epoch: [0][170/196]	Time 0.521 (0.538)	Data 0.244 (0.248)	Loss 2.3248 (2.3394)	Acc@1 7.812 (9.832)
Epoch: [0][170/196]	Time 0.527 (0.537)	Data 0.245 (0.247)	Loss 2.3098 (2.3361)	Acc@1 10.938 (10.837)
Epoch: [0][180/196]	Time 0.523 (0.537)	Data 0.244 (0.247)	Loss 2.3135 (2.3350)	Acc@1 9.375 (10.748)
Epoch: [0][180/196]	Time 0.524 (0.537)	Data 0.245 (0.248)	Loss 2.3017 (2.3377)	Acc@1 9.375 (9.858)
Epoch: [0][190/196]	Time 0.523 (0.537)	Data 0.244 (0.248)	Loss 2.2937 (2.3362)	Acc@1 14.062 (9.841)
Epoch: [0][190/196]	Time 0.526 (0.536)	Data 0.244 (0.247)	Loss 2.3140 (2.3337)	Acc@1 10.938 (10.668)
Test: [0/157]	Time 0.710 (0.710)	Data 0.434 (0.434)	Loss 2.3050 (2.3050)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.721 (0.721)	Data 0.444 (0.444)	Loss 2.3050 (2.3050)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.266 (0.308)	Data 0.006 (0.046)	Loss 2.3188 (2.3064)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.265 (0.314)	Data 0.005 (0.048)	Loss 2.3188 (2.3064)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.267 (0.289)	Data 0.007 (0.027)	Loss 2.3001 (2.3072)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.268 (0.291)	Data 0.007 (0.028)	Loss 2.3001 (2.3072)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.268 (0.282)	Data 0.008 (0.021)	Loss 2.3161 (2.3081)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.268 (0.284)	Data 0.006 (0.021)	Loss 2.3161 (2.3081)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.270 (0.279)	Data 0.009 (0.018)	Loss 2.3031 (2.3079)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.269 (0.280)	Data 0.008 (0.018)	Loss 2.3031 (2.3079)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.268 (0.277)	Data 0.008 (0.016)	Loss 2.3064 (2.3079)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.268 (0.278)	Data 0.007 (0.016)	Loss 2.3064 (2.3079)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.269 (0.275)	Data 0.007 (0.014)	Loss 2.3147 (2.3084)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.266 (0.276)	Data 0.007 (0.014)	Loss 2.3147 (2.3084)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.268 (0.274)	Data 0.008 (0.013)	Loss 2.3160 (2.3079)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.268 (0.275)	Data 0.007 (0.013)	Loss 2.3160 (2.3079)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.269 (0.274)	Data 0.007 (0.012)	Loss 2.3061 (2.3081)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.267 (0.274)	Data 0.007 (0.012)	Loss 2.3061 (2.3081)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.269 (0.273)	Data 0.008 (0.012)	Loss 2.2994 (2.3080)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.269 (0.274)	Data 0.008 (0.012)	Loss 2.2994 (2.3080)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.269 (0.273)	Data 0.008 (0.011)	Loss 2.3005 (2.3077)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.268 (0.273)	Data 0.007 (0.011)	Loss 2.3005 (2.3077)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.271 (0.272)	Data 0.008 (0.011)	Loss 2.3190 (2.3075)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.267 (0.273)	Data 0.006 (0.011)	Loss 2.3190 (2.3075)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.269 (0.272)	Data 0.007 (0.011)	Loss 2.2801 (2.3074)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.270 (0.272)	Data 0.008 (0.011)	Loss 2.2801 (2.3074)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.269 (0.272)	Data 0.008 (0.011)	Loss 2.3254 (2.3077)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.268 (0.272)	Data 0.007 (0.010)	Loss 2.3254 (2.3077)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.271 (0.272)	Data 0.007 (0.010)	Loss 2.2941 (2.3079)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.269 (0.272)	Data 0.007 (0.010)	Loss 2.2941 (2.3079)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.268 (0.272)	Data 0.007 (0.010)	Loss 2.3015 (2.3077)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.271 (0.272)	Data 0.008 (0.010)	Loss 2.3015 (2.3077)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run fine-jazz-5 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/7cmo52td
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_192113-7cmo52td/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.008 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run easy-pine-6 at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter/runs/k6je7fdn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_seq_bn_0.5_0.25_fourier_adapter
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_192113-k6je7fdn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29504 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 29503) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_19:23:59
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 29503)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_192427-i2vsf8iq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-leaf-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_par_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_par_bn_0.5_0.25_fourier_adapter/runs/i2vsf8iq
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_192426-aso7wjxy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-bee-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_par_bn_0.5_0.25_fourier_adapter
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_par_bn_0.5_0.25_fourier_adapter/runs/aso7wjxy
Training 7.642142530040832% of the parameters
starting training
Training 7.642142530040832% of the parameters
starting training
Epoch: [0][0/196]	Time 2.200 (2.200)	Data 0.487 (0.487)	Loss 2.3360 (2.3360)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.258 (2.258)	Data 0.454 (0.454)	Loss 2.3292 (2.3292)	Acc@1 6.250 (6.250)
Epoch: [0][10/196]	Time 0.569 (0.723)	Data 0.279 (0.302)	Loss 2.3125 (2.3469)	Acc@1 12.500 (10.511)Epoch: [0][10/196]	Time 0.558 (0.719)	Data 0.266 (0.291)	Loss 2.3187 (2.3660)	Acc@1 14.062 (10.085)

Epoch: [0][20/196]	Time 0.561 (0.647)	Data 0.269 (0.287)	Loss 2.3561 (2.3618)	Acc@1 9.375 (10.342)
Epoch: [0][20/196]	Time 0.572 (0.645)	Data 0.280 (0.282)	Loss 2.3710 (2.3740)	Acc@1 4.688 (9.524)

==========================================
SLURM_CLUSTER_NAME = paramhimalaya
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = iitmandi
SLURM_JOB_ID = 566901
SLURM_JOB_NAME = Certified-Robustness.job
SLURM_JOB_NODELIST = gpu008
SLURM_JOB_USER = ravihm.scee.iitmandi
SLURM_JOB_UID = 46161
SLURM_JOB_PARTITION = gpu
SLURM_TASK_PID = 19018
SLURM_SUBMIT_DIR = /home/ravihm.scee.iitmandi/Certified-Robustness
SLURM_CPUS_ON_NODE = 1
SLURM_NTASKS = 
SLURM_TASK_PID = 19018
==========================================
/var/share/slurm/d/job566901/slurm_script: line 11: activate: No such file or directory
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 221, in main
    os.makedirs(args.outdir+folder+"/"+str(args.noise_sd))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/scratch/ravihm.scee.iitmandi/models/cifar10/vit/adapters_compacter_0.5/0.25'
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 19126 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 19125) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:43:31
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 19125)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_174409-1rvz8vwo
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_174409-4y64tala
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-thunder-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/1rvz8vwo
wandb: Syncing run jumping-glitter-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/4y64tala
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 9.183 (9.183)	Data 1.124 (1.124)	Loss 1.8132 (1.8132)	Acc@1 43.750 (43.750)Epoch: [0][0/196]	Time 9.179 (9.179)	Data 1.121 (1.121)	Loss 1.7559 (1.7559)	Acc@1 39.062 (39.062)

Epoch: [0][10/196]	Time 0.455 (1.249)	Data 0.192 (0.272)	Loss 1.5118 (1.6007)	Acc@1 53.125 (48.722)
Epoch: [0][10/196]	Time 0.466 (1.250)	Data 0.197 (0.273)	Loss 1.6303 (1.5877)	Acc@1 45.312 (47.585)
Epoch: [0][20/196]	Time 0.453 (0.873)	Data 0.195 (0.236)	Loss 1.6367 (1.5548)	Acc@1 42.188 (49.405)
Epoch: [0][20/196]	Time 0.461 (0.873)	Data 0.196 (0.235)	Loss 1.7017 (1.5923)	Acc@1 45.312 (48.289)
Epoch: [0][30/196]	Time 0.459 (0.740)	Data 0.193 (0.223)	Loss 1.5212 (1.5348)	Acc@1 46.875 (50.101)Epoch: [0][30/196]	Time 0.463 (0.740)	Data 0.195 (0.222)	Loss 1.5559 (1.5753)	Acc@1 50.000 (48.992)

Epoch: [0][40/196]	Time 0.453 (0.672)	Data 0.189 (0.215)	Loss 1.2622 (1.5303)	Acc@1 59.375 (50.724)
Epoch: [0][40/196]	Time 0.460 (0.672)	Data 0.196 (0.216)	Loss 1.3746 (1.4916)	Acc@1 59.375 (51.829)
Epoch: [0][50/196]	Time 0.452 (0.630)	Data 0.191 (0.211)	Loss 1.2179 (1.4548)	Acc@1 62.500 (53.431)
Epoch: [0][50/196]	Time 0.459 (0.630)	Data 0.194 (0.211)	Loss 1.4658 (1.4965)	Acc@1 51.562 (52.175)
Epoch: [0][60/196]	Time 0.466 (0.603)	Data 0.199 (0.209)	Loss 1.1120 (1.4642)	Acc@1 59.375 (53.330)Epoch: [0][60/196]	Time 0.464 (0.602)	Data 0.196 (0.209)	Loss 1.4209 (1.4236)	Acc@1 46.875 (54.431)

Epoch: [0][70/196]	Time 0.461 (0.583)	Data 0.196 (0.207)	Loss 1.1490 (1.4365)	Acc@1 68.750 (54.423)
Epoch: [0][70/196]	Time 0.460 (0.583)	Data 0.194 (0.207)	Loss 1.3490 (1.4018)	Acc@1 62.500 (55.348)
Epoch: [0][80/196]	Time 0.467 (0.568)	Data 0.199 (0.206)	Loss 1.0956 (1.4073)	Acc@1 67.188 (55.517)Epoch: [0][80/196]	Time 0.465 (0.568)	Data 0.195 (0.206)	Loss 1.1059 (1.3720)	Acc@1 64.062 (56.385)

Epoch: [0][90/196]	Time 0.463 (0.556)	Data 0.200 (0.205)	Loss 1.1074 (1.3798)	Acc@1 71.875 (56.611)
Epoch: [0][90/196]	Time 0.465 (0.556)	Data 0.199 (0.204)	Loss 1.1130 (1.3421)	Acc@1 70.312 (57.606)
Epoch: [0][100/196]	Time 0.465 (0.547)	Data 0.199 (0.204)	Loss 1.1218 (1.3583)	Acc@1 65.625 (57.302)
Epoch: [0][100/196]	Time 0.471 (0.547)	Data 0.200 (0.204)	Loss 1.1282 (1.3169)	Acc@1 70.312 (58.663)
Epoch: [0][110/196]	Time 0.462 (0.539)	Data 0.195 (0.203)	Loss 0.9397 (1.2948)	Acc@1 70.312 (59.572)Epoch: [0][110/196]	Time 0.465 (0.539)	Data 0.197 (0.203)	Loss 1.1188 (1.3310)	Acc@1 65.625 (58.390)

Epoch: [0][120/196]	Time 0.462 (0.533)	Data 0.199 (0.203)	Loss 1.1711 (1.3079)	Acc@1 62.500 (59.168)Epoch: [0][120/196]	Time 0.460 (0.533)	Data 0.195 (0.202)	Loss 0.9357 (1.2705)	Acc@1 75.000 (60.653)

Epoch: [0][130/196]	Time 0.459 (0.528)	Data 0.193 (0.202)	Loss 1.1122 (1.2502)	Acc@1 68.750 (61.427)Epoch: [0][130/196]	Time 0.465 (0.528)	Data 0.200 (0.202)	Loss 1.1703 (1.2855)	Acc@1 59.375 (59.876)

Epoch: [0][140/196]	Time 0.466 (0.523)	Data 0.198 (0.202)	Loss 1.2196 (1.2337)	Acc@1 62.500 (62.001)Epoch: [0][140/196]	Time 0.467 (0.523)	Data 0.200 (0.202)	Loss 1.0035 (1.2680)	Acc@1 64.062 (60.483)

Epoch: [0][150/196]	Time 0.461 (0.519)	Data 0.196 (0.202)	Loss 0.9818 (1.2490)	Acc@1 73.438 (61.134)
Epoch: [0][150/196]	Time 0.464 (0.519)	Data 0.198 (0.201)	Loss 0.9526 (1.2162)	Acc@1 68.750 (62.469)
Epoch: [0][160/196]	Time 0.468 (0.516)	Data 0.204 (0.202)	Loss 0.9432 (1.2271)	Acc@1 67.188 (61.957)Epoch: [0][160/196]	Time 0.462 (0.516)	Data 0.197 (0.201)	Loss 1.1099 (1.2013)	Acc@1 68.750 (63.014)

Epoch: [0][170/196]	Time 0.465 (0.513)	Data 0.197 (0.201)	Loss 0.9578 (1.1843)	Acc@1 67.188 (63.551)Epoch: [0][170/196]	Time 0.471 (0.513)	Data 0.204 (0.201)	Loss 0.9024 (1.2089)	Acc@1 71.875 (62.491)

Epoch: [0][180/196]	Time 0.462 (0.510)	Data 0.198 (0.201)	Loss 0.8209 (1.1900)	Acc@1 78.125 (63.182)
Epoch: [0][180/196]	Time 0.463 (0.510)	Data 0.198 (0.201)	Loss 0.7155 (1.1657)	Acc@1 78.125 (64.209)
Epoch: [0][190/196]	Time 0.465 (0.508)	Data 0.201 (0.201)	Loss 0.8852 (1.1715)	Acc@1 73.438 (63.842)
Epoch: [0][190/196]	Time 0.464 (0.508)	Data 0.200 (0.201)	Loss 0.8544 (1.1526)	Acc@1 71.875 (64.660)
Test: [0/157]	Time 0.627 (0.627)	Data 0.385 (0.385)	Loss 0.9699 (0.9699)	Acc@1 71.875 (71.875)
Test: [0/157]	Time 0.666 (0.666)	Data 0.434 (0.434)	Loss 0.9699 (0.9699)	Acc@1 71.875 (71.875)
Test: [10/157]	Time 0.232 (0.271)	Data 0.005 (0.042)	Loss 0.9474 (0.8761)	Acc@1 73.438 (73.864)
Test: [10/157]	Time 0.234 (0.275)	Data 0.005 (0.046)	Loss 0.9474 (0.8761)	Acc@1 73.438 (73.864)
Test: [20/157]	Time 0.238 (0.253)	Data 0.007 (0.025)	Loss 0.8756 (0.8572)	Acc@1 71.875 (74.107)
Test: [20/157]	Time 0.238 (0.257)	Data 0.007 (0.027)	Loss 0.8756 (0.8572)	Acc@1 71.875 (74.107)
Test: [30/157]	Time 0.236 (0.247)	Data 0.006 (0.019)	Loss 0.9074 (0.8614)	Acc@1 75.000 (74.194)
Test: [30/157]	Time 0.235 (0.251)	Data 0.007 (0.021)	Loss 0.9074 (0.8614)	Acc@1 75.000 (74.194)
Test: [40/157]	Time 0.238 (0.244)	Data 0.007 (0.016)	Loss 0.8809 (0.8699)	Acc@1 70.312 (73.704)
Test: [40/157]	Time 0.236 (0.248)	Data 0.007 (0.018)	Loss 0.8809 (0.8699)	Acc@1 70.312 (73.704)
Test: [50/157]	Time 0.234 (0.243)	Data 0.007 (0.015)	Loss 0.7127 (0.8615)	Acc@1 79.688 (74.050)
Test: [50/157]	Time 0.236 (0.246)	Data 0.007 (0.016)	Loss 0.7127 (0.8615)	Acc@1 79.688 (74.050)
Test: [60/157]	Time 0.235 (0.241)	Data 0.007 (0.013)	Loss 0.9034 (0.8690)	Acc@1 68.750 (73.770)
Test: [60/157]	Time 0.238 (0.244)	Data 0.008 (0.014)	Loss 0.9034 (0.8690)	Acc@1 68.750 (73.770)
Test: [70/157]	Time 0.236 (0.240)	Data 0.007 (0.012)	Loss 0.8451 (0.8622)	Acc@1 73.438 (74.142)
Test: [70/157]	Time 0.237 (0.243)	Data 0.007 (0.013)	Loss 0.8451 (0.8622)	Acc@1 73.438 (74.142)
Test: [80/157]	Time 0.236 (0.240)	Data 0.007 (0.012)	Loss 0.9309 (0.8598)	Acc@1 71.875 (74.190)
Test: [80/157]	Time 0.237 (0.243)	Data 0.007 (0.013)	Loss 0.9309 (0.8598)	Acc@1 71.875 (74.190)
Test: [90/157]	Time 0.236 (0.239)	Data 0.008 (0.011)	Loss 0.7041 (0.8589)	Acc@1 76.562 (74.193)
Test: [90/157]	Time 0.238 (0.242)	Data 0.008 (0.012)	Loss 0.7041 (0.8589)	Acc@1 76.562 (74.193)
Test: [100/157]	Time 0.236 (0.239)	Data 0.007 (0.011)	Loss 0.8845 (0.8626)	Acc@1 75.000 (74.180)
Test: [100/157]	Time 0.235 (0.241)	Data 0.007 (0.011)	Loss 0.8845 (0.8626)	Acc@1 75.000 (74.180)
Test: [110/157]	Time 0.235 (0.238)	Data 0.007 (0.011)	Loss 0.8699 (0.8701)	Acc@1 71.875 (73.832)
Test: [110/157]	Time 0.236 (0.241)	Data 0.007 (0.011)	Loss 0.8699 (0.8701)	Acc@1 71.875 (73.832)
Test: [120/157]	Time 0.238 (0.238)	Data 0.008 (0.010)	Loss 0.7995 (0.8677)	Acc@1 79.688 (73.980)
Test: [120/157]	Time 0.236 (0.241)	Data 0.007 (0.011)	Loss 0.7995 (0.8677)	Acc@1 79.688 (73.980)
Test: [130/157]	Time 0.234 (0.238)	Data 0.007 (0.010)	Loss 1.0638 (0.8681)	Acc@1 65.625 (74.022)
Test: [130/157]	Time 0.236 (0.240)	Data 0.006 (0.010)	Loss 1.0638 (0.8681)	Acc@1 65.625 (74.022)
Test: [140/157]	Time 0.236 (0.238)	Data 0.008 (0.010)	Loss 0.7942 (0.8700)	Acc@1 79.688 (74.003)
Test: [140/157]	Time 0.237 (0.240)	Data 0.007 (0.010)	Loss 0.7942 (0.8700)	Acc@1 79.688 (74.003)
Test: [150/157]	Time 0.235 (0.238)	Data 0.007 (0.010)	Loss 0.7607 (0.8705)	Acc@1 79.688 (73.893)
Test: [150/157]	Time 0.238 (0.240)	Data 0.007 (0.010)	Loss 0.7607 (0.8705)	Acc@1 79.688 (73.893)
New Best Found: 73.8%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 73.8%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run jumping-glitter-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/4y64tala
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_174409-4y64tala/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run likely-thunder-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/1rvz8vwo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_174409-1rvz8vwo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 19240 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 19241) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:47:00
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 19241)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_174729-nmyfqv3f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-dawn-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/nmyfqv3f
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_174729-b59x7v9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-dew-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/b59x7v9w
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.081 (2.081)	Data 0.411 (0.411)	Loss 1.8132 (1.8132)	Acc@1 43.750 (43.750)
Epoch: [0][0/196]	Time 2.212 (2.212)	Data 0.543 (0.543)	Loss 1.7559 (1.7559)	Acc@1 39.062 (39.062)
Epoch: [0][10/196]	Time 0.461 (0.619)	Data 0.199 (0.229)	Loss 1.0176 (1.1448)	Acc@1 68.750 (63.636)
Epoch: [0][10/196]	Time 0.463 (0.611)	Data 0.194 (0.215)	Loss 0.6694 (1.1094)	Acc@1 79.688 (66.477)
Epoch: [0][20/196]	Time 0.451 (0.543)	Data 0.191 (0.213)	Loss 0.8577 (0.8998)	Acc@1 78.125 (71.280)
Epoch: [0][20/196]	Time 0.455 (0.539)	Data 0.190 (0.204)	Loss 0.8283 (0.9153)	Acc@1 76.562 (73.140)
Epoch: [0][30/196]	Time 0.458 (0.516)	Data 0.192 (0.207)	Loss 0.7912 (0.7783)	Acc@1 79.688 (75.554)
Epoch: [0][30/196]	Time 0.458 (0.513)	Data 0.192 (0.201)	Loss 0.5391 (0.7832)	Acc@1 82.812 (76.663)
Epoch: [0][40/196]	Time 0.461 (0.503)	Data 0.192 (0.204)	Loss 0.4002 (0.7042)	Acc@1 90.625 (77.973)
Epoch: [0][40/196]	Time 0.464 (0.500)	Data 0.196 (0.199)	Loss 0.3584 (0.7168)	Acc@1 89.062 (78.620)
Epoch: [0][50/196]	Time 0.458 (0.492)	Data 0.194 (0.198)	Loss 0.5990 (0.6732)	Acc@1 79.688 (79.871)Epoch: [0][50/196]	Time 0.459 (0.494)	Data 0.192 (0.202)	Loss 0.2943 (0.6472)	Acc@1 93.750 (79.933)

Epoch: [0][60/196]	Time 0.460 (0.489)	Data 0.194 (0.201)	Loss 0.5694 (0.6080)	Acc@1 84.375 (81.122)Epoch: [0][60/196]	Time 0.462 (0.487)	Data 0.198 (0.198)	Loss 0.4352 (0.6414)	Acc@1 84.375 (80.763)

Epoch: [0][70/196]	Time 0.459 (0.485)	Data 0.194 (0.200)	Loss 0.5269 (0.5796)	Acc@1 82.812 (81.954)
Epoch: [0][70/196]	Time 0.458 (0.484)	Data 0.193 (0.197)	Loss 0.3137 (0.6017)	Acc@1 89.062 (81.778)
Epoch: [0][80/196]	Time 0.460 (0.481)	Data 0.194 (0.197)	Loss 0.3956 (0.5755)	Acc@1 84.375 (82.350)
Epoch: [0][80/196]	Time 0.461 (0.482)	Data 0.193 (0.199)	Loss 0.4306 (0.5521)	Acc@1 89.062 (82.774)
Epoch: [0][90/196]	Time 0.459 (0.480)	Data 0.193 (0.198)	Loss 0.4029 (0.5337)	Acc@1 85.938 (83.328)
Epoch: [0][90/196]	Time 0.461 (0.479)	Data 0.196 (0.197)	Loss 0.6362 (0.5532)	Acc@1 79.688 (82.864)
Epoch: [0][100/196]	Time 0.454 (0.478)	Data 0.193 (0.198)	Loss 0.3624 (0.5172)	Acc@1 85.938 (83.663)
Epoch: [0][100/196]	Time 0.454 (0.477)	Data 0.194 (0.197)	Loss 0.4542 (0.5415)	Acc@1 87.500 (83.246)
Epoch: [0][110/196]	Time 0.457 (0.476)	Data 0.192 (0.198)	Loss 0.2736 (0.4998)	Acc@1 87.500 (84.234)
Epoch: [0][110/196]	Time 0.456 (0.475)	Data 0.191 (0.196)	Loss 0.5277 (0.5260)	Acc@1 85.938 (83.756)
Epoch: [0][120/196]	Time 0.460 (0.475)	Data 0.194 (0.197)	Loss 0.3512 (0.4855)	Acc@1 84.375 (84.633)
Epoch: [0][120/196]	Time 0.460 (0.474)	Data 0.195 (0.196)	Loss 0.2645 (0.5149)	Acc@1 93.750 (84.117)
Epoch: [0][130/196]	Time 0.460 (0.474)	Data 0.193 (0.197)	Loss 0.3237 (0.4761)	Acc@1 85.938 (84.912)
Epoch: [0][130/196]	Time 0.460 (0.473)	Data 0.192 (0.196)	Loss 0.5204 (0.5055)	Acc@1 84.375 (84.458)
Epoch: [0][140/196]	Time 0.459 (0.472)	Data 0.194 (0.196)	Loss 0.3362 (0.5001)	Acc@1 90.625 (84.652)
Epoch: [0][140/196]	Time 0.467 (0.473)	Data 0.198 (0.197)	Loss 0.4148 (0.4672)	Acc@1 87.500 (85.217)
Epoch: [0][150/196]	Time 0.460 (0.472)	Data 0.194 (0.196)	Loss 0.3912 (0.4927)	Acc@1 87.500 (84.830)
Epoch: [0][150/196]	Time 0.461 (0.472)	Data 0.194 (0.197)	Loss 0.3521 (0.4601)	Acc@1 89.062 (85.410)
Epoch: [0][160/196]	Time 0.460 (0.471)	Data 0.192 (0.196)	Loss 0.3464 (0.4566)	Acc@1 89.062 (85.491)
Epoch: [0][160/196]	Time 0.461 (0.471)	Data 0.195 (0.196)	Loss 0.1916 (0.4811)	Acc@1 93.750 (85.122)
Epoch: [0][170/196]	Time 0.456 (0.471)	Data 0.192 (0.196)	Loss 0.4154 (0.4480)	Acc@1 85.938 (85.718)
Epoch: [0][170/196]	Time 0.459 (0.470)	Data 0.193 (0.196)	Loss 0.1223 (0.4749)	Acc@1 98.438 (85.298)
Epoch: [0][180/196]	Time 0.461 (0.470)	Data 0.197 (0.196)	Loss 0.3209 (0.4662)	Acc@1 90.625 (85.549)Epoch: [0][180/196]	Time 0.459 (0.470)	Data 0.194 (0.196)	Loss 0.4128 (0.4418)	Acc@1 87.500 (85.946)

Epoch: [0][190/196]	Time 0.463 (0.470)	Data 0.197 (0.196)	Loss 0.3042 (0.4396)	Acc@1 87.500 (85.978)
Epoch: [0][190/196]	Time 0.460 (0.469)	Data 0.194 (0.196)	Loss 0.2591 (0.4592)	Acc@1 93.750 (85.774)
Test: [0/157]	Time 0.658 (0.658)	Data 0.411 (0.411)	Loss 0.5454 (0.5454)	Acc@1 85.938 (85.938)
Test: [0/157]	Time 0.671 (0.671)	Data 0.428 (0.428)	Loss 0.5454 (0.5454)	Acc@1 85.938 (85.938)
Test: [10/157]	Time 0.236 (0.276)	Data 0.006 (0.046)	Loss 0.4726 (0.3586)	Acc@1 87.500 (89.631)
Test: [10/157]	Time 0.234 (0.276)	Data 0.006 (0.045)	Loss 0.4726 (0.3586)	Acc@1 87.500 (89.631)
Test: [20/157]	Time 0.237 (0.257)	Data 0.008 (0.027)	Loss 0.2239 (0.3130)	Acc@1 89.062 (90.327)
Test: [20/157]	Time 0.237 (0.257)	Data 0.009 (0.027)	Loss 0.2239 (0.3130)	Acc@1 89.062 (90.327)
Test: [30/157]	Time 0.234 (0.250)	Data 0.006 (0.021)	Loss 0.4085 (0.3291)	Acc@1 89.062 (89.415)
Test: [30/157]	Time 0.236 (0.251)	Data 0.008 (0.021)	Loss 0.4085 (0.3291)	Acc@1 89.062 (89.415)
Test: [40/157]	Time 0.238 (0.246)	Data 0.006 (0.017)	Loss 0.5774 (0.3462)	Acc@1 78.125 (88.948)
Test: [40/157]	Time 0.238 (0.247)	Data 0.008 (0.018)	Loss 0.5774 (0.3462)	Acc@1 78.125 (88.948)
Test: [50/157]	Time 0.237 (0.245)	Data 0.007 (0.016)	Loss 0.3295 (0.3370)	Acc@1 89.062 (89.001)
Test: [50/157]	Time 0.237 (0.245)	Data 0.008 (0.016)	Loss 0.3295 (0.3370)	Acc@1 89.062 (89.001)
Test: [60/157]	Time 0.237 (0.244)	Data 0.008 (0.014)	Loss 0.2042 (0.3360)	Acc@1 92.188 (89.011)
Test: [60/157]	Time 0.237 (0.244)	Data 0.007 (0.014)	Loss 0.2042 (0.3360)	Acc@1 92.188 (89.011)
Test: [70/157]	Time 0.234 (0.243)	Data 0.007 (0.013)	Loss 0.3776 (0.3333)	Acc@1 85.938 (89.107)
Test: [70/157]	Time 0.237 (0.243)	Data 0.008 (0.013)	Loss 0.3776 (0.3333)	Acc@1 85.938 (89.107)
Test: [80/157]	Time 0.235 (0.242)	Data 0.006 (0.013)	Loss 0.2649 (0.3424)	Acc@1 89.062 (88.870)
Test: [80/157]	Time 0.239 (0.242)	Data 0.008 (0.013)	Loss 0.2649 (0.3424)	Acc@1 89.062 (88.870)
Test: [90/157]	Time 0.239 (0.241)	Data 0.009 (0.012)	Loss 0.3356 (0.3442)	Acc@1 89.062 (88.753)
Test: [90/157]	Time 0.237 (0.242)	Data 0.008 (0.012)	Loss 0.3356 (0.3442)	Acc@1 89.062 (88.753)
Test: [100/157]	Time 0.237 (0.241)	Data 0.008 (0.012)	Loss 0.3590 (0.3408)	Acc@1 84.375 (88.877)
Test: [100/157]	Time 0.236 (0.241)	Data 0.007 (0.012)	Loss 0.3590 (0.3408)	Acc@1 84.375 (88.877)
Test: [110/157]	Time 0.235 (0.241)	Data 0.007 (0.011)	Loss 0.6154 (0.3473)	Acc@1 81.250 (88.640)
Test: [110/157]	Time 0.236 (0.241)	Data 0.008 (0.011)	Loss 0.6154 (0.3473)	Acc@1 81.250 (88.640)
Test: [120/157]	Time 0.238 (0.240)	Data 0.007 (0.011)	Loss 0.1876 (0.3442)	Acc@1 92.188 (88.740)
Test: [120/157]	Time 0.237 (0.240)	Data 0.008 (0.011)	Loss 0.1876 (0.3442)	Acc@1 92.188 (88.740)
Test: [130/157]	Time 0.240 (0.240)	Data 0.009 (0.011)	Loss 0.4123 (0.3457)	Acc@1 87.500 (88.717)
Test: [130/157]	Time 0.238 (0.240)	Data 0.009 (0.011)	Loss 0.4123 (0.3457)	Acc@1 87.500 (88.717)
Test: [140/157]	Time 0.237 (0.240)	Data 0.008 (0.011)	Loss 0.3126 (0.3486)	Acc@1 90.625 (88.641)
Test: [140/157]	Time 0.239 (0.240)	Data 0.009 (0.011)	Loss 0.3126 (0.3486)	Acc@1 90.625 (88.641)
Test: [150/157]	Time 0.236 (0.240)	Data 0.009 (0.011)	Loss 0.1608 (0.3457)	Acc@1 95.312 (88.700)
Test: [150/157]	Time 0.251 (0.240)	Data 0.014 (0.011)	Loss 0.1608 (0.3457)	Acc@1 95.312 (88.700)
New Best Found: 88.69%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 88.69%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run dashing-dawn-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/nmyfqv3f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_174729-nmyfqv3f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run easy-dew-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/b59x7v9w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_174729-b59x7v9w/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20986 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 20985) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:50:01
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 20985)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175031-uqmqvtbk
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175031-6w3os789
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-capybara-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/uqmqvtbk
wandb: Syncing run cool-hill-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/6w3os789
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.202 (2.202)	Data 0.559 (0.559)	Loss 1.8132 (1.8132)	Acc@1 43.750 (43.750)
Epoch: [0][0/196]	Time 2.125 (2.125)	Data 0.479 (0.479)	Loss 1.7559 (1.7559)	Acc@1 39.062 (39.062)
Epoch: [0][10/196]	Time 0.458 (0.620)	Data 0.191 (0.227)	Loss 1.5118 (1.6007)	Acc@1 53.125 (48.722)
Epoch: [0][10/196]	Time 0.452 (0.611)	Data 0.187 (0.219)	Loss 1.6303 (1.5877)	Acc@1 45.312 (47.585)
Epoch: [0][20/196]	Time 0.464 (0.544)	Data 0.199 (0.210)	Loss 1.7017 (1.5923)	Acc@1 45.312 (48.289)
Epoch: [0][20/196]	Time 0.461 (0.540)	Data 0.192 (0.208)	Loss 1.6367 (1.5548)	Acc@1 42.188 (49.405)
Epoch: [0][30/196]	Time 0.463 (0.517)	Data 0.196 (0.205)	Loss 1.5559 (1.5753)	Acc@1 50.000 (48.992)
Epoch: [0][30/196]	Time 0.468 (0.514)	Data 0.199 (0.204)	Loss 1.5212 (1.5348)	Acc@1 46.875 (50.101)
Epoch: [0][40/196]	Time 0.458 (0.501)	Data 0.194 (0.202)	Loss 1.3746 (1.4916)	Acc@1 59.375 (51.829)
Epoch: [0][40/196]	Time 0.465 (0.504)	Data 0.197 (0.203)	Loss 1.2622 (1.5303)	Acc@1 59.375 (50.724)
Epoch: [0][50/196]	Time 0.460 (0.496)	Data 0.196 (0.201)	Loss 1.4658 (1.4965)	Acc@1 51.562 (52.175)
Epoch: [0][50/196]	Time 0.469 (0.494)	Data 0.200 (0.201)	Loss 1.2179 (1.4548)	Acc@1 62.500 (53.431)
Epoch: [0][60/196]	Time 0.462 (0.490)	Data 0.195 (0.200)	Loss 1.1120 (1.4642)	Acc@1 59.375 (53.330)
Epoch: [0][60/196]	Time 0.480 (0.489)	Data 0.209 (0.200)	Loss 1.4209 (1.4236)	Acc@1 46.875 (54.431)
Epoch: [0][70/196]	Time 0.460 (0.486)	Data 0.196 (0.200)	Loss 1.1490 (1.4365)	Acc@1 68.750 (54.423)
Epoch: [0][70/196]	Time 0.459 (0.485)	Data 0.193 (0.199)	Loss 1.3490 (1.4018)	Acc@1 62.500 (55.348)
Epoch: [0][80/196]	Time 0.465 (0.483)	Data 0.198 (0.200)	Loss 1.0956 (1.4073)	Acc@1 67.188 (55.517)
Epoch: [0][80/196]	Time 0.466 (0.482)	Data 0.197 (0.198)	Loss 1.1059 (1.3720)	Acc@1 64.062 (56.385)
Epoch: [0][90/196]	Time 0.461 (0.480)	Data 0.196 (0.198)	Loss 1.1130 (1.3421)	Acc@1 70.312 (57.606)Epoch: [0][90/196]	Time 0.462 (0.481)	Data 0.195 (0.199)	Loss 1.1074 (1.3798)	Acc@1 71.875 (56.611)

Epoch: [0][100/196]	Time 0.458 (0.478)	Data 0.197 (0.198)	Loss 1.1282 (1.3169)	Acc@1 70.312 (58.663)
Epoch: [0][100/196]	Time 0.462 (0.479)	Data 0.195 (0.199)	Loss 1.1218 (1.3583)	Acc@1 65.625 (57.302)
Epoch: [0][110/196]	Time 0.460 (0.478)	Data 0.200 (0.199)	Loss 1.1188 (1.3310)	Acc@1 65.625 (58.390)
Epoch: [0][110/196]	Time 0.461 (0.477)	Data 0.195 (0.198)	Loss 0.9397 (1.2948)	Acc@1 70.312 (59.572)
Epoch: [0][120/196]	Time 0.465 (0.476)	Data 0.197 (0.198)	Loss 1.1711 (1.3079)	Acc@1 62.500 (59.168)
Epoch: [0][120/196]	Time 0.464 (0.476)	Data 0.196 (0.197)	Loss 0.9357 (1.2705)	Acc@1 75.000 (60.653)
Epoch: [0][130/196]	Time 0.463 (0.475)	Data 0.196 (0.198)	Loss 1.1703 (1.2855)	Acc@1 59.375 (59.876)
Epoch: [0][130/196]	Time 0.462 (0.474)	Data 0.196 (0.197)	Loss 1.1122 (1.2502)	Acc@1 68.750 (61.427)
Epoch: [0][140/196]	Time 0.461 (0.474)	Data 0.196 (0.198)	Loss 1.0035 (1.2680)	Acc@1 64.062 (60.483)
Epoch: [0][140/196]	Time 0.462 (0.474)	Data 0.195 (0.197)	Loss 1.2196 (1.2337)	Acc@1 62.500 (62.001)
Epoch: [0][150/196]	Time 0.462 (0.473)	Data 0.195 (0.198)	Loss 0.9818 (1.2490)	Acc@1 73.438 (61.134)
Epoch: [0][150/196]	Time 0.461 (0.473)	Data 0.193 (0.197)	Loss 0.9526 (1.2162)	Acc@1 68.750 (62.469)
Epoch: [0][160/196]	Time 0.461 (0.473)	Data 0.195 (0.198)	Loss 0.9432 (1.2271)	Acc@1 67.188 (61.957)
Epoch: [0][160/196]	Time 0.462 (0.472)	Data 0.196 (0.197)	Loss 1.1099 (1.2013)	Acc@1 68.750 (63.014)
Epoch: [0][170/196]	Time 0.462 (0.472)	Data 0.195 (0.198)	Loss 0.9024 (1.2089)	Acc@1 71.875 (62.491)
Epoch: [0][170/196]	Time 0.465 (0.472)	Data 0.197 (0.197)	Loss 0.9578 (1.1843)	Acc@1 67.188 (63.551)
Epoch: [0][180/196]	Time 0.463 (0.472)	Data 0.197 (0.198)	Loss 0.8209 (1.1900)	Acc@1 78.125 (63.182)
Epoch: [0][180/196]	Time 0.461 (0.471)	Data 0.197 (0.197)	Loss 0.7155 (1.1657)	Acc@1 78.125 (64.209)
Epoch: [0][190/196]	Time 0.455 (0.471)	Data 0.192 (0.198)	Loss 0.8852 (1.1715)	Acc@1 73.438 (63.842)
Epoch: [0][190/196]	Time 0.458 (0.471)	Data 0.193 (0.197)	Loss 0.8544 (1.1526)	Acc@1 71.875 (64.660)
Test: [0/157]	Time 0.649 (0.649)	Data 0.403 (0.403)	Loss 0.9699 (0.9699)	Acc@1 71.875 (71.875)
Test: [0/157]	Time 0.646 (0.646)	Data 0.403 (0.403)	Loss 0.9699 (0.9699)	Acc@1 71.875 (71.875)
Test: [10/157]	Time 0.234 (0.276)	Data 0.005 (0.045)	Loss 0.9474 (0.8761)	Acc@1 73.438 (73.864)
Test: [10/157]	Time 0.234 (0.276)	Data 0.005 (0.044)	Loss 0.9474 (0.8761)	Acc@1 73.438 (73.864)
Test: [20/157]	Time 0.239 (0.257)	Data 0.009 (0.027)	Loss 0.8756 (0.8572)	Acc@1 71.875 (74.107)
Test: [20/157]	Time 0.237 (0.258)	Data 0.008 (0.027)	Loss 0.8756 (0.8572)	Acc@1 71.875 (74.107)
Test: [30/157]	Time 0.240 (0.251)	Data 0.009 (0.021)	Loss 0.9074 (0.8614)	Acc@1 75.000 (74.194)
Test: [30/157]	Time 0.240 (0.252)	Data 0.010 (0.021)	Loss 0.9074 (0.8614)	Acc@1 75.000 (74.194)
Test: [40/157]	Time 0.240 (0.248)	Data 0.008 (0.018)	Loss 0.8809 (0.8699)	Acc@1 70.312 (73.704)
Test: [40/157]	Time 0.237 (0.248)	Data 0.007 (0.017)	Loss 0.8809 (0.8699)	Acc@1 70.312 (73.704)
Test: [50/157]	Time 0.237 (0.246)	Data 0.008 (0.016)	Loss 0.7127 (0.8615)	Acc@1 79.688 (74.050)
Test: [50/157]	Time 0.236 (0.246)	Data 0.007 (0.016)	Loss 0.7127 (0.8615)	Acc@1 79.688 (74.050)
Test: [60/157]	Time 0.236 (0.245)	Data 0.007 (0.015)	Loss 0.9034 (0.8690)	Acc@1 68.750 (73.770)
Test: [60/157]	Time 0.238 (0.245)	Data 0.007 (0.014)	Loss 0.9034 (0.8690)	Acc@1 68.750 (73.770)
Test: [70/157]	Time 0.236 (0.244)	Data 0.007 (0.014)	Loss 0.8451 (0.8622)	Acc@1 73.438 (74.142)
Test: [70/157]	Time 0.240 (0.244)	Data 0.009 (0.013)	Loss 0.8451 (0.8622)	Acc@1 73.438 (74.142)
Test: [80/157]	Time 0.244 (0.243)	Data 0.011 (0.013)	Loss 0.9309 (0.8598)	Acc@1 71.875 (74.190)
Test: [80/157]	Time 0.236 (0.243)	Data 0.006 (0.012)	Loss 0.9309 (0.8598)	Acc@1 71.875 (74.190)
Test: [90/157]	Time 0.236 (0.242)	Data 0.007 (0.012)	Loss 0.7041 (0.8589)	Acc@1 76.562 (74.193)
Test: [90/157]	Time 0.236 (0.243)	Data 0.007 (0.012)	Loss 0.7041 (0.8589)	Acc@1 76.562 (74.193)
Test: [100/157]	Time 0.237 (0.242)	Data 0.008 (0.012)	Loss 0.8845 (0.8626)	Acc@1 75.000 (74.180)
Test: [100/157]	Time 0.240 (0.242)	Data 0.008 (0.011)	Loss 0.8845 (0.8626)	Acc@1 75.000 (74.180)
Test: [110/157]	Time 0.239 (0.242)	Data 0.008 (0.012)	Loss 0.8699 (0.8701)	Acc@1 71.875 (73.832)
Test: [110/157]	Time 0.240 (0.242)	Data 0.009 (0.011)	Loss 0.8699 (0.8701)	Acc@1 71.875 (73.832)
Test: [120/157]	Time 0.237 (0.241)	Data 0.008 (0.011)	Loss 0.7995 (0.8677)	Acc@1 79.688 (73.980)
Test: [120/157]	Time 0.236 (0.241)	Data 0.007 (0.011)	Loss 0.7995 (0.8677)	Acc@1 79.688 (73.980)
Test: [130/157]	Time 0.236 (0.241)	Data 0.008 (0.011)	Loss 1.0638 (0.8681)	Acc@1 65.625 (74.022)
Test: [130/157]	Time 0.236 (0.241)	Data 0.007 (0.011)	Loss 1.0638 (0.8681)	Acc@1 65.625 (74.022)
Test: [140/157]	Time 0.237 (0.241)	Data 0.008 (0.011)	Loss 0.7942 (0.8700)	Acc@1 79.688 (74.003)
Test: [140/157]	Time 0.236 (0.241)	Data 0.007 (0.010)	Loss 0.7942 (0.8700)	Acc@1 79.688 (74.003)
Test: [150/157]	Time 0.235 (0.240)	Data 0.006 (0.011)	Loss 0.7607 (0.8705)	Acc@1 79.688 (73.893)
Test: [150/157]	Time 0.237 (0.241)	Data 0.007 (0.010)	Loss 0.7607 (0.8705)	Acc@1 79.688 (73.893)
New Best Found: 73.8%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 73.8%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run youthful-capybara-8 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/uqmqvtbk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175031-uqmqvtbk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run cool-hill-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25/runs/6w3os789
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175031-6w3os789/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22697 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 22698) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:53:06
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 22698)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175336-g27x23to
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-field-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/g27x23to
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175336-iertf8vv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-paper-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/iertf8vv
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 3.530 (3.530)	Data 0.808 (0.808)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][0/196]	Time 3.169 (3.169)	Data 0.471 (0.471)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.486 (0.731)	Data 0.196 (0.221)	Loss 2.3075 (2.3042)	Acc@1 9.375 (10.795)
Epoch: [0][10/196]	Time 0.491 (0.765)	Data 0.202 (0.254)	Loss 2.3104 (2.3040)	Acc@1 3.125 (8.097)
Epoch: [0][20/196]	Time 0.496 (0.635)	Data 0.206 (0.229)	Loss 2.2975 (2.3051)	Acc@1 12.500 (9.301)
Epoch: [0][20/196]	Time 0.495 (0.618)	Data 0.202 (0.212)	Loss 2.2960 (2.3044)	Acc@1 14.062 (11.235)
Epoch: [0][30/196]	Time 0.486 (0.589)	Data 0.199 (0.221)	Loss 2.3025 (2.3043)	Acc@1 7.812 (9.980)
Epoch: [0][30/196]	Time 0.494 (0.577)	Data 0.200 (0.208)	Loss 2.3079 (2.3042)	Acc@1 10.938 (10.988)
Epoch: [0][40/196]	Time 0.491 (0.565)	Data 0.202 (0.216)	Loss 2.3170 (2.3041)	Acc@1 9.375 (9.985)Epoch: [0][40/196]	Time 0.489 (0.556)	Data 0.200 (0.206)	Loss 2.3066 (2.3044)	Acc@1 3.125 (10.518)

Epoch: [0][50/196]	Time 0.493 (0.543)	Data 0.201 (0.205)	Loss 2.3177 (2.3051)	Acc@1 7.812 (10.018)
Epoch: [0][50/196]	Time 0.493 (0.550)	Data 0.202 (0.213)	Loss 2.3063 (2.3044)	Acc@1 9.375 (9.926)
Epoch: [0][60/196]	Time 0.486 (0.534)	Data 0.198 (0.204)	Loss 2.3067 (2.3050)	Acc@1 7.812 (9.810)
Epoch: [0][60/196]	Time 0.491 (0.541)	Data 0.200 (0.211)	Loss 2.3058 (2.3047)	Acc@1 9.375 (9.836)
Epoch: [0][70/196]	Time 0.491 (0.528)	Data 0.200 (0.204)	Loss 2.2874 (2.3047)	Acc@1 9.375 (10.189)
Epoch: [0][70/196]	Time 0.497 (0.534)	Data 0.207 (0.210)	Loss 2.2917 (2.3040)	Acc@1 4.688 (9.815)
Epoch: [0][80/196]	Time 0.490 (0.528)	Data 0.200 (0.209)	Loss 2.3117 (2.3052)	Acc@1 7.812 (9.587)
Epoch: [0][80/196]	Time 0.493 (0.524)	Data 0.201 (0.203)	Loss 2.2964 (2.3051)	Acc@1 6.250 (10.031)
Epoch: [0][90/196]	Time 0.488 (0.524)	Data 0.199 (0.208)	Loss 2.3125 (2.3055)	Acc@1 10.938 (9.512)
Epoch: [0][90/196]	Time 0.489 (0.520)	Data 0.197 (0.203)	Loss 2.3070 (2.3054)	Acc@1 6.250 (9.976)
Epoch: [0][100/196]	Time 0.486 (0.517)	Data 0.197 (0.202)	Loss 2.3081 (2.3053)	Acc@1 6.250 (10.040)
Epoch: [0][100/196]	Time 0.490 (0.521)	Data 0.199 (0.207)	Loss 2.3091 (2.3057)	Acc@1 17.188 (9.344)
Epoch: [0][110/196]	Time 0.492 (0.518)	Data 0.201 (0.207)	Loss 2.3027 (2.3059)	Acc@1 14.062 (9.445)
Epoch: [0][110/196]	Time 0.491 (0.515)	Data 0.199 (0.202)	Loss 2.2969 (2.3051)	Acc@1 12.500 (10.008)
Epoch: [0][120/196]	Time 0.486 (0.516)	Data 0.198 (0.206)	Loss 2.3142 (2.3061)	Acc@1 9.375 (9.336)
Epoch: [0][120/196]	Time 0.489 (0.513)	Data 0.197 (0.202)	Loss 2.2990 (2.3055)	Acc@1 1.562 (9.788)
Epoch: [0][130/196]	Time 0.489 (0.511)	Data 0.199 (0.202)	Loss 2.3167 (2.3058)	Acc@1 3.125 (9.649)
Epoch: [0][130/196]	Time 0.494 (0.514)	Data 0.204 (0.206)	Loss 2.3225 (2.3061)	Acc@1 6.250 (9.423)
Epoch: [0][140/196]	Time 0.491 (0.512)	Data 0.200 (0.205)	Loss 2.3131 (2.3061)	Acc@1 9.375 (9.497)Epoch: [0][140/196]	Time 0.490 (0.510)	Data 0.199 (0.201)	Loss 2.2839 (2.3058)	Acc@1 12.500 (9.630)

Epoch: [0][150/196]	Time 0.490 (0.508)	Data 0.200 (0.201)	Loss 2.3064 (2.3058)	Acc@1 6.250 (9.613)Epoch: [0][150/196]	Time 0.493 (0.511)	Data 0.202 (0.205)	Loss 2.3148 (2.3062)	Acc@1 7.812 (9.489)

Epoch: [0][160/196]	Time 0.489 (0.507)	Data 0.199 (0.201)	Loss 2.3082 (2.3058)	Acc@1 7.812 (9.491)
Epoch: [0][160/196]	Time 0.492 (0.509)	Data 0.202 (0.205)	Loss 2.2975 (2.3059)	Acc@1 7.812 (9.550)
Epoch: [0][170/196]	Time 0.491 (0.506)	Data 0.199 (0.201)	Loss 2.3150 (2.3058)	Acc@1 10.938 (9.512)
Epoch: [0][170/196]	Time 0.496 (0.508)	Data 0.209 (0.205)	Loss 2.3261 (2.3060)	Acc@1 7.812 (9.695)
Epoch: [0][180/196]	Time 0.487 (0.507)	Data 0.199 (0.204)	Loss 2.3013 (2.3061)	Acc@1 9.375 (9.703)
Epoch: [0][180/196]	Time 0.489 (0.505)	Data 0.199 (0.201)	Loss 2.3097 (2.3060)	Acc@1 9.375 (9.461)
Epoch: [0][190/196]	Time 0.487 (0.506)	Data 0.201 (0.204)	Loss 2.3053 (2.3063)	Acc@1 14.062 (9.612)
Epoch: [0][190/196]	Time 0.489 (0.504)	Data 0.200 (0.201)	Loss 2.3106 (2.3060)	Acc@1 10.938 (9.465)
Test: [0/157]	Time 0.687 (0.687)	Data 0.443 (0.443)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.697 (0.697)	Data 0.449 (0.449)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.265 (0.291)	Data 0.012 (0.047)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.249 (0.292)	Data 0.005 (0.048)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.249 (0.271)	Data 0.007 (0.028)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.254 (0.272)	Data 0.009 (0.028)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.247 (0.264)	Data 0.007 (0.021)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.251 (0.266)	Data 0.007 (0.022)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.251 (0.261)	Data 0.007 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.251 (0.262)	Data 0.008 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.249 (0.259)	Data 0.007 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.248 (0.259)	Data 0.006 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.248 (0.257)	Data 0.007 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.247 (0.258)	Data 0.007 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.251 (0.256)	Data 0.007 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.248 (0.256)	Data 0.006 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.251 (0.255)	Data 0.007 (0.012)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.250 (0.255)	Data 0.007 (0.012)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.252 (0.255)	Data 0.008 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.250 (0.255)	Data 0.007 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.249 (0.254)	Data 0.007 (0.011)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.251 (0.254)	Data 0.007 (0.011)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.250 (0.254)	Data 0.006 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.249 (0.254)	Data 0.007 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.249 (0.253)	Data 0.008 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.249 (0.254)	Data 0.007 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.248 (0.253)	Data 0.006 (0.010)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.251 (0.253)	Data 0.008 (0.010)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.248 (0.253)	Data 0.007 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run fearless-paper-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/iertf8vv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175336-iertf8vv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run fragrant-field-1 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/g27x23to
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175336-g27x23to/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 24407 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24406) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:56:22
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 24406)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175651-hf4psren
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-star-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/hf4psren
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_175652-0z14y4b4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-firebrand-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/0z14y4b4
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.151 (2.151)	Data 0.458 (0.458)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.235 (2.235)	Data 0.491 (0.491)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][10/196]	Time 0.497 (0.656)	Data 0.206 (0.238)	Loss 2.3012 (2.3025)	Acc@1 12.500 (10.227)
Epoch: [0][10/196]	Time 0.492 (0.649)	Data 0.200 (0.226)	Loss 2.3026 (2.3029)	Acc@1 7.812 (9.659)
Epoch: [0][20/196]	Time 0.488 (0.577)	Data 0.201 (0.221)	Loss 2.3039 (2.3027)	Acc@1 4.688 (9.673)Epoch: [0][20/196]	Time 0.488 (0.574)	Data 0.201 (0.214)	Loss 2.3024 (2.3027)	Acc@1 12.500 (9.896)

Epoch: [0][30/196]	Time 0.490 (0.549)	Data 0.201 (0.214)	Loss 2.3027 (2.3027)	Acc@1 12.500 (9.173)
Epoch: [0][30/196]	Time 0.492 (0.547)	Data 0.202 (0.210)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.181)
Epoch: [0][40/196]	Time 0.488 (0.533)	Data 0.199 (0.208)	Loss 2.3024 (2.3027)	Acc@1 7.812 (9.794)
Epoch: [0][40/196]	Time 0.491 (0.534)	Data 0.201 (0.211)	Loss 2.3029 (2.3026)	Acc@1 9.375 (9.604)
Epoch: [0][50/196]	Time 0.486 (0.524)	Data 0.199 (0.206)	Loss 2.3026 (2.3027)	Acc@1 12.500 (9.804)
Epoch: [0][50/196]	Time 0.490 (0.526)	Data 0.199 (0.209)	Loss 2.3037 (2.3026)	Acc@1 9.375 (9.835)
Epoch: [0][60/196]	Time 0.489 (0.519)	Data 0.201 (0.206)	Loss 2.3038 (2.3027)	Acc@1 6.250 (9.836)
Epoch: [0][60/196]	Time 0.489 (0.520)	Data 0.200 (0.208)	Loss 2.3029 (2.3026)	Acc@1 12.500 (9.785)
Epoch: [0][70/196]	Time 0.491 (0.515)	Data 0.204 (0.205)	Loss 2.2995 (2.3026)	Acc@1 26.562 (10.299)
Epoch: [0][70/196]	Time 0.491 (0.516)	Data 0.202 (0.207)	Loss 2.3010 (2.3026)	Acc@1 12.500 (10.101)
Epoch: [0][80/196]	Time 0.488 (0.513)	Data 0.201 (0.206)	Loss 2.3033 (2.3026)	Acc@1 6.250 (9.954)
Epoch: [0][80/196]	Time 0.493 (0.512)	Data 0.203 (0.205)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.012)
Epoch: [0][90/196]	Time 0.489 (0.510)	Data 0.202 (0.205)	Loss 2.3025 (2.3027)	Acc@1 10.938 (10.130)
Epoch: [0][90/196]	Time 0.490 (0.510)	Data 0.201 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.027)
Epoch: [0][100/196]	Time 0.489 (0.508)	Data 0.200 (0.205)	Loss 2.3006 (2.3026)	Acc@1 14.062 (9.994)Epoch: [0][100/196]	Time 0.491 (0.508)	Data 0.201 (0.204)	Loss 2.3035 (2.3027)	Acc@1 3.125 (10.303)

Epoch: [0][110/196]	Time 0.489 (0.507)	Data 0.201 (0.205)	Loss 2.3032 (2.3026)	Acc@1 10.938 (9.938)
Epoch: [0][110/196]	Time 0.494 (0.506)	Data 0.205 (0.204)	Loss 2.3023 (2.3027)	Acc@1 10.938 (10.248)
Epoch: [0][120/196]	Time 0.490 (0.505)	Data 0.203 (0.204)	Loss 2.3021 (2.3026)	Acc@1 12.500 (10.292)
Epoch: [0][120/196]	Time 0.489 (0.506)	Data 0.201 (0.205)	Loss 2.3015 (2.3026)	Acc@1 9.375 (10.085)
Epoch: [0][130/196]	Time 0.492 (0.504)	Data 0.203 (0.204)	Loss 2.3045 (2.3026)	Acc@1 7.812 (10.222)
Epoch: [0][130/196]	Time 0.490 (0.505)	Data 0.202 (0.204)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.138)
Epoch: [0][140/196]	Time 0.490 (0.503)	Data 0.203 (0.204)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.073)
Epoch: [0][140/196]	Time 0.491 (0.504)	Data 0.202 (0.204)	Loss 2.3023 (2.3026)	Acc@1 12.500 (10.250)
Epoch: [0][150/196]	Time 0.490 (0.503)	Data 0.202 (0.204)	Loss 2.3030 (2.3026)	Acc@1 9.375 (10.192)
Epoch: [0][150/196]	Time 0.493 (0.502)	Data 0.205 (0.204)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.006)
Epoch: [0][160/196]	Time 0.492 (0.502)	Data 0.203 (0.204)	Loss 2.3037 (2.3027)	Acc@1 6.250 (9.812)
Epoch: [0][160/196]	Time 0.499 (0.502)	Data 0.208 (0.204)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.122)
Epoch: [0][170/196]	Time 0.490 (0.501)	Data 0.203 (0.204)	Loss 2.3024 (2.3027)	Acc@1 14.062 (9.905)
Epoch: [0][170/196]	Time 0.490 (0.501)	Data 0.202 (0.204)	Loss 2.3022 (2.3026)	Acc@1 14.062 (10.106)
Epoch: [0][180/196]	Time 0.490 (0.501)	Data 0.201 (0.204)	Loss 2.3018 (2.3026)	Acc@1 9.375 (10.040)Epoch: [0][180/196]	Time 0.492 (0.500)	Data 0.204 (0.204)	Loss 2.3029 (2.3026)	Acc@1 9.375 (10.031)

Epoch: [0][190/196]	Time 0.490 (0.500)	Data 0.202 (0.204)	Loss 2.3035 (2.3027)	Acc@1 3.125 (9.858)
Epoch: [0][190/196]	Time 0.491 (0.500)	Data 0.201 (0.203)	Loss 2.3029 (2.3026)	Acc@1 10.938 (10.095)
Test: [0/157]	Time 0.675 (0.675)	Data 0.430 (0.430)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [0/157]	Time 0.713 (0.713)	Data 0.454 (0.454)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [10/157]	Time 0.249 (0.289)	Data 0.006 (0.046)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [10/157]	Time 0.249 (0.295)	Data 0.005 (0.049)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [20/157]	Time 0.258 (0.272)	Data 0.008 (0.028)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [20/157]	Time 0.254 (0.274)	Data 0.008 (0.028)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [30/157]	Time 0.251 (0.265)	Data 0.008 (0.021)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [30/157]	Time 0.252 (0.267)	Data 0.006 (0.022)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [40/157]	Time 0.250 (0.261)	Data 0.009 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [40/157]	Time 0.253 (0.263)	Data 0.008 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [50/157]	Time 0.251 (0.259)	Data 0.007 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [50/157]	Time 0.252 (0.261)	Data 0.008 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [60/157]	Time 0.249 (0.258)	Data 0.007 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [60/157]	Time 0.256 (0.259)	Data 0.009 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [70/157]	Time 0.249 (0.257)	Data 0.007 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [70/157]	Time 0.252 (0.258)	Data 0.007 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [80/157]	Time 0.251 (0.256)	Data 0.007 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [80/157]	Time 0.251 (0.258)	Data 0.008 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [90/157]	Time 0.250 (0.255)	Data 0.008 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [90/157]	Time 0.253 (0.257)	Data 0.008 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [100/157]	Time 0.249 (0.255)	Data 0.006 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [100/157]	Time 0.252 (0.256)	Data 0.008 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.250 (0.254)	Data 0.007 (0.011)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [110/157]	Time 0.251 (0.256)	Data 0.006 (0.011)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [120/157]	Time 0.250 (0.254)	Data 0.008 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [120/157]	Time 0.251 (0.256)	Data 0.007 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [130/157]	Time 0.249 (0.254)	Data 0.007 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [130/157]	Time 0.254 (0.255)	Data 0.007 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [140/157]	Time 0.250 (0.253)	Data 0.006 (0.010)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [140/157]	Time 0.254 (0.255)	Data 0.009 (0.011)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [150/157]	Time 0.250 (0.253)	Data 0.008 (0.010)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
Test: [150/157]	Time 0.250 (0.255)	Data 0.007 (0.010)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run confused-star-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/hf4psren
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175651-hf4psren/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: \ 0.006 MB of 0.011 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run lilac-firebrand-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/0z14y4b4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_175652-0z14y4b4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 26143 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 26144) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_17:59:35
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 26144)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180005-a552ntqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-water-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/a552ntqq
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180005-gmwcim0i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sunset-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/gmwcim0i
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.236 (2.236)	Data 0.441 (0.441)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.038 (2.038)	Data 0.380 (0.380)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][10/196]	Time 0.495 (0.630)	Data 0.206 (0.216)	Loss 2.3026 (2.3029)	Acc@1 7.812 (9.659)
Epoch: [0][10/196]	Time 0.488 (0.663)	Data 0.198 (0.236)	Loss 2.3012 (2.3025)	Acc@1 12.500 (10.227)
Epoch: [0][20/196]	Time 0.490 (0.581)	Data 0.201 (0.218)	Loss 2.3039 (2.3027)	Acc@1 4.688 (9.673)
Epoch: [0][20/196]	Time 0.495 (0.564)	Data 0.205 (0.209)	Loss 2.3024 (2.3027)	Acc@1 12.500 (9.896)
Epoch: [0][30/196]	Time 0.492 (0.540)	Data 0.204 (0.207)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.181)
Epoch: [0][30/196]	Time 0.487 (0.552)	Data 0.198 (0.213)	Loss 2.3027 (2.3027)	Acc@1 12.500 (9.173)
Epoch: [0][40/196]	Time 0.490 (0.529)	Data 0.200 (0.206)	Loss 2.3024 (2.3027)	Acc@1 7.812 (9.794)
Epoch: [0][40/196]	Time 0.491 (0.537)	Data 0.200 (0.210)	Loss 2.3029 (2.3026)	Acc@1 9.375 (9.604)
Epoch: [0][50/196]	Time 0.490 (0.528)	Data 0.201 (0.208)	Loss 2.3037 (2.3026)	Acc@1 9.375 (9.835)
Epoch: [0][50/196]	Time 0.490 (0.521)	Data 0.202 (0.206)	Loss 2.3026 (2.3027)	Acc@1 12.500 (9.804)
Epoch: [0][60/196]	Time 0.491 (0.522)	Data 0.201 (0.207)	Loss 2.3029 (2.3026)	Acc@1 12.500 (9.785)
Epoch: [0][60/196]	Time 0.496 (0.517)	Data 0.206 (0.205)	Loss 2.3038 (2.3027)	Acc@1 6.250 (9.836)
Epoch: [0][70/196]	Time 0.501 (0.513)	Data 0.209 (0.205)	Loss 2.2995 (2.3026)	Acc@1 26.562 (10.299)
Epoch: [0][70/196]	Time 0.495 (0.518)	Data 0.202 (0.206)	Loss 2.3010 (2.3026)	Acc@1 12.500 (10.101)
Epoch: [0][80/196]	Time 0.484 (0.510)	Data 0.199 (0.205)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.012)
Epoch: [0][80/196]	Time 0.489 (0.515)	Data 0.199 (0.205)	Loss 2.3033 (2.3026)	Acc@1 6.250 (9.954)
Epoch: [0][90/196]	Time 0.496 (0.508)	Data 0.207 (0.205)	Loss 2.3025 (2.3027)	Acc@1 10.938 (10.130)
Epoch: [0][90/196]	Time 0.492 (0.512)	Data 0.200 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.027)
Epoch: [0][100/196]	Time 0.496 (0.507)	Data 0.206 (0.205)	Loss 2.3035 (2.3027)	Acc@1 3.125 (10.303)
Epoch: [0][100/196]	Time 0.494 (0.510)	Data 0.201 (0.205)	Loss 2.3006 (2.3026)	Acc@1 14.062 (9.994)
Epoch: [0][110/196]	Time 0.489 (0.506)	Data 0.202 (0.205)	Loss 2.3023 (2.3027)	Acc@1 10.938 (10.248)
Epoch: [0][110/196]	Time 0.490 (0.509)	Data 0.200 (0.204)	Loss 2.3032 (2.3026)	Acc@1 10.938 (9.938)
Epoch: [0][120/196]	Time 0.490 (0.504)	Data 0.203 (0.205)	Loss 2.3021 (2.3026)	Acc@1 12.500 (10.292)
Epoch: [0][120/196]	Time 0.498 (0.507)	Data 0.205 (0.204)	Loss 2.3015 (2.3026)	Acc@1 9.375 (10.085)
Epoch: [0][130/196]	Time 0.496 (0.504)	Data 0.207 (0.205)	Loss 2.3045 (2.3026)	Acc@1 7.812 (10.222)
Epoch: [0][130/196]	Time 0.491 (0.506)	Data 0.202 (0.204)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.138)
Epoch: [0][140/196]	Time 0.489 (0.503)	Data 0.202 (0.205)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.073)
Epoch: [0][140/196]	Time 0.490 (0.505)	Data 0.200 (0.204)	Loss 2.3023 (2.3026)	Acc@1 12.500 (10.250)
Epoch: [0][150/196]	Time 0.494 (0.502)	Data 0.207 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.006)
Epoch: [0][150/196]	Time 0.492 (0.504)	Data 0.200 (0.203)	Loss 2.3030 (2.3026)	Acc@1 9.375 (10.192)
Epoch: [0][160/196]	Time 0.490 (0.502)	Data 0.202 (0.205)	Loss 2.3037 (2.3027)	Acc@1 6.250 (9.812)
Epoch: [0][160/196]	Time 0.494 (0.504)	Data 0.204 (0.203)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.122)
Epoch: [0][170/196]	Time 0.493 (0.501)	Data 0.206 (0.205)	Loss 2.3024 (2.3027)	Acc@1 14.062 (9.905)
Epoch: [0][170/196]	Time 0.492 (0.503)	Data 0.201 (0.203)	Loss 2.3022 (2.3026)	Acc@1 14.062 (10.106)
Epoch: [0][180/196]	Time 0.491 (0.501)	Data 0.205 (0.205)	Loss 2.3029 (2.3026)	Acc@1 9.375 (10.031)
Epoch: [0][180/196]	Time 0.492 (0.503)	Data 0.202 (0.203)	Loss 2.3018 (2.3026)	Acc@1 9.375 (10.040)
Epoch: [0][190/196]	Time 0.491 (0.500)	Data 0.204 (0.204)	Loss 2.3035 (2.3027)	Acc@1 3.125 (9.858)Epoch: [0][190/196]	Time 0.492 (0.502)	Data 0.203 (0.203)	Loss 2.3029 (2.3026)	Acc@1 10.938 (10.095)

Test: [0/157]	Time 0.679 (0.679)	Data 0.422 (0.422)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [0/157]	Time 0.702 (0.702)	Data 0.455 (0.455)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [10/157]	Time 0.246 (0.291)	Data 0.006 (0.046)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [10/157]	Time 0.249 (0.294)	Data 0.006 (0.049)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [20/157]	Time 0.250 (0.270)	Data 0.007 (0.027)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [20/157]	Time 0.251 (0.273)	Data 0.007 (0.029)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [30/157]	Time 0.249 (0.264)	Data 0.009 (0.021)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [30/157]	Time 0.252 (0.266)	Data 0.008 (0.022)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [40/157]	Time 0.252 (0.260)	Data 0.008 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [40/157]	Time 0.249 (0.262)	Data 0.007 (0.019)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [50/157]	Time 0.248 (0.258)	Data 0.007 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [50/157]	Time 0.248 (0.260)	Data 0.007 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [60/157]	Time 0.247 (0.257)	Data 0.007 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [60/157]	Time 0.252 (0.258)	Data 0.008 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [70/157]	Time 0.254 (0.256)	Data 0.010 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [70/157]	Time 0.253 (0.257)	Data 0.008 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [80/157]	Time 0.250 (0.255)	Data 0.007 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [80/157]	Time 0.249 (0.257)	Data 0.008 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [90/157]	Time 0.249 (0.255)	Data 0.007 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [90/157]	Time 0.252 (0.256)	Data 0.008 (0.013)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [100/157]	Time 0.250 (0.254)	Data 0.007 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [100/157]	Time 0.249 (0.256)	Data 0.007 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.252 (0.254)	Data 0.007 (0.012)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [110/157]	Time 0.256 (0.255)	Data 0.010 (0.012)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [120/157]	Time 0.252 (0.254)	Data 0.008 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [120/157]	Time 0.251 (0.255)	Data 0.009 (0.012)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [130/157]	Time 0.250 (0.253)	Data 0.008 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [130/157]	Time 0.251 (0.255)	Data 0.007 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.011)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [140/157]	Time 0.251 (0.255)	Data 0.007 (0.011)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [150/157]	Time 0.250 (0.253)	Data 0.007 (0.011)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
Test: [150/157]	Time 0.251 (0.255)	Data 0.008 (0.011)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run visionary-water-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/a552ntqq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180005-a552ntqq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run woven-sunset-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/gmwcim0i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180005-gmwcim0i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27863 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27862) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:02:48
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27862)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180318-d3r7x3ev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-water-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/d3r7x3ev
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180318-pzcve4qk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-wind-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/pzcve4qk
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.354 (2.354)	Data 0.666 (0.666)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][0/196]	Time 2.262 (2.262)	Data 0.488 (0.488)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.489 (0.671)	Data 0.200 (0.251)	Loss 2.3104 (2.3040)	Acc@1 3.125 (8.097)Epoch: [0][10/196]	Time 0.492 (0.653)	Data 0.203 (0.227)	Loss 2.3075 (2.3042)	Acc@1 9.375 (10.795)

Epoch: [0][20/196]	Time 0.493 (0.576)	Data 0.205 (0.214)	Loss 2.2960 (2.3044)	Acc@1 14.062 (11.235)Epoch: [0][20/196]	Time 0.489 (0.585)	Data 0.199 (0.227)	Loss 2.2975 (2.3051)	Acc@1 12.500 (9.301)

Epoch: [0][30/196]	Time 0.491 (0.555)	Data 0.203 (0.219)	Loss 2.3025 (2.3043)	Acc@1 7.812 (9.980)
Epoch: [0][30/196]	Time 0.492 (0.549)	Data 0.201 (0.211)	Loss 2.3079 (2.3042)	Acc@1 10.938 (10.988)
Epoch: [0][40/196]	Time 0.495 (0.540)	Data 0.201 (0.215)	Loss 2.3170 (2.3041)	Acc@1 9.375 (9.985)
Epoch: [0][40/196]	Time 0.498 (0.535)	Data 0.206 (0.209)	Loss 2.3066 (2.3044)	Acc@1 3.125 (10.518)
Epoch: [0][50/196]	Time 0.488 (0.527)	Data 0.199 (0.207)	Loss 2.3177 (2.3051)	Acc@1 7.812 (10.018)
Epoch: [0][50/196]	Time 0.488 (0.530)	Data 0.198 (0.213)	Loss 2.3063 (2.3044)	Acc@1 9.375 (9.926)
Epoch: [0][60/196]	Time 0.492 (0.521)	Data 0.202 (0.206)	Loss 2.3067 (2.3050)	Acc@1 7.812 (9.810)
Epoch: [0][60/196]	Time 0.496 (0.524)	Data 0.205 (0.211)	Loss 2.3058 (2.3047)	Acc@1 9.375 (9.836)
Epoch: [0][70/196]	Time 0.494 (0.517)	Data 0.204 (0.206)	Loss 2.2874 (2.3047)	Acc@1 9.375 (10.189)Epoch: [0][70/196]	Time 0.495 (0.519)	Data 0.206 (0.209)	Loss 2.2917 (2.3040)	Acc@1 4.688 (9.815)

Epoch: [0][80/196]	Time 0.490 (0.516)	Data 0.200 (0.208)	Loss 2.3117 (2.3052)	Acc@1 7.812 (9.587)
Epoch: [0][80/196]	Time 0.492 (0.514)	Data 0.203 (0.205)	Loss 2.2964 (2.3051)	Acc@1 6.250 (10.031)
Epoch: [0][90/196]	Time 0.492 (0.513)	Data 0.203 (0.208)	Loss 2.3125 (2.3055)	Acc@1 10.938 (9.512)
Epoch: [0][90/196]	Time 0.492 (0.511)	Data 0.201 (0.205)	Loss 2.3070 (2.3054)	Acc@1 6.250 (9.976)
Epoch: [0][100/196]	Time 0.489 (0.509)	Data 0.200 (0.204)	Loss 2.3081 (2.3053)	Acc@1 6.250 (10.040)
Epoch: [0][100/196]	Time 0.491 (0.511)	Data 0.201 (0.207)	Loss 2.3091 (2.3057)	Acc@1 17.188 (9.344)
Epoch: [0][110/196]	Time 0.488 (0.508)	Data 0.200 (0.204)	Loss 2.2969 (2.3051)	Acc@1 12.500 (10.008)
Epoch: [0][110/196]	Time 0.490 (0.509)	Data 0.199 (0.206)	Loss 2.3027 (2.3059)	Acc@1 14.062 (9.445)
Epoch: [0][120/196]	Time 0.489 (0.508)	Data 0.199 (0.206)	Loss 2.3142 (2.3061)	Acc@1 9.375 (9.336)
Epoch: [0][120/196]	Time 0.492 (0.506)	Data 0.201 (0.204)	Loss 2.2990 (2.3055)	Acc@1 1.562 (9.788)
Epoch: [0][130/196]	Time 0.491 (0.505)	Data 0.201 (0.204)	Loss 2.3167 (2.3058)	Acc@1 3.125 (9.649)
Epoch: [0][130/196]	Time 0.495 (0.507)	Data 0.204 (0.206)	Loss 2.3225 (2.3061)	Acc@1 6.250 (9.423)
Epoch: [0][140/196]	Time 0.488 (0.505)	Data 0.199 (0.205)	Loss 2.3131 (2.3061)	Acc@1 9.375 (9.497)
Epoch: [0][140/196]	Time 0.493 (0.504)	Data 0.204 (0.203)	Loss 2.2839 (2.3058)	Acc@1 12.500 (9.630)
Epoch: [0][150/196]	Time 0.492 (0.503)	Data 0.203 (0.203)	Loss 2.3064 (2.3058)	Acc@1 6.250 (9.613)
Epoch: [0][150/196]	Time 0.490 (0.504)	Data 0.201 (0.205)	Loss 2.3148 (2.3062)	Acc@1 7.812 (9.489)
Epoch: [0][160/196]	Time 0.492 (0.502)	Data 0.202 (0.203)	Loss 2.3082 (2.3058)	Acc@1 7.812 (9.491)
Epoch: [0][160/196]	Time 0.490 (0.504)	Data 0.198 (0.205)	Loss 2.2975 (2.3059)	Acc@1 7.812 (9.550)
Epoch: [0][170/196]	Time 0.487 (0.503)	Data 0.198 (0.204)	Loss 2.3261 (2.3060)	Acc@1 7.812 (9.695)
Epoch: [0][170/196]	Time 0.490 (0.502)	Data 0.200 (0.203)	Loss 2.3150 (2.3058)	Acc@1 10.938 (9.512)
Epoch: [0][180/196]	Time 0.491 (0.501)	Data 0.201 (0.203)	Loss 2.3097 (2.3060)	Acc@1 9.375 (9.461)
Epoch: [0][180/196]	Time 0.491 (0.502)	Data 0.201 (0.204)	Loss 2.3013 (2.3061)	Acc@1 9.375 (9.703)
Epoch: [0][190/196]	Time 0.491 (0.501)	Data 0.201 (0.203)	Loss 2.3106 (2.3060)	Acc@1 10.938 (9.465)Epoch: [0][190/196]	Time 0.492 (0.502)	Data 0.202 (0.204)	Loss 2.3053 (2.3063)	Acc@1 14.062 (9.612)

Test: [0/157]	Time 0.674 (0.674)	Data 0.415 (0.415)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.678 (0.678)	Data 0.426 (0.426)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.249 (0.290)	Data 0.006 (0.045)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.247 (0.290)	Data 0.006 (0.046)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.249 (0.271)	Data 0.007 (0.027)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.251 (0.271)	Data 0.007 (0.026)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.250 (0.264)	Data 0.007 (0.021)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.249 (0.265)	Data 0.007 (0.020)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.252 (0.261)	Data 0.008 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.251 (0.261)	Data 0.008 (0.017)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.248 (0.259)	Data 0.007 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.248 (0.259)	Data 0.006 (0.015)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.250 (0.257)	Data 0.007 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.248 (0.258)	Data 0.006 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.251 (0.256)	Data 0.008 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.251 (0.257)	Data 0.007 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.251 (0.255)	Data 0.007 (0.013)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.250 (0.256)	Data 0.007 (0.012)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.250 (0.255)	Data 0.007 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.248 (0.256)	Data 0.007 (0.011)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.251 (0.254)	Data 0.009 (0.012)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.250 (0.255)	Data 0.007 (0.011)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.249 (0.254)	Data 0.008 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.249 (0.255)	Data 0.006 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.250 (0.254)	Data 0.007 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.251 (0.254)	Data 0.007 (0.010)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.251 (0.253)	Data 0.007 (0.011)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.250 (0.254)	Data 0.007 (0.010)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.251 (0.253)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.251 (0.254)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.250 (0.253)	Data 0.006 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.250 (0.253)	Data 0.006 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: üöÄ View run fallen-water-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/d3r7x3ev
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180318-d3r7x3ev/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run summer-wind-8 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention/runs/pzcve4qk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180318-pzcve4qk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 29594 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 29595) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:06:02
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 29595)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180631-aybd6vf3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-fog-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/aybd6vf3
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180631-mldjnn3o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-tree-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/mldjnn3o
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.220 (2.220)	Data 0.414 (0.414)	Loss 2.2949 (2.2949)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.351 (2.351)	Data 0.452 (0.452)	Loss 2.2965 (2.2965)	Acc@1 12.500 (12.500)
Epoch: [0][10/196]	Time 0.542 (0.705)	Data 0.208 (0.228)	Loss 2.2850 (2.3271)	Acc@1 14.062 (9.375)
Epoch: [0][10/196]	Time 0.542 (0.724)	Data 0.210 (0.255)	Loss 2.3537 (2.3236)	Acc@1 12.500 (10.511)
Epoch: [0][20/196]	Time 0.543 (0.638)	Data 0.211 (0.234)	Loss 2.3448 (2.3474)	Acc@1 9.375 (10.417)
Epoch: [0][20/196]	Time 0.550 (0.629)	Data 0.216 (0.219)	Loss 2.3345 (2.3525)	Acc@1 4.688 (9.375)
Epoch: [0][30/196]	Time 0.545 (0.602)	Data 0.210 (0.217)	Loss 2.3493 (2.3658)	Acc@1 7.812 (8.921)Epoch: [0][30/196]	Time 0.546 (0.608)	Data 0.209 (0.226)	Loss 2.3795 (2.3557)	Acc@1 10.938 (10.887)

Epoch: [0][40/196]	Time 0.547 (0.587)	Data 0.215 (0.215)	Loss 2.4405 (2.3677)	Acc@1 4.688 (8.689)
Epoch: [0][40/196]	Time 0.548 (0.592)	Data 0.213 (0.222)	Loss 2.3736 (2.3558)	Acc@1 6.250 (10.556)
Epoch: [0][50/196]	Time 0.540 (0.579)	Data 0.207 (0.214)	Loss 2.3933 (2.3684)	Acc@1 9.375 (8.548)
Epoch: [0][50/196]	Time 0.541 (0.583)	Data 0.206 (0.219)	Loss 2.4296 (2.3561)	Acc@1 7.812 (10.141)
Epoch: [0][60/196]	Time 0.545 (0.573)	Data 0.212 (0.214)	Loss 2.3988 (2.3754)	Acc@1 9.375 (8.914)
Epoch: [0][60/196]	Time 0.549 (0.577)	Data 0.213 (0.218)	Loss 2.3798 (2.3612)	Acc@1 7.812 (10.092)
Epoch: [0][70/196]	Time 0.544 (0.572)	Data 0.209 (0.217)	Loss 2.4282 (2.3661)	Acc@1 9.375 (9.881)
Epoch: [0][70/196]	Time 0.550 (0.569)	Data 0.214 (0.213)	Loss 2.3101 (2.3733)	Acc@1 4.688 (9.001)
Epoch: [0][80/196]	Time 0.544 (0.569)	Data 0.211 (0.216)	Loss 2.3248 (2.3668)	Acc@1 14.062 (9.684)
Epoch: [0][80/196]	Time 0.544 (0.566)	Data 0.207 (0.213)	Loss 2.3433 (2.3730)	Acc@1 14.062 (9.336)
Epoch: [0][90/196]	Time 0.540 (0.566)	Data 0.206 (0.215)	Loss 2.3747 (2.3679)	Acc@1 10.938 (9.667)
Epoch: [0][90/196]	Time 0.540 (0.564)	Data 0.207 (0.213)	Loss 2.4098 (2.3733)	Acc@1 15.625 (9.461)
Epoch: [0][100/196]	Time 0.540 (0.561)	Data 0.210 (0.212)	Loss 2.5042 (2.3744)	Acc@1 10.938 (9.344)
Epoch: [0][100/196]	Time 0.547 (0.563)	Data 0.212 (0.215)	Loss 2.5468 (2.3704)	Acc@1 9.375 (9.669)
Epoch: [0][110/196]	Time 0.542 (0.560)	Data 0.209 (0.212)	Loss 2.3950 (2.3731)	Acc@1 12.500 (9.516)
Epoch: [0][110/196]	Time 0.545 (0.562)	Data 0.209 (0.214)	Loss 2.3370 (2.3682)	Acc@1 14.062 (9.699)
Epoch: [0][120/196]	Time 0.538 (0.558)	Data 0.206 (0.212)	Loss 2.2980 (2.3708)	Acc@1 14.062 (9.866)
Epoch: [0][120/196]	Time 0.541 (0.560)	Data 0.206 (0.214)	Loss 2.3060 (2.3673)	Acc@1 12.500 (10.008)
Epoch: [0][130/196]	Time 0.543 (0.557)	Data 0.210 (0.212)	Loss 2.3228 (2.3732)	Acc@1 9.375 (9.721)
Epoch: [0][130/196]	Time 0.542 (0.559)	Data 0.208 (0.213)	Loss 2.2955 (2.3680)	Acc@1 17.188 (10.019)
Epoch: [0][140/196]	Time 0.540 (0.556)	Data 0.209 (0.212)	Loss 2.3818 (2.3742)	Acc@1 7.812 (9.741)
Epoch: [0][140/196]	Time 0.542 (0.558)	Data 0.207 (0.213)	Loss 2.2573 (2.3716)	Acc@1 20.312 (10.018)
Epoch: [0][150/196]	Time 0.545 (0.555)	Data 0.212 (0.212)	Loss 2.4544 (2.3750)	Acc@1 7.812 (9.789)
Epoch: [0][150/196]	Time 0.545 (0.557)	Data 0.210 (0.213)	Loss 2.5187 (2.3754)	Acc@1 6.250 (9.965)
Epoch: [0][160/196]	Time 0.543 (0.556)	Data 0.208 (0.212)	Loss 2.4080 (2.3769)	Acc@1 3.125 (9.860)
Epoch: [0][160/196]	Time 0.545 (0.555)	Data 0.208 (0.211)	Loss 2.3447 (2.3739)	Acc@1 9.375 (9.831)
Epoch: [0][170/196]	Time 0.542 (0.554)	Data 0.207 (0.211)	Loss 2.3055 (2.3728)	Acc@1 7.812 (9.850)
Epoch: [0][170/196]	Time 0.544 (0.555)	Data 0.207 (0.212)	Loss 2.3092 (2.3756)	Acc@1 10.938 (9.868)
Epoch: [0][180/196]	Time 0.539 (0.555)	Data 0.206 (0.212)	Loss 2.4121 (2.3753)	Acc@1 14.062 (9.953)
Epoch: [0][180/196]	Time 0.540 (0.554)	Data 0.208 (0.211)	Loss 2.3407 (2.3713)	Acc@1 10.938 (9.988)
Epoch: [0][190/196]	Time 0.538 (0.553)	Data 0.208 (0.211)	Loss 2.3004 (2.3705)	Acc@1 14.062 (10.021)
Epoch: [0][190/196]	Time 0.544 (0.554)	Data 0.207 (0.212)	Loss 2.3565 (2.3740)	Acc@1 10.938 (9.948)
Test: [0/157]	Time 0.730 (0.730)	Data 0.449 (0.449)	Loss 2.3351 (2.3351)	Acc@1 12.500 (12.500)
Test: [0/157]	Time 0.728 (0.728)	Data 0.448 (0.448)	Loss 2.3351 (2.3351)	Acc@1 12.500 (12.500)
Test: [10/157]	Time 0.272 (0.315)	Data 0.006 (0.048)	Loss 2.3492 (2.3490)	Acc@1 10.938 (13.920)
Test: [10/157]	Time 0.287 (0.317)	Data 0.012 (0.048)	Loss 2.3492 (2.3490)	Acc@1 10.938 (13.920)
Test: [20/157]	Time 0.271 (0.294)	Data 0.007 (0.029)	Loss 2.3869 (2.3584)	Acc@1 17.188 (14.137)
Test: [20/157]	Time 0.274 (0.297)	Data 0.007 (0.029)	Loss 2.3869 (2.3584)	Acc@1 17.188 (14.137)
Test: [30/157]	Time 0.274 (0.288)	Data 0.007 (0.022)	Loss 2.3701 (2.3645)	Acc@1 15.625 (14.819)
Test: [30/157]	Time 0.272 (0.289)	Data 0.006 (0.022)	Loss 2.3701 (2.3645)	Acc@1 15.625 (14.819)
Test: [40/157]	Time 0.275 (0.284)	Data 0.009 (0.018)	Loss 2.3022 (2.3603)	Acc@1 17.188 (14.596)
Test: [40/157]	Time 0.277 (0.285)	Data 0.008 (0.018)	Loss 2.3022 (2.3603)	Acc@1 17.188 (14.596)
Test: [50/157]	Time 0.274 (0.282)	Data 0.008 (0.016)	Loss 2.3875 (2.3650)	Acc@1 12.500 (14.706)
Test: [50/157]	Time 0.274 (0.283)	Data 0.008 (0.016)	Loss 2.3875 (2.3650)	Acc@1 12.500 (14.706)
Test: [60/157]	Time 0.269 (0.280)	Data 0.007 (0.015)	Loss 2.3956 (2.3664)	Acc@1 9.375 (14.882)
Test: [60/157]	Time 0.273 (0.281)	Data 0.007 (0.015)	Loss 2.3956 (2.3664)	Acc@1 9.375 (14.882)
Test: [70/157]	Time 0.274 (0.279)	Data 0.007 (0.014)	Loss 2.3110 (2.3670)	Acc@1 18.750 (15.119)
Test: [70/157]	Time 0.273 (0.280)	Data 0.007 (0.014)	Loss 2.3110 (2.3670)	Acc@1 18.750 (15.119)
Test: [80/157]	Time 0.274 (0.279)	Data 0.008 (0.013)	Loss 2.3309 (2.3713)	Acc@1 17.188 (15.123)
Test: [80/157]	Time 0.274 (0.279)	Data 0.007 (0.013)	Loss 2.3309 (2.3713)	Acc@1 17.188 (15.123)
Test: [90/157]	Time 0.275 (0.278)	Data 0.009 (0.013)	Loss 2.4107 (2.3726)	Acc@1 14.062 (15.144)
Test: [90/157]	Time 0.277 (0.279)	Data 0.010 (0.012)	Loss 2.4107 (2.3726)	Acc@1 14.062 (15.144)
Test: [100/157]	Time 0.273 (0.278)	Data 0.008 (0.012)	Loss 2.3396 (2.3713)	Acc@1 25.000 (15.501)
Test: [100/157]	Time 0.273 (0.278)	Data 0.007 (0.012)	Loss 2.3396 (2.3713)	Acc@1 25.000 (15.501)
Test: [110/157]	Time 0.274 (0.277)	Data 0.008 (0.012)	Loss 2.3110 (2.3708)	Acc@1 12.500 (15.470)
Test: [110/157]	Time 0.271 (0.278)	Data 0.007 (0.011)	Loss 2.3110 (2.3708)	Acc@1 12.500 (15.470)
Test: [120/157]	Time 0.272 (0.277)	Data 0.009 (0.011)	Loss 2.3190 (2.3725)	Acc@1 25.000 (15.522)
Test: [120/157]	Time 0.276 (0.277)	Data 0.008 (0.011)	Loss 2.3190 (2.3725)	Acc@1 25.000 (15.522)
Test: [130/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.3749 (2.3731)	Acc@1 14.062 (15.375)
Test: [130/157]	Time 0.275 (0.277)	Data 0.008 (0.011)	Loss 2.3749 (2.3731)	Acc@1 14.062 (15.375)
Test: [140/157]	Time 0.276 (0.276)	Data 0.009 (0.011)	Loss 2.3060 (2.3723)	Acc@1 17.188 (15.381)
Test: [140/157]	Time 0.273 (0.277)	Data 0.007 (0.011)	Loss 2.3060 (2.3723)	Acc@1 17.188 (15.381)
Test: [150/157]	Time 0.291 (0.276)	Data 0.016 (0.011)	Loss 2.4419 (2.3727)	Acc@1 15.625 (15.377)
Test: [150/157]	Time 0.272 (0.277)	Data 0.006 (0.010)	Loss 2.4419 (2.3727)	Acc@1 15.625 (15.377)
New Best Found: 15.37%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 15.37%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run jumping-fog-1 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/aybd6vf3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180631-aybd6vf3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run copper-tree-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/mldjnn3o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180631-mldjnn3o/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31326 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 31327) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:09:28
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 31327)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180957-grozn6et
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-dust-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/grozn6et
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_180957-0ff2itf2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-waterfall-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/0ff2itf2
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.209 (2.209)	Data 0.422 (0.422)	Loss 2.2949 (2.2949)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.169 (2.169)	Data 0.418 (0.418)	Loss 2.2965 (2.2965)	Acc@1 12.500 (12.500)
Epoch: [0][10/196]	Time 0.539 (0.687)	Data 0.204 (0.225)	Loss 2.3245 (2.3075)	Acc@1 3.125 (8.523)
Epoch: [0][10/196]	Time 0.542 (0.714)	Data 0.207 (0.247)	Loss 2.3262 (2.3078)	Acc@1 9.375 (11.222)
Epoch: [0][20/196]	Time 0.547 (0.632)	Data 0.211 (0.228)	Loss 2.3015 (2.3078)	Acc@1 21.875 (10.789)Epoch: [0][20/196]	Time 0.544 (0.618)	Data 0.208 (0.217)	Loss 2.2915 (2.3079)	Acc@1 15.625 (9.896)

Epoch: [0][30/196]	Time 0.544 (0.594)	Data 0.212 (0.214)	Loss 2.3046 (2.3060)	Acc@1 9.375 (9.728)
Epoch: [0][30/196]	Time 0.545 (0.603)	Data 0.211 (0.222)	Loss 2.3032 (2.3073)	Acc@1 12.500 (10.383)
Epoch: [0][40/196]	Time 0.540 (0.589)	Data 0.206 (0.218)	Loss 2.3024 (2.3066)	Acc@1 9.375 (9.832)
Epoch: [0][40/196]	Time 0.542 (0.582)	Data 0.208 (0.213)	Loss 2.3134 (2.3053)	Acc@1 6.250 (9.680)
Epoch: [0][50/196]	Time 0.539 (0.580)	Data 0.204 (0.216)	Loss 2.3117 (2.3062)	Acc@1 7.812 (10.049)Epoch: [0][50/196]	Time 0.539 (0.574)	Data 0.205 (0.212)	Loss 2.3000 (2.3048)	Acc@1 9.375 (10.018)

Epoch: [0][60/196]	Time 0.548 (0.574)	Data 0.213 (0.215)	Loss 2.3067 (2.3058)	Acc@1 7.812 (9.734)
Epoch: [0][60/196]	Time 0.548 (0.569)	Data 0.211 (0.212)	Loss 2.3102 (2.3048)	Acc@1 3.125 (9.759)
Epoch: [0][70/196]	Time 0.541 (0.565)	Data 0.210 (0.211)	Loss 2.3007 (2.3045)	Acc@1 7.812 (9.463)Epoch: [0][70/196]	Time 0.541 (0.569)	Data 0.210 (0.214)	Loss 2.3013 (2.3055)	Acc@1 15.625 (9.595)

Epoch: [0][80/196]	Time 0.538 (0.566)	Data 0.207 (0.213)	Loss 2.3027 (2.3054)	Acc@1 6.250 (9.491)Epoch: [0][80/196]	Time 0.539 (0.562)	Data 0.208 (0.211)	Loss 2.3010 (2.3045)	Acc@1 7.812 (9.298)

Epoch: [0][90/196]	Time 0.538 (0.563)	Data 0.205 (0.213)	Loss 2.3010 (2.3051)	Acc@1 6.250 (9.392)
Epoch: [0][90/196]	Time 0.542 (0.560)	Data 0.208 (0.211)	Loss 2.3012 (2.3044)	Acc@1 10.938 (9.306)
Epoch: [0][100/196]	Time 0.541 (0.561)	Data 0.209 (0.213)	Loss 2.2944 (2.3049)	Acc@1 15.625 (9.545)Epoch: [0][100/196]	Time 0.541 (0.558)	Data 0.208 (0.211)	Loss 2.3091 (2.3042)	Acc@1 3.125 (9.545)

Epoch: [0][110/196]	Time 0.541 (0.557)	Data 0.209 (0.211)	Loss 2.2996 (2.3041)	Acc@1 9.375 (9.488)
Epoch: [0][110/196]	Time 0.542 (0.559)	Data 0.209 (0.212)	Loss 2.3023 (2.3048)	Acc@1 10.938 (9.544)
Epoch: [0][120/196]	Time 0.539 (0.558)	Data 0.205 (0.212)	Loss 2.2986 (2.3047)	Acc@1 14.062 (9.724)
Epoch: [0][120/196]	Time 0.540 (0.556)	Data 0.207 (0.211)	Loss 2.3042 (2.3039)	Acc@1 12.500 (9.569)
Epoch: [0][130/196]	Time 0.540 (0.557)	Data 0.207 (0.212)	Loss 2.3106 (2.3047)	Acc@1 10.938 (9.697)
Epoch: [0][130/196]	Time 0.547 (0.555)	Data 0.209 (0.211)	Loss 2.3115 (2.3039)	Acc@1 6.250 (9.649)
Epoch: [0][140/196]	Time 0.545 (0.554)	Data 0.213 (0.211)	Loss 2.3030 (2.3039)	Acc@1 12.500 (9.685)
Epoch: [0][140/196]	Time 0.546 (0.556)	Data 0.213 (0.212)	Loss 2.2988 (2.3045)	Acc@1 12.500 (9.885)
Epoch: [0][150/196]	Time 0.543 (0.553)	Data 0.210 (0.211)	Loss 2.3050 (2.3038)	Acc@1 6.250 (9.820)
Epoch: [0][150/196]	Time 0.545 (0.555)	Data 0.210 (0.211)	Loss 2.3032 (2.3044)	Acc@1 18.750 (9.923)
Epoch: [0][160/196]	Time 0.542 (0.554)	Data 0.211 (0.211)	Loss 2.3058 (2.3043)	Acc@1 6.250 (9.870)
Epoch: [0][160/196]	Time 0.544 (0.553)	Data 0.212 (0.211)	Loss 2.3034 (2.3038)	Acc@1 9.375 (9.928)
Epoch: [0][170/196]	Time 0.545 (0.552)	Data 0.213 (0.211)	Loss 2.3076 (2.3038)	Acc@1 12.500 (9.996)
Epoch: [0][170/196]	Time 0.546 (0.554)	Data 0.209 (0.212)	Loss 2.3078 (2.3042)	Acc@1 9.375 (9.777)
Epoch: [0][180/196]	Time 0.541 (0.554)	Data 0.209 (0.212)	Loss 2.3026 (2.3042)	Acc@1 10.938 (9.729)
Epoch: [0][180/196]	Time 0.543 (0.552)	Data 0.208 (0.211)	Loss 2.3035 (2.3038)	Acc@1 9.375 (9.997)
Epoch: [0][190/196]	Time 0.542 (0.552)	Data 0.211 (0.211)	Loss 2.3001 (2.3037)	Acc@1 12.500 (10.103)
Epoch: [0][190/196]	Time 0.544 (0.553)	Data 0.209 (0.212)	Loss 2.3028 (2.3041)	Acc@1 9.375 (9.751)
Test: [0/157]	Time 0.678 (0.678)	Data 0.402 (0.402)	Loss 2.3007 (2.3007)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.709 (0.709)	Data 0.429 (0.429)	Loss 2.3007 (2.3007)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.272 (0.310)	Data 0.006 (0.044)	Loss 2.3039 (2.3025)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.272 (0.316)	Data 0.005 (0.048)	Loss 2.3039 (2.3025)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.272 (0.293)	Data 0.008 (0.026)	Loss 2.3041 (2.3030)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.272 (0.295)	Data 0.006 (0.028)	Loss 2.3041 (2.3030)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.275 (0.286)	Data 0.008 (0.020)	Loss 2.3041 (2.3034)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.273 (0.287)	Data 0.007 (0.021)	Loss 2.3041 (2.3034)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.272 (0.283)	Data 0.008 (0.017)	Loss 2.3019 (2.3035)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.273 (0.284)	Data 0.008 (0.018)	Loss 2.3019 (2.3035)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.275 (0.281)	Data 0.007 (0.015)	Loss 2.2985 (2.3032)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.270 (0.282)	Data 0.007 (0.016)	Loss 2.2985 (2.3032)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.272 (0.279)	Data 0.008 (0.014)	Loss 2.3019 (2.3032)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.271 (0.280)	Data 0.006 (0.014)	Loss 2.3019 (2.3032)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.276 (0.279)	Data 0.008 (0.013)	Loss 2.3037 (2.3031)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.273 (0.279)	Data 0.007 (0.013)	Loss 2.3037 (2.3031)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.274 (0.278)	Data 0.007 (0.013)	Loss 2.3035 (2.3031)	Acc@1 10.938 (9.780)Test: [80/157]	Time 0.270 (0.278)	Data 0.008 (0.013)	Loss 2.3035 (2.3031)	Acc@1 10.938 (9.780)

Test: [90/157]	Time 0.275 (0.277)	Data 0.007 (0.012)	Loss 2.2969 (2.3028)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.273 (0.277)	Data 0.007 (0.012)	Loss 2.2969 (2.3028)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.271 (0.277)	Data 0.006 (0.012)	Loss 2.3040 (2.3027)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.272 (0.277)	Data 0.007 (0.012)	Loss 2.3040 (2.3027)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.272 (0.276)	Data 0.007 (0.011)	Loss 2.3044 (2.3027)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.272 (0.277)	Data 0.007 (0.011)	Loss 2.3044 (2.3027)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.2947 (2.3027)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.273 (0.276)	Data 0.008 (0.011)	Loss 2.2947 (2.3027)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.272 (0.276)	Data 0.008 (0.011)	Loss 2.3040 (2.3028)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.273 (0.276)	Data 0.007 (0.011)	Loss 2.3040 (2.3028)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.274 (0.275)	Data 0.007 (0.010)	Loss 2.2999 (2.3029)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.275 (0.276)	Data 0.009 (0.010)	Loss 2.2999 (2.3029)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.269 (0.275)	Data 0.006 (0.010)	Loss 2.3059 (2.3028)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.271 (0.275)	Data 0.007 (0.010)	Loss 2.3059 (2.3028)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run atomic-dust-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/grozn6et
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180957-grozn6et/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run colorful-waterfall-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/0ff2itf2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_180957-0ff2itf2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 33062 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33061) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:12:54
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_181322-h8h2b5eg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-snowflake-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/h8h2b5eg
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_181322-yv3vjaue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-aardvark-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/yv3vjaue
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.094 (2.094)	Data 0.386 (0.386)	Loss 2.2949 (2.2949)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.208 (2.208)	Data 0.430 (0.430)	Loss 2.2965 (2.2965)	Acc@1 12.500 (12.500)
Epoch: [0][10/196]	Time 0.546 (0.729)	Data 0.215 (0.270)	Loss 2.3262 (2.3078)	Acc@1 9.375 (11.222)
Epoch: [0][10/196]	Time 0.542 (0.695)	Data 0.208 (0.227)	Loss 2.3245 (2.3075)	Acc@1 3.125 (8.523)
Epoch: [0][20/196]	Time 0.543 (0.622)	Data 0.212 (0.219)	Loss 2.2915 (2.3079)	Acc@1 15.625 (9.896)
Epoch: [0][20/196]	Time 0.543 (0.641)	Data 0.210 (0.241)	Loss 2.3015 (2.3078)	Acc@1 21.875 (10.789)
Epoch: [0][30/196]	Time 0.537 (0.597)	Data 0.205 (0.216)	Loss 2.3046 (2.3060)	Acc@1 9.375 (9.728)
Epoch: [0][30/196]	Time 0.540 (0.609)	Data 0.205 (0.230)	Loss 2.3032 (2.3073)	Acc@1 12.500 (10.383)
Epoch: [0][40/196]	Time 0.543 (0.583)	Data 0.211 (0.214)	Loss 2.3134 (2.3053)	Acc@1 6.250 (9.680)
Epoch: [0][40/196]	Time 0.544 (0.593)	Data 0.206 (0.225)	Loss 2.3024 (2.3066)	Acc@1 9.375 (9.832)
Epoch: [0][50/196]	Time 0.543 (0.576)	Data 0.209 (0.214)	Loss 2.3000 (2.3048)	Acc@1 9.375 (10.018)
Epoch: [0][50/196]	Time 0.541 (0.583)	Data 0.205 (0.221)	Loss 2.3117 (2.3062)	Acc@1 7.812 (10.049)
Epoch: [0][60/196]	Time 0.541 (0.577)	Data 0.206 (0.219)	Loss 2.3067 (2.3058)	Acc@1 7.812 (9.734)Epoch: [0][60/196]	Time 0.542 (0.570)	Data 0.207 (0.213)	Loss 2.3102 (2.3048)	Acc@1 3.125 (9.759)

Epoch: [0][70/196]	Time 0.539 (0.566)	Data 0.207 (0.212)	Loss 2.3007 (2.3045)	Acc@1 7.812 (9.463)
Epoch: [0][70/196]	Time 0.546 (0.572)	Data 0.211 (0.217)	Loss 2.3013 (2.3055)	Acc@1 15.625 (9.595)
Epoch: [0][80/196]	Time 0.539 (0.568)	Data 0.206 (0.216)	Loss 2.3027 (2.3054)	Acc@1 6.250 (9.491)
Epoch: [0][80/196]	Time 0.540 (0.563)	Data 0.206 (0.212)	Loss 2.3010 (2.3045)	Acc@1 7.812 (9.298)
Epoch: [0][90/196]	Time 0.540 (0.565)	Data 0.206 (0.215)	Loss 2.3010 (2.3051)	Acc@1 6.250 (9.392)
Epoch: [0][90/196]	Time 0.542 (0.561)	Data 0.206 (0.211)	Loss 2.3012 (2.3044)	Acc@1 10.938 (9.306)
Epoch: [0][100/196]	Time 0.538 (0.559)	Data 0.207 (0.211)	Loss 2.3091 (2.3042)	Acc@1 3.125 (9.545)
Epoch: [0][100/196]	Time 0.543 (0.563)	Data 0.205 (0.214)	Loss 2.2944 (2.3049)	Acc@1 15.625 (9.545)
Epoch: [0][110/196]	Time 0.539 (0.558)	Data 0.205 (0.211)	Loss 2.2996 (2.3041)	Acc@1 9.375 (9.488)
Epoch: [0][110/196]	Time 0.543 (0.561)	Data 0.204 (0.213)	Loss 2.3023 (2.3048)	Acc@1 10.938 (9.544)
Epoch: [0][120/196]	Time 0.542 (0.557)	Data 0.210 (0.211)	Loss 2.3042 (2.3039)	Acc@1 12.500 (9.569)
Epoch: [0][120/196]	Time 0.544 (0.560)	Data 0.208 (0.213)	Loss 2.2986 (2.3047)	Acc@1 14.062 (9.724)
Epoch: [0][130/196]	Time 0.543 (0.559)	Data 0.209 (0.212)	Loss 2.3106 (2.3047)	Acc@1 10.938 (9.697)
Epoch: [0][130/196]	Time 0.544 (0.556)	Data 0.208 (0.211)	Loss 2.3115 (2.3039)	Acc@1 6.250 (9.649)
Epoch: [0][140/196]	Time 0.545 (0.558)	Data 0.208 (0.212)	Loss 2.2988 (2.3045)	Acc@1 12.500 (9.885)
Epoch: [0][140/196]	Time 0.543 (0.555)	Data 0.207 (0.210)	Loss 2.3030 (2.3039)	Acc@1 12.500 (9.685)
Epoch: [0][150/196]	Time 0.543 (0.554)	Data 0.207 (0.210)	Loss 2.3050 (2.3038)	Acc@1 6.250 (9.820)
Epoch: [0][150/196]	Time 0.545 (0.557)	Data 0.207 (0.211)	Loss 2.3032 (2.3044)	Acc@1 18.750 (9.923)
Epoch: [0][160/196]	Time 0.541 (0.553)	Data 0.208 (0.210)	Loss 2.3034 (2.3038)	Acc@1 9.375 (9.928)
Epoch: [0][160/196]	Time 0.543 (0.556)	Data 0.206 (0.211)	Loss 2.3058 (2.3043)	Acc@1 6.250 (9.870)
Epoch: [0][170/196]	Time 0.545 (0.553)	Data 0.212 (0.210)	Loss 2.3076 (2.3038)	Acc@1 12.500 (9.996)
Epoch: [0][170/196]	Time 0.548 (0.555)	Data 0.209 (0.211)	Loss 2.3078 (2.3042)	Acc@1 9.375 (9.777)
Epoch: [0][180/196]	Time 0.540 (0.554)	Data 0.205 (0.211)	Loss 2.3026 (2.3042)	Acc@1 10.938 (9.729)Epoch: [0][180/196]	Time 0.541 (0.552)	Data 0.208 (0.210)	Loss 2.3035 (2.3038)	Acc@1 9.375 (9.997)

Epoch: [0][190/196]	Time 0.543 (0.552)	Data 0.209 (0.210)	Loss 2.3001 (2.3037)	Acc@1 12.500 (10.103)
Epoch: [0][190/196]	Time 0.543 (0.554)	Data 0.207 (0.210)	Loss 2.3028 (2.3041)	Acc@1 9.375 (9.751)
Test: [0/157]	Time 0.681 (0.681)	Data 0.402 (0.402)	Loss 2.3007 (2.3007)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.695 (0.695)	Data 0.416 (0.416)	Loss 2.3007 (2.3007)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.271 (0.311)	Data 0.005 (0.044)	Loss 2.3039 (2.3025)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.286 (0.310)	Data 0.012 (0.044)	Loss 2.3039 (2.3025)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.271 (0.292)	Data 0.007 (0.026)	Loss 2.3041 (2.3030)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.271 (0.292)	Data 0.008 (0.026)	Loss 2.3041 (2.3030)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.270 (0.285)	Data 0.007 (0.020)	Loss 2.3041 (2.3034)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.273 (0.285)	Data 0.006 (0.020)	Loss 2.3041 (2.3034)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.270 (0.282)	Data 0.008 (0.017)	Loss 2.3019 (2.3035)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.272 (0.282)	Data 0.007 (0.017)	Loss 2.3019 (2.3035)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.270 (0.280)	Data 0.007 (0.015)	Loss 2.2985 (2.3032)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.272 (0.280)	Data 0.007 (0.015)	Loss 2.2985 (2.3032)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.271 (0.278)	Data 0.008 (0.014)	Loss 2.3019 (2.3032)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.272 (0.279)	Data 0.007 (0.013)	Loss 2.3019 (2.3032)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.271 (0.277)	Data 0.007 (0.013)	Loss 2.3037 (2.3031)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.271 (0.278)	Data 0.006 (0.013)	Loss 2.3037 (2.3031)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.270 (0.276)	Data 0.007 (0.012)	Loss 2.3035 (2.3031)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.273 (0.277)	Data 0.007 (0.012)	Loss 2.3035 (2.3031)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.270 (0.276)	Data 0.007 (0.012)	Loss 2.2969 (2.3028)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.272 (0.276)	Data 0.007 (0.011)	Loss 2.2969 (2.3028)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.270 (0.275)	Data 0.008 (0.011)	Loss 2.3040 (2.3027)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.272 (0.276)	Data 0.007 (0.011)	Loss 2.3040 (2.3027)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.270 (0.275)	Data 0.007 (0.011)	Loss 2.3044 (2.3027)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.271 (0.275)	Data 0.007 (0.010)	Loss 2.3044 (2.3027)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.272 (0.275)	Data 0.008 (0.010)	Loss 2.2947 (2.3027)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.271 (0.275)	Data 0.007 (0.010)	Loss 2.2947 (2.3027)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.273 (0.274)	Data 0.007 (0.010)	Loss 2.3040 (2.3028)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.272 (0.275)	Data 0.006 (0.010)	Loss 2.3040 (2.3028)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.270 (0.274)	Data 0.008 (0.010)	Loss 2.2999 (2.3029)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.271 (0.274)	Data 0.007 (0.010)	Loss 2.2999 (2.3029)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.270 (0.274)	Data 0.007 (0.010)	Loss 2.3059 (2.3028)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.276 (0.274)	Data 0.008 (0.010)	Loss 2.3059 (2.3028)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run daily-snowflake-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/h8h2b5eg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_181322-h8h2b5eg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run balmy-aardvark-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/yv3vjaue
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_181322-yv3vjaue/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 34787 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 34788) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:16:19
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 34788)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_181647-t3n3i29r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-disco-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/t3n3i29r
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_181647-hee5nk6z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-durian-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/hee5nk6z
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.191 (2.191)	Data 0.399 (0.399)	Loss 2.2949 (2.2949)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.165 (2.165)	Data 0.454 (0.454)	Loss 2.2965 (2.2965)	Acc@1 12.500 (12.500)
Epoch: [0][10/196]	Time 0.542 (0.692)	Data 0.205 (0.232)	Loss 2.2850 (2.3271)	Acc@1 14.062 (9.375)Epoch: [0][10/196]	Time 0.544 (0.701)	Data 0.205 (0.231)	Loss 2.3537 (2.3236)	Acc@1 12.500 (10.511)

Epoch: [0][20/196]	Time 0.548 (0.621)	Data 0.215 (0.221)	Loss 2.3345 (2.3525)	Acc@1 4.688 (9.375)
Epoch: [0][20/196]	Time 0.543 (0.626)	Data 0.206 (0.219)	Loss 2.3448 (2.3474)	Acc@1 9.375 (10.417)
Epoch: [0][30/196]	Time 0.543 (0.595)	Data 0.208 (0.217)	Loss 2.3493 (2.3658)	Acc@1 7.812 (8.921)
Epoch: [0][30/196]	Time 0.545 (0.599)	Data 0.204 (0.215)	Loss 2.3795 (2.3557)	Acc@1 10.938 (10.887)
Epoch: [0][40/196]	Time 0.544 (0.583)	Data 0.211 (0.215)	Loss 2.4405 (2.3677)	Acc@1 4.688 (8.689)Epoch: [0][40/196]	Time 0.540 (0.585)	Data 0.205 (0.212)	Loss 2.3736 (2.3558)	Acc@1 6.250 (10.556)

Epoch: [0][50/196]	Time 0.547 (0.575)	Data 0.211 (0.214)	Loss 2.3933 (2.3684)	Acc@1 9.375 (8.548)
Epoch: [0][50/196]	Time 0.548 (0.577)	Data 0.206 (0.211)	Loss 2.4296 (2.3561)	Acc@1 7.812 (10.141)
Epoch: [0][60/196]	Time 0.543 (0.570)	Data 0.210 (0.213)	Loss 2.3988 (2.3754)	Acc@1 9.375 (8.914)
Epoch: [0][60/196]	Time 0.542 (0.571)	Data 0.205 (0.210)	Loss 2.3798 (2.3612)	Acc@1 7.812 (10.092)
Epoch: [0][70/196]	Time 0.543 (0.566)	Data 0.208 (0.212)	Loss 2.3101 (2.3733)	Acc@1 4.688 (9.001)
Epoch: [0][70/196]	Time 0.546 (0.567)	Data 0.207 (0.210)	Loss 2.4282 (2.3661)	Acc@1 9.375 (9.881)
Epoch: [0][80/196]	Time 0.549 (0.564)	Data 0.212 (0.209)	Loss 2.3248 (2.3668)	Acc@1 14.062 (9.684)
Epoch: [0][80/196]	Time 0.543 (0.563)	Data 0.209 (0.212)	Loss 2.3433 (2.3730)	Acc@1 14.062 (9.336)
Epoch: [0][90/196]	Time 0.548 (0.562)	Data 0.211 (0.209)	Loss 2.3747 (2.3679)	Acc@1 10.938 (9.667)
Epoch: [0][90/196]	Time 0.548 (0.561)	Data 0.210 (0.212)	Loss 2.4098 (2.3733)	Acc@1 15.625 (9.461)
Epoch: [0][100/196]	Time 0.545 (0.559)	Data 0.211 (0.211)	Loss 2.5042 (2.3744)	Acc@1 10.938 (9.344)
Epoch: [0][100/196]	Time 0.542 (0.560)	Data 0.206 (0.209)	Loss 2.5468 (2.3704)	Acc@1 9.375 (9.669)
Epoch: [0][110/196]	Time 0.546 (0.558)	Data 0.209 (0.208)	Loss 2.3370 (2.3682)	Acc@1 14.062 (9.699)
Epoch: [0][110/196]	Time 0.544 (0.558)	Data 0.207 (0.211)	Loss 2.3950 (2.3731)	Acc@1 12.500 (9.516)
Epoch: [0][120/196]	Time 0.542 (0.557)	Data 0.206 (0.208)	Loss 2.3060 (2.3673)	Acc@1 12.500 (10.008)
Epoch: [0][120/196]	Time 0.543 (0.556)	Data 0.206 (0.211)	Loss 2.2980 (2.3708)	Acc@1 14.062 (9.866)
Epoch: [0][130/196]	Time 0.542 (0.555)	Data 0.208 (0.210)	Loss 2.3228 (2.3732)	Acc@1 9.375 (9.721)
Epoch: [0][130/196]	Time 0.545 (0.556)	Data 0.209 (0.208)	Loss 2.2955 (2.3680)	Acc@1 17.188 (10.019)
Epoch: [0][140/196]	Time 0.549 (0.554)	Data 0.212 (0.210)	Loss 2.3818 (2.3742)	Acc@1 7.812 (9.741)
Epoch: [0][140/196]	Time 0.546 (0.555)	Data 0.205 (0.208)	Loss 2.2573 (2.3716)	Acc@1 20.312 (10.018)
Epoch: [0][150/196]	Time 0.548 (0.554)	Data 0.214 (0.210)	Loss 2.4544 (2.3750)	Acc@1 7.812 (9.789)
Epoch: [0][150/196]	Time 0.545 (0.554)	Data 0.206 (0.208)	Loss 2.5187 (2.3754)	Acc@1 6.250 (9.965)
Epoch: [0][160/196]	Time 0.547 (0.553)	Data 0.209 (0.210)	Loss 2.3447 (2.3739)	Acc@1 9.375 (9.831)
Epoch: [0][160/196]	Time 0.550 (0.554)	Data 0.215 (0.208)	Loss 2.4080 (2.3769)	Acc@1 3.125 (9.860)
Epoch: [0][170/196]	Time 0.547 (0.552)	Data 0.209 (0.210)	Loss 2.3055 (2.3728)	Acc@1 7.812 (9.850)
Epoch: [0][170/196]	Time 0.545 (0.553)	Data 0.206 (0.208)	Loss 2.3092 (2.3756)	Acc@1 10.938 (9.868)
Epoch: [0][180/196]	Time 0.543 (0.552)	Data 0.208 (0.210)	Loss 2.3407 (2.3713)	Acc@1 10.938 (9.988)
Epoch: [0][180/196]	Time 0.544 (0.553)	Data 0.207 (0.208)	Loss 2.4121 (2.3753)	Acc@1 14.062 (9.953)
Epoch: [0][190/196]	Time 0.542 (0.552)	Data 0.207 (0.208)	Loss 2.3565 (2.3740)	Acc@1 10.938 (9.948)
Epoch: [0][190/196]	Time 0.545 (0.551)	Data 0.209 (0.210)	Loss 2.3004 (2.3705)	Acc@1 14.062 (10.021)
Test: [0/157]	Time 0.681 (0.681)	Data 0.407 (0.407)	Loss 2.3351 (2.3351)	Acc@1 12.500 (12.500)
Test: [0/157]	Time 0.686 (0.686)	Data 0.409 (0.409)	Loss 2.3351 (2.3351)	Acc@1 12.500 (12.500)
Test: [10/157]	Time 0.270 (0.308)	Data 0.005 (0.044)	Loss 2.3492 (2.3490)	Acc@1 10.938 (13.920)
Test: [10/157]	Time 0.271 (0.312)	Data 0.005 (0.045)	Loss 2.3492 (2.3490)	Acc@1 10.938 (13.920)
Test: [20/157]	Time 0.269 (0.290)	Data 0.007 (0.026)	Loss 2.3869 (2.3584)	Acc@1 17.188 (14.137)
Test: [20/157]	Time 0.271 (0.292)	Data 0.007 (0.026)	Loss 2.3869 (2.3584)	Acc@1 17.188 (14.137)
Test: [30/157]	Time 0.271 (0.284)	Data 0.007 (0.020)	Loss 2.3701 (2.3645)	Acc@1 15.625 (14.819)
Test: [30/157]	Time 0.269 (0.285)	Data 0.007 (0.020)	Loss 2.3701 (2.3645)	Acc@1 15.625 (14.819)
Test: [40/157]	Time 0.272 (0.281)	Data 0.008 (0.017)	Loss 2.3022 (2.3603)	Acc@1 17.188 (14.596)
Test: [40/157]	Time 0.271 (0.281)	Data 0.007 (0.017)	Loss 2.3022 (2.3603)	Acc@1 17.188 (14.596)
Test: [50/157]	Time 0.269 (0.279)	Data 0.007 (0.015)	Loss 2.3875 (2.3650)	Acc@1 12.500 (14.706)
Test: [50/157]	Time 0.270 (0.279)	Data 0.007 (0.015)	Loss 2.3875 (2.3650)	Acc@1 12.500 (14.706)
Test: [60/157]	Time 0.269 (0.277)	Data 0.008 (0.014)	Loss 2.3956 (2.3664)	Acc@1 9.375 (14.882)
Test: [60/157]	Time 0.270 (0.278)	Data 0.006 (0.013)	Loss 2.3956 (2.3664)	Acc@1 9.375 (14.882)
Test: [70/157]	Time 0.271 (0.276)	Data 0.007 (0.013)	Loss 2.3110 (2.3670)	Acc@1 18.750 (15.119)
Test: [70/157]	Time 0.270 (0.277)	Data 0.007 (0.012)	Loss 2.3110 (2.3670)	Acc@1 18.750 (15.119)
Test: [80/157]	Time 0.271 (0.276)	Data 0.007 (0.012)	Loss 2.3309 (2.3713)	Acc@1 17.188 (15.123)
Test: [80/157]	Time 0.270 (0.276)	Data 0.007 (0.012)	Loss 2.3309 (2.3713)	Acc@1 17.188 (15.123)
Test: [90/157]	Time 0.271 (0.275)	Data 0.007 (0.012)	Loss 2.4107 (2.3726)	Acc@1 14.062 (15.144)
Test: [90/157]	Time 0.268 (0.275)	Data 0.007 (0.011)	Loss 2.4107 (2.3726)	Acc@1 14.062 (15.144)
Test: [100/157]	Time 0.271 (0.275)	Data 0.008 (0.011)	Loss 2.3396 (2.3713)	Acc@1 25.000 (15.501)
Test: [100/157]	Time 0.272 (0.275)	Data 0.007 (0.011)	Loss 2.3396 (2.3713)	Acc@1 25.000 (15.501)
Test: [110/157]	Time 0.270 (0.274)	Data 0.008 (0.011)	Loss 2.3110 (2.3708)	Acc@1 12.500 (15.470)
Test: [110/157]	Time 0.268 (0.275)	Data 0.006 (0.010)	Loss 2.3110 (2.3708)	Acc@1 12.500 (15.470)
Test: [120/157]	Time 0.269 (0.274)	Data 0.008 (0.011)	Loss 2.3190 (2.3725)	Acc@1 25.000 (15.522)
Test: [120/157]	Time 0.270 (0.274)	Data 0.007 (0.010)	Loss 2.3190 (2.3725)	Acc@1 25.000 (15.522)
Test: [130/157]	Time 0.269 (0.274)	Data 0.007 (0.010)	Loss 2.3749 (2.3731)	Acc@1 14.062 (15.375)
Test: [130/157]	Time 0.269 (0.274)	Data 0.007 (0.010)	Loss 2.3749 (2.3731)	Acc@1 14.062 (15.375)
Test: [140/157]	Time 0.272 (0.274)	Data 0.007 (0.010)	Loss 2.3060 (2.3723)	Acc@1 17.188 (15.381)
Test: [140/157]	Time 0.271 (0.274)	Data 0.007 (0.010)	Loss 2.3060 (2.3723)	Acc@1 17.188 (15.381)
Test: [150/157]	Time 0.271 (0.274)	Data 0.006 (0.009)	Loss 2.4419 (2.3727)	Acc@1 15.625 (15.377)
Test: [150/157]	Time 0.292 (0.274)	Data 0.016 (0.010)	Loss 2.4419 (2.3727)	Acc@1 15.625 (15.377)
New Best Found: 15.37%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 15.37%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run twilight-durian-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/hee5nk6z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_181647-hee5nk6z/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run fanciful-disco-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert/runs/t3n3i29r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_0.25_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_181647-t3n3i29r/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 36528 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 36527) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:19:44
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 36527)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: - Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182014-mkujsj8x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-armadillo-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/mkujsj8x
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182014-0nxt0v8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-voice-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/0nxt0v8m
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.057 (2.057)	Data 0.483 (0.483)	Loss 2.3423 (2.3423)	Acc@1 3.125 (3.125)
Epoch: [0][0/196]	Time 2.260 (2.260)	Data 0.602 (0.602)	Loss 2.2918 (2.2918)	Acc@1 15.625 (15.625)
Epoch: [0][10/196]	Time 0.463 (0.608)	Data 0.197 (0.222)	Loss 2.2583 (2.2759)	Acc@1 12.500 (14.915)
Epoch: [0][10/196]	Time 0.460 (0.624)	Data 0.192 (0.231)	Loss 2.1423 (2.2451)	Acc@1 20.312 (16.619)
Epoch: [0][20/196]	Time 0.467 (0.538)	Data 0.198 (0.208)	Loss 2.0070 (2.2384)	Acc@1 20.312 (17.485)
Epoch: [0][20/196]	Time 0.466 (0.546)	Data 0.196 (0.213)	Loss 2.0151 (2.2177)	Acc@1 23.438 (18.080)
Epoch: [0][30/196]	Time 0.470 (0.519)	Data 0.200 (0.208)	Loss 1.9465 (2.1795)	Acc@1 29.688 (19.456)
Epoch: [0][30/196]	Time 0.474 (0.514)	Data 0.202 (0.204)	Loss 1.9781 (2.1969)	Acc@1 29.688 (18.750)
Epoch: [0][40/196]	Time 0.462 (0.505)	Data 0.196 (0.204)	Loss 1.9632 (2.1314)	Acc@1 28.125 (21.075)
Epoch: [0][40/196]	Time 0.465 (0.501)	Data 0.195 (0.201)	Loss 1.9712 (2.1448)	Acc@1 32.812 (20.351)
Epoch: [0][50/196]	Time 0.460 (0.493)	Data 0.193 (0.199)	Loss 1.6542 (2.0904)	Acc@1 48.438 (22.365)Epoch: [0][50/196]	Time 0.462 (0.496)	Data 0.196 (0.202)	Loss 1.7849 (2.0867)	Acc@1 35.938 (22.580)

Epoch: [0][60/196]	Time 0.462 (0.491)	Data 0.194 (0.201)	Loss 1.7632 (2.0556)	Acc@1 37.500 (23.899)
Epoch: [0][60/196]	Time 0.461 (0.488)	Data 0.193 (0.198)	Loss 1.8537 (2.0450)	Acc@1 28.125 (24.001)
Epoch: [0][70/196]	Time 0.455 (0.486)	Data 0.194 (0.200)	Loss 1.8303 (2.0264)	Acc@1 32.812 (25.132)
Epoch: [0][70/196]	Time 0.461 (0.484)	Data 0.194 (0.197)	Loss 1.8139 (2.0163)	Acc@1 34.375 (24.846)
Epoch: [0][80/196]	Time 0.460 (0.483)	Data 0.194 (0.199)	Loss 1.7700 (1.9955)	Acc@1 28.125 (26.177)
Epoch: [0][80/196]	Time 0.459 (0.481)	Data 0.192 (0.197)	Loss 1.6762 (1.9918)	Acc@1 45.312 (26.003)
Epoch: [0][90/196]	Time 0.462 (0.479)	Data 0.194 (0.196)	Loss 1.7919 (1.9626)	Acc@1 37.500 (27.112)
Epoch: [0][90/196]	Time 0.461 (0.481)	Data 0.193 (0.199)	Loss 1.8026 (1.9671)	Acc@1 37.500 (27.284)
Epoch: [0][100/196]	Time 0.458 (0.477)	Data 0.191 (0.196)	Loss 1.6801 (1.9371)	Acc@1 35.938 (28.171)Epoch: [0][100/196]	Time 0.458 (0.479)	Data 0.192 (0.198)	Loss 1.7878 (1.9480)	Acc@1 28.125 (27.970)

Epoch: [0][110/196]	Time 0.461 (0.475)	Data 0.193 (0.196)	Loss 1.4967 (1.9135)	Acc@1 48.438 (29.153)Epoch: [0][110/196]	Time 0.462 (0.477)	Data 0.194 (0.198)	Loss 1.4788 (1.9265)	Acc@1 46.875 (28.899)

Epoch: [0][120/196]	Time 0.463 (0.474)	Data 0.195 (0.196)	Loss 1.5771 (1.8929)	Acc@1 45.312 (29.946)Epoch: [0][120/196]	Time 0.462 (0.476)	Data 0.194 (0.197)	Loss 1.5890 (1.9059)	Acc@1 39.062 (29.791)

Epoch: [0][130/196]	Time 0.459 (0.473)	Data 0.192 (0.195)	Loss 1.5704 (1.8752)	Acc@1 40.625 (30.654)Epoch: [0][130/196]	Time 0.459 (0.474)	Data 0.193 (0.197)	Loss 1.8051 (1.8918)	Acc@1 29.688 (30.105)

Epoch: [0][140/196]	Time 0.461 (0.472)	Data 0.194 (0.195)	Loss 1.6953 (1.8625)	Acc@1 35.938 (31.161)
Epoch: [0][140/196]	Time 0.462 (0.473)	Data 0.195 (0.197)	Loss 1.6741 (1.8781)	Acc@1 34.375 (30.796)
Epoch: [0][150/196]	Time 0.463 (0.471)	Data 0.193 (0.195)	Loss 1.4156 (1.8456)	Acc@1 40.625 (31.716)Epoch: [0][150/196]	Time 0.463 (0.473)	Data 0.195 (0.197)	Loss 1.5296 (1.8609)	Acc@1 50.000 (31.478)

Epoch: [0][160/196]	Time 0.460 (0.472)	Data 0.193 (0.197)	Loss 1.6453 (1.8437)	Acc@1 43.750 (31.920)
Epoch: [0][160/196]	Time 0.462 (0.471)	Data 0.193 (0.195)	Loss 1.7649 (1.8301)	Acc@1 37.500 (32.385)
Epoch: [0][170/196]	Time 0.461 (0.471)	Data 0.195 (0.196)	Loss 1.9132 (1.8304)	Acc@1 31.250 (32.401)
Epoch: [0][170/196]	Time 0.461 (0.470)	Data 0.193 (0.195)	Loss 1.7634 (1.8166)	Acc@1 40.625 (32.895)
Epoch: [0][180/196]	Time 0.463 (0.470)	Data 0.195 (0.195)	Loss 1.4070 (1.8005)	Acc@1 51.562 (33.589)Epoch: [0][180/196]	Time 0.462 (0.471)	Data 0.195 (0.196)	Loss 1.6762 (1.8135)	Acc@1 31.250 (33.020)

Epoch: [0][190/196]	Time 0.457 (0.469)	Data 0.192 (0.195)	Loss 1.6140 (1.7858)	Acc@1 43.750 (34.195)
Epoch: [0][190/196]	Time 0.458 (0.470)	Data 0.193 (0.196)	Loss 1.5760 (1.8000)	Acc@1 43.750 (33.557)
Test: [0/157]	Time 0.680 (0.680)	Data 0.440 (0.440)	Loss 1.4183 (1.4183)	Acc@1 50.000 (50.000)
Test: [0/157]	Time 0.696 (0.696)	Data 0.451 (0.451)	Loss 1.4183 (1.4183)	Acc@1 50.000 (50.000)
Test: [10/157]	Time 0.236 (0.278)	Data 0.006 (0.047)	Loss 1.4472 (1.5171)	Acc@1 48.438 (45.597)
Test: [10/157]	Time 0.233 (0.278)	Data 0.005 (0.048)	Loss 1.4472 (1.5171)	Acc@1 48.438 (45.597)
Test: [20/157]	Time 0.236 (0.258)	Data 0.007 (0.028)	Loss 1.4729 (1.4870)	Acc@1 37.500 (47.173)
Test: [20/157]	Time 0.241 (0.260)	Data 0.009 (0.028)	Loss 1.4729 (1.4870)	Acc@1 37.500 (47.173)
Test: [30/157]	Time 0.237 (0.252)	Data 0.008 (0.022)	Loss 1.5406 (1.4918)	Acc@1 50.000 (47.077)
Test: [30/157]	Time 0.237 (0.253)	Data 0.007 (0.022)	Loss 1.5406 (1.4918)	Acc@1 50.000 (47.077)
Test: [40/157]	Time 0.240 (0.248)	Data 0.009 (0.018)	Loss 1.6104 (1.4971)	Acc@1 43.750 (46.799)
Test: [40/157]	Time 0.239 (0.250)	Data 0.009 (0.019)	Loss 1.6104 (1.4971)	Acc@1 43.750 (46.799)
Test: [50/157]	Time 0.237 (0.247)	Data 0.007 (0.017)	Loss 1.7055 (1.4929)	Acc@1 40.625 (46.722)
Test: [50/157]	Time 0.238 (0.248)	Data 0.009 (0.017)	Loss 1.7055 (1.4929)	Acc@1 40.625 (46.722)
Test: [60/157]	Time 0.239 (0.245)	Data 0.009 (0.015)	Loss 1.4702 (1.4824)	Acc@1 48.438 (46.926)
Test: [60/157]	Time 0.238 (0.247)	Data 0.008 (0.015)	Loss 1.4702 (1.4824)	Acc@1 48.438 (46.926)
Test: [70/157]	Time 0.235 (0.244)	Data 0.007 (0.014)	Loss 1.3929 (1.4790)	Acc@1 51.562 (47.425)
Test: [70/157]	Time 0.239 (0.245)	Data 0.009 (0.014)	Loss 1.3929 (1.4790)	Acc@1 51.562 (47.425)
Test: [80/157]	Time 0.237 (0.243)	Data 0.007 (0.013)	Loss 1.4180 (1.4760)	Acc@1 45.312 (47.685)
Test: [80/157]	Time 0.236 (0.245)	Data 0.007 (0.014)	Loss 1.4180 (1.4760)	Acc@1 45.312 (47.685)
Test: [90/157]	Time 0.241 (0.243)	Data 0.010 (0.013)	Loss 1.3975 (1.4803)	Acc@1 53.125 (47.476)
Test: [90/157]	Time 0.239 (0.244)	Data 0.010 (0.013)	Loss 1.3975 (1.4803)	Acc@1 53.125 (47.476)
Test: [100/157]	Time 0.239 (0.242)	Data 0.009 (0.012)	Loss 1.5484 (1.4830)	Acc@1 42.188 (47.355)
Test: [100/157]	Time 0.239 (0.243)	Data 0.009 (0.012)	Loss 1.5484 (1.4830)	Acc@1 42.188 (47.355)
Test: [110/157]	Time 0.238 (0.242)	Data 0.007 (0.012)	Loss 1.4530 (1.4818)	Acc@1 54.688 (47.593)
Test: [110/157]	Time 0.236 (0.243)	Data 0.007 (0.012)	Loss 1.4530 (1.4818)	Acc@1 54.688 (47.593)
Test: [120/157]	Time 0.237 (0.242)	Data 0.007 (0.012)	Loss 1.5551 (1.4842)	Acc@1 35.938 (47.340)
Test: [120/157]	Time 0.237 (0.242)	Data 0.007 (0.012)	Loss 1.5551 (1.4842)	Acc@1 35.938 (47.340)
Test: [130/157]	Time 0.238 (0.242)	Data 0.007 (0.011)	Loss 1.6048 (1.4907)	Acc@1 45.312 (47.006)
Test: [130/157]	Time 0.239 (0.242)	Data 0.010 (0.011)	Loss 1.6048 (1.4907)	Acc@1 45.312 (47.006)
Test: [140/157]	Time 0.240 (0.241)	Data 0.009 (0.011)	Loss 1.5207 (1.4904)	Acc@1 50.000 (47.163)
Test: [140/157]	Time 0.239 (0.242)	Data 0.008 (0.011)	Loss 1.5207 (1.4904)	Acc@1 50.000 (47.163)
Test: [150/157]	Time 0.241 (0.241)	Data 0.009 (0.011)	Loss 1.3747 (1.4897)	Acc@1 56.250 (47.175)
Test: [150/157]	Time 0.255 (0.242)	Data 0.014 (0.011)	Loss 1.3747 (1.4897)	Acc@1 56.250 (47.175)
New Best Found: 47.04%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 47.04%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run elated-voice-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/0nxt0v8m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182014-0nxt0v8m/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run olive-armadillo-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/mkujsj8x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182014-mkujsj8x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 38253 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 38254) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:22:49
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 38254)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182319-vfjxd8a6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-tree-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/vfjxd8a6
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182319-51qokn6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-rain-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/51qokn6u
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.134 (2.134)	Data 0.531 (0.531)	Loss 2.2918 (2.2918)	Acc@1 15.625 (15.625)
Epoch: [0][0/196]	Time 2.139 (2.139)	Data 0.454 (0.454)	Loss 2.3423 (2.3423)	Acc@1 3.125 (3.125)
Epoch: [0][10/196]	Time 0.464 (0.611)	Data 0.197 (0.217)	Loss 2.2948 (2.3087)	Acc@1 15.625 (11.506)
Epoch: [0][10/196]	Time 0.460 (0.613)	Data 0.193 (0.223)	Loss 2.3102 (2.3144)	Acc@1 14.062 (10.795)
Epoch: [0][20/196]	Time 0.462 (0.540)	Data 0.194 (0.209)	Loss 2.2632 (2.3120)	Acc@1 10.938 (10.119)
Epoch: [0][20/196]	Time 0.461 (0.539)	Data 0.196 (0.206)	Loss 2.3075 (2.3125)	Acc@1 7.812 (10.268)
Epoch: [0][30/196]	Time 0.460 (0.514)	Data 0.195 (0.203)	Loss 2.2686 (2.3045)	Acc@1 12.500 (10.887)
Epoch: [0][30/196]	Time 0.460 (0.515)	Data 0.193 (0.204)	Loss 2.2852 (2.3055)	Acc@1 17.188 (10.685)
Epoch: [0][40/196]	Time 0.462 (0.501)	Data 0.196 (0.201)	Loss 2.2846 (2.2993)	Acc@1 12.500 (11.471)
Epoch: [0][40/196]	Time 0.461 (0.502)	Data 0.192 (0.201)	Loss 2.2854 (2.3000)	Acc@1 12.500 (11.090)
Epoch: [0][50/196]	Time 0.464 (0.494)	Data 0.199 (0.200)	Loss 2.2911 (2.2951)	Acc@1 18.750 (12.469)
Epoch: [0][50/196]	Time 0.458 (0.494)	Data 0.189 (0.200)	Loss 2.2735 (2.2956)	Acc@1 17.188 (11.918)
Epoch: [0][60/196]	Time 0.460 (0.489)	Data 0.195 (0.199)	Loss 2.2798 (2.2917)	Acc@1 12.500 (12.859)
Epoch: [0][60/196]	Time 0.460 (0.488)	Data 0.194 (0.199)	Loss 2.2799 (2.2912)	Acc@1 9.375 (13.064)
Epoch: [0][70/196]	Time 0.463 (0.485)	Data 0.196 (0.199)	Loss 2.2651 (2.2879)	Acc@1 18.750 (13.622)Epoch: [0][70/196]	Time 0.465 (0.485)	Data 0.198 (0.199)	Loss 2.2637 (2.2880)	Acc@1 17.188 (13.358)

Epoch: [0][80/196]	Time 0.462 (0.482)	Data 0.195 (0.198)	Loss 2.2712 (2.2856)	Acc@1 14.062 (13.754)
Epoch: [0][80/196]	Time 0.462 (0.482)	Data 0.195 (0.198)	Loss 2.2552 (2.2862)	Acc@1 17.188 (14.178)
Epoch: [0][90/196]	Time 0.464 (0.480)	Data 0.199 (0.198)	Loss 2.2591 (2.2825)	Acc@1 20.312 (14.320)
Epoch: [0][90/196]	Time 0.462 (0.479)	Data 0.196 (0.198)	Loss 2.2750 (2.2826)	Acc@1 18.750 (14.870)
Epoch: [0][100/196]	Time 0.463 (0.478)	Data 0.195 (0.198)	Loss 2.2425 (2.2800)	Acc@1 17.188 (15.377)
Epoch: [0][100/196]	Time 0.467 (0.478)	Data 0.198 (0.198)	Loss 2.2572 (2.2798)	Acc@1 21.875 (14.960)
Epoch: [0][110/196]	Time 0.460 (0.476)	Data 0.195 (0.197)	Loss 2.2454 (2.2773)	Acc@1 17.188 (15.456)
Epoch: [0][110/196]	Time 0.461 (0.476)	Data 0.195 (0.198)	Loss 2.2334 (2.2781)	Acc@1 21.875 (15.597)
Epoch: [0][120/196]	Time 0.464 (0.475)	Data 0.198 (0.197)	Loss 2.2343 (2.2751)	Acc@1 21.875 (15.638)
Epoch: [0][120/196]	Time 0.464 (0.475)	Data 0.198 (0.197)	Loss 2.2304 (2.2760)	Acc@1 26.562 (15.922)
Epoch: [0][130/196]	Time 0.462 (0.474)	Data 0.196 (0.197)	Loss 2.2476 (2.2736)	Acc@1 23.438 (16.341)Epoch: [0][130/196]	Time 0.466 (0.474)	Data 0.199 (0.197)	Loss 2.2791 (2.2734)	Acc@1 14.062 (16.090)

Epoch: [0][140/196]	Time 0.462 (0.473)	Data 0.196 (0.197)	Loss 2.2427 (2.2716)	Acc@1 23.438 (16.390)
Epoch: [0][140/196]	Time 0.463 (0.473)	Data 0.196 (0.197)	Loss 2.2405 (2.2727)	Acc@1 18.750 (16.345)
Epoch: [0][150/196]	Time 0.457 (0.472)	Data 0.195 (0.197)	Loss 2.2118 (2.2703)	Acc@1 26.562 (16.505)
Epoch: [0][150/196]	Time 0.461 (0.473)	Data 0.196 (0.197)	Loss 2.2219 (2.2695)	Acc@1 17.188 (16.722)
Epoch: [0][160/196]	Time 0.463 (0.472)	Data 0.199 (0.197)	Loss 2.2364 (2.2678)	Acc@1 23.438 (16.935)
Epoch: [0][160/196]	Time 0.462 (0.472)	Data 0.196 (0.197)	Loss 2.2347 (2.2680)	Acc@1 20.312 (16.916)
Epoch: [0][170/196]	Time 0.461 (0.471)	Data 0.195 (0.197)	Loss 2.2255 (2.2662)	Acc@1 23.438 (17.114)
Epoch: [0][170/196]	Time 0.461 (0.471)	Data 0.193 (0.197)	Loss 2.2580 (2.2661)	Acc@1 9.375 (17.060)
Epoch: [0][180/196]	Time 0.463 (0.471)	Data 0.199 (0.197)	Loss 2.2369 (2.2638)	Acc@1 17.188 (17.239)
Epoch: [0][180/196]	Time 0.463 (0.471)	Data 0.198 (0.197)	Loss 2.2530 (2.2648)	Acc@1 14.062 (17.205)
Epoch: [0][190/196]	Time 0.457 (0.470)	Data 0.194 (0.197)	Loss 2.2400 (2.2623)	Acc@1 12.500 (17.351)
Epoch: [0][190/196]	Time 0.460 (0.470)	Data 0.196 (0.197)	Loss 2.2483 (2.2626)	Acc@1 15.625 (17.441)
Test: [0/157]	Time 0.631 (0.631)	Data 0.387 (0.387)	Loss 2.1909 (2.1909)	Acc@1 31.250 (31.250)
Test: [0/157]	Time 0.655 (0.655)	Data 0.413 (0.413)	Loss 2.1909 (2.1909)	Acc@1 31.250 (31.250)
Test: [10/157]	Time 0.235 (0.273)	Data 0.005 (0.042)	Loss 2.2476 (2.2198)	Acc@1 18.750 (21.449)
Test: [10/157]	Time 0.233 (0.274)	Data 0.005 (0.044)	Loss 2.2476 (2.2198)	Acc@1 18.750 (21.449)
Test: [20/157]	Time 0.237 (0.255)	Data 0.007 (0.025)	Loss 2.2181 (2.2246)	Acc@1 18.750 (20.759)
Test: [20/157]	Time 0.254 (0.256)	Data 0.015 (0.026)	Loss 2.2181 (2.2246)	Acc@1 18.750 (20.759)
Test: [30/157]	Time 0.237 (0.249)	Data 0.007 (0.019)	Loss 2.2464 (2.2241)	Acc@1 18.750 (20.262)
Test: [30/157]	Time 0.236 (0.249)	Data 0.007 (0.020)	Loss 2.2464 (2.2241)	Acc@1 18.750 (20.262)
Test: [40/157]	Time 0.240 (0.246)	Data 0.010 (0.016)	Loss 2.2321 (2.2237)	Acc@1 23.438 (20.160)
Test: [40/157]	Time 0.239 (0.246)	Data 0.009 (0.017)	Loss 2.2321 (2.2237)	Acc@1 23.438 (20.160)
Test: [50/157]	Time 0.235 (0.244)	Data 0.007 (0.015)	Loss 2.2580 (2.2235)	Acc@1 15.625 (20.496)
Test: [50/157]	Time 0.237 (0.244)	Data 0.008 (0.015)	Loss 2.2580 (2.2235)	Acc@1 15.625 (20.496)
Test: [60/157]	Time 0.236 (0.243)	Data 0.007 (0.014)	Loss 2.2357 (2.2236)	Acc@1 20.312 (20.108)
Test: [60/157]	Time 0.236 (0.243)	Data 0.007 (0.013)	Loss 2.2357 (2.2236)	Acc@1 20.312 (20.108)
Test: [70/157]	Time 0.235 (0.242)	Data 0.007 (0.013)	Loss 2.1938 (2.2232)	Acc@1 21.875 (19.894)
Test: [70/157]	Time 0.235 (0.242)	Data 0.007 (0.013)	Loss 2.1938 (2.2232)	Acc@1 21.875 (19.894)
Test: [80/157]	Time 0.236 (0.241)	Data 0.007 (0.012)	Loss 2.1946 (2.2229)	Acc@1 31.250 (19.830)
Test: [80/157]	Time 0.235 (0.242)	Data 0.007 (0.012)	Loss 2.1946 (2.2229)	Acc@1 31.250 (19.830)
Test: [90/157]	Time 0.235 (0.240)	Data 0.007 (0.012)	Loss 2.2250 (2.2241)	Acc@1 14.062 (19.952)
Test: [90/157]	Time 0.236 (0.241)	Data 0.006 (0.011)	Loss 2.2250 (2.2241)	Acc@1 14.062 (19.952)
Test: [100/157]	Time 0.236 (0.240)	Data 0.007 (0.011)	Loss 2.2191 (2.2232)	Acc@1 23.438 (20.297)
Test: [100/157]	Time 0.236 (0.241)	Data 0.007 (0.011)	Loss 2.2191 (2.2232)	Acc@1 23.438 (20.297)
Test: [110/157]	Time 0.235 (0.240)	Data 0.007 (0.011)	Loss 2.2280 (2.2234)	Acc@1 15.625 (20.242)
Test: [110/157]	Time 0.237 (0.240)	Data 0.007 (0.011)	Loss 2.2280 (2.2234)	Acc@1 15.625 (20.242)
Test: [120/157]	Time 0.237 (0.239)	Data 0.007 (0.010)	Loss 2.2179 (2.2232)	Acc@1 23.438 (20.028)
Test: [120/157]	Time 0.237 (0.240)	Data 0.007 (0.010)	Loss 2.2179 (2.2232)	Acc@1 23.438 (20.028)
Test: [130/157]	Time 0.235 (0.239)	Data 0.007 (0.010)	Loss 2.1953 (2.2241)	Acc@1 21.875 (19.931)
Test: [130/157]	Time 0.237 (0.240)	Data 0.007 (0.010)	Loss 2.1953 (2.2241)	Acc@1 21.875 (19.931)
Test: [140/157]	Time 0.235 (0.239)	Data 0.007 (0.010)	Loss 2.2053 (2.2243)	Acc@1 29.688 (20.069)
Test: [140/157]	Time 0.236 (0.239)	Data 0.007 (0.010)	Loss 2.2053 (2.2243)	Acc@1 29.688 (20.069)
Test: [150/157]	Time 0.234 (0.239)	Data 0.006 (0.010)	Loss 2.2200 (2.2241)	Acc@1 20.312 (20.281)
Test: [150/157]	Time 0.236 (0.239)	Data 0.006 (0.010)	Loss 2.2200 (2.2241)	Acc@1 20.312 (20.281)
New Best Found: 20.41%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 20.41%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run honest-tree-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/vfjxd8a6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182319-vfjxd8a6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run rural-rain-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/51qokn6u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182319-51qokn6u/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 39969 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 39970) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:25:52
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 39970)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Normalization :  True
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182620-ki1xowa7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-plant-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/ki1xowa7
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182620-d6h5yfpe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-donkey-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/d6h5yfpe
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.013 (2.013)	Data 0.389 (0.389)	Loss 2.3423 (2.3423)	Acc@1 3.125 (3.125)
Epoch: [0][0/196]	Time 2.028 (2.028)	Data 0.420 (0.420)	Loss 2.2918 (2.2918)	Acc@1 15.625 (15.625)
Epoch: [0][10/196]	Time 0.463 (0.622)	Data 0.194 (0.234)	Loss 2.2583 (2.2759)	Acc@1 12.500 (14.915)
Epoch: [0][10/196]	Time 0.459 (0.602)	Data 0.191 (0.212)	Loss 2.1423 (2.2451)	Acc@1 20.312 (16.619)
Epoch: [0][20/196]	Time 0.460 (0.545)	Data 0.195 (0.215)	Loss 2.0070 (2.2384)	Acc@1 20.312 (17.485)
Epoch: [0][20/196]	Time 0.460 (0.535)	Data 0.192 (0.202)	Loss 2.0151 (2.2177)	Acc@1 23.438 (18.080)
Epoch: [0][30/196]	Time 0.459 (0.518)	Data 0.195 (0.208)	Loss 1.9781 (2.1969)	Acc@1 29.688 (18.750)
Epoch: [0][30/196]	Time 0.459 (0.511)	Data 0.191 (0.199)	Loss 1.9465 (2.1795)	Acc@1 29.688 (19.456)
Epoch: [0][40/196]	Time 0.463 (0.504)	Data 0.195 (0.205)	Loss 1.9712 (2.1448)	Acc@1 32.812 (20.351)
Epoch: [0][40/196]	Time 0.463 (0.498)	Data 0.195 (0.197)	Loss 1.9632 (2.1314)	Acc@1 28.125 (21.075)
Epoch: [0][50/196]	Time 0.459 (0.496)	Data 0.192 (0.203)	Loss 1.6542 (2.0904)	Acc@1 48.438 (22.365)
Epoch: [0][50/196]	Time 0.463 (0.491)	Data 0.194 (0.197)	Loss 1.7849 (2.0867)	Acc@1 35.938 (22.580)
Epoch: [0][60/196]	Time 0.467 (0.490)	Data 0.202 (0.202)	Loss 1.8537 (2.0450)	Acc@1 28.125 (24.001)
Epoch: [0][60/196]	Time 0.461 (0.487)	Data 0.194 (0.196)	Loss 1.7632 (2.0556)	Acc@1 37.500 (23.899)
Epoch: [0][70/196]	Time 0.469 (0.483)	Data 0.199 (0.196)	Loss 1.8303 (2.0264)	Acc@1 32.812 (25.132)Epoch: [0][70/196]	Time 0.473 (0.486)	Data 0.204 (0.201)	Loss 1.8139 (2.0163)	Acc@1 34.375 (24.846)

Epoch: [0][80/196]	Time 0.459 (0.483)	Data 0.192 (0.200)	Loss 1.6762 (1.9918)	Acc@1 45.312 (26.003)
Epoch: [0][80/196]	Time 0.462 (0.481)	Data 0.192 (0.196)	Loss 1.7700 (1.9955)	Acc@1 28.125 (26.177)
Epoch: [0][90/196]	Time 0.461 (0.481)	Data 0.195 (0.200)	Loss 1.7919 (1.9626)	Acc@1 37.500 (27.112)
Epoch: [0][90/196]	Time 0.461 (0.479)	Data 0.193 (0.196)	Loss 1.8026 (1.9671)	Acc@1 37.500 (27.284)
Epoch: [0][100/196]	Time 0.467 (0.479)	Data 0.199 (0.199)	Loss 1.6801 (1.9371)	Acc@1 35.938 (28.171)
Epoch: [0][100/196]	Time 0.461 (0.477)	Data 0.192 (0.195)	Loss 1.7878 (1.9480)	Acc@1 28.125 (27.970)
Epoch: [0][110/196]	Time 0.457 (0.478)	Data 0.191 (0.199)	Loss 1.4967 (1.9135)	Acc@1 48.438 (29.153)
Epoch: [0][110/196]	Time 0.461 (0.476)	Data 0.193 (0.195)	Loss 1.4788 (1.9265)	Acc@1 46.875 (28.899)
Epoch: [0][120/196]	Time 0.461 (0.476)	Data 0.194 (0.198)	Loss 1.5771 (1.8929)	Acc@1 45.312 (29.946)
Epoch: [0][120/196]	Time 0.459 (0.474)	Data 0.192 (0.195)	Loss 1.5890 (1.9059)	Acc@1 39.062 (29.791)
Epoch: [0][130/196]	Time 0.463 (0.475)	Data 0.194 (0.198)	Loss 1.5704 (1.8752)	Acc@1 40.625 (30.654)Epoch: [0][130/196]	Time 0.462 (0.473)	Data 0.193 (0.195)	Loss 1.8051 (1.8918)	Acc@1 29.688 (30.105)

Epoch: [0][140/196]	Time 0.458 (0.474)	Data 0.193 (0.198)	Loss 1.6953 (1.8625)	Acc@1 35.938 (31.161)
Epoch: [0][140/196]	Time 0.461 (0.472)	Data 0.193 (0.194)	Loss 1.6741 (1.8781)	Acc@1 34.375 (30.796)
Epoch: [0][150/196]	Time 0.459 (0.473)	Data 0.191 (0.197)	Loss 1.4156 (1.8456)	Acc@1 40.625 (31.716)
Epoch: [0][150/196]	Time 0.464 (0.472)	Data 0.195 (0.194)	Loss 1.5296 (1.8609)	Acc@1 50.000 (31.478)
Epoch: [0][160/196]	Time 0.453 (0.471)	Data 0.190 (0.194)	Loss 1.6453 (1.8437)	Acc@1 43.750 (31.920)
Epoch: [0][160/196]	Time 0.461 (0.472)	Data 0.194 (0.197)	Loss 1.7649 (1.8301)	Acc@1 37.500 (32.385)
Epoch: [0][170/196]	Time 0.458 (0.472)	Data 0.192 (0.197)	Loss 1.7634 (1.8166)	Acc@1 40.625 (32.895)
Epoch: [0][170/196]	Time 0.465 (0.470)	Data 0.194 (0.194)	Loss 1.9132 (1.8304)	Acc@1 31.250 (32.401)
Epoch: [0][180/196]	Time 0.460 (0.471)	Data 0.195 (0.197)	Loss 1.4070 (1.8005)	Acc@1 51.562 (33.589)
Epoch: [0][180/196]	Time 0.465 (0.470)	Data 0.199 (0.194)	Loss 1.6762 (1.8135)	Acc@1 31.250 (33.020)
Epoch: [0][190/196]	Time 0.452 (0.469)	Data 0.193 (0.194)	Loss 1.5760 (1.8000)	Acc@1 43.750 (33.557)
Epoch: [0][190/196]	Time 0.458 (0.470)	Data 0.194 (0.197)	Loss 1.6140 (1.7858)	Acc@1 43.750 (34.195)
Test: [0/157]	Time 0.682 (0.682)	Data 0.439 (0.439)	Loss 1.4183 (1.4183)	Acc@1 50.000 (50.000)
Test: [0/157]	Time 0.699 (0.699)	Data 0.460 (0.460)	Loss 1.4183 (1.4183)	Acc@1 50.000 (50.000)
Test: [10/157]	Time 0.235 (0.279)	Data 0.006 (0.047)	Loss 1.4472 (1.5171)	Acc@1 48.438 (45.597)
Test: [10/157]	Time 0.252 (0.280)	Data 0.013 (0.050)	Loss 1.4472 (1.5171)	Acc@1 48.438 (45.597)
Test: [20/157]	Time 0.237 (0.258)	Data 0.008 (0.028)	Loss 1.4729 (1.4870)	Acc@1 37.500 (47.173)
Test: [20/157]	Time 0.234 (0.259)	Data 0.007 (0.030)	Loss 1.4729 (1.4870)	Acc@1 37.500 (47.173)
Test: [30/157]	Time 0.235 (0.251)	Data 0.006 (0.021)	Loss 1.5406 (1.4918)	Acc@1 50.000 (47.077)
Test: [30/157]	Time 0.235 (0.251)	Data 0.007 (0.022)	Loss 1.5406 (1.4918)	Acc@1 50.000 (47.077)
Test: [40/157]	Time 0.237 (0.248)	Data 0.007 (0.018)	Loss 1.6104 (1.4971)	Acc@1 43.750 (46.799)
Test: [40/157]	Time 0.235 (0.247)	Data 0.008 (0.019)	Loss 1.6104 (1.4971)	Acc@1 43.750 (46.799)
Test: [50/157]	Time 0.234 (0.245)	Data 0.007 (0.017)	Loss 1.7055 (1.4929)	Acc@1 40.625 (46.722)
Test: [50/157]	Time 0.236 (0.245)	Data 0.007 (0.016)	Loss 1.7055 (1.4929)	Acc@1 40.625 (46.722)
Test: [60/157]	Time 0.235 (0.243)	Data 0.007 (0.015)	Loss 1.4702 (1.4824)	Acc@1 48.438 (46.926)
Test: [60/157]	Time 0.237 (0.244)	Data 0.008 (0.015)	Loss 1.4702 (1.4824)	Acc@1 48.438 (46.926)
Test: [70/157]	Time 0.234 (0.242)	Data 0.007 (0.014)	Loss 1.3929 (1.4790)	Acc@1 51.562 (47.425)
Test: [70/157]	Time 0.236 (0.243)	Data 0.007 (0.014)	Loss 1.3929 (1.4790)	Acc@1 51.562 (47.425)
Test: [80/157]	Time 0.234 (0.241)	Data 0.007 (0.013)	Loss 1.4180 (1.4760)	Acc@1 45.312 (47.685)
Test: [80/157]	Time 0.237 (0.242)	Data 0.007 (0.013)	Loss 1.4180 (1.4760)	Acc@1 45.312 (47.685)
Test: [90/157]	Time 0.234 (0.241)	Data 0.007 (0.013)	Loss 1.3975 (1.4803)	Acc@1 53.125 (47.476)
Test: [90/157]	Time 0.235 (0.241)	Data 0.006 (0.012)	Loss 1.3975 (1.4803)	Acc@1 53.125 (47.476)
Test: [100/157]	Time 0.234 (0.240)	Data 0.007 (0.012)	Loss 1.5484 (1.4830)	Acc@1 42.188 (47.355)
Test: [100/157]	Time 0.234 (0.241)	Data 0.007 (0.012)	Loss 1.5484 (1.4830)	Acc@1 42.188 (47.355)
Test: [110/157]	Time 0.234 (0.239)	Data 0.007 (0.012)	Loss 1.4530 (1.4818)	Acc@1 54.688 (47.593)
Test: [110/157]	Time 0.235 (0.240)	Data 0.007 (0.011)	Loss 1.4530 (1.4818)	Acc@1 54.688 (47.593)
Test: [120/157]	Time 0.234 (0.239)	Data 0.007 (0.011)	Loss 1.5551 (1.4842)	Acc@1 35.938 (47.340)
Test: [120/157]	Time 0.238 (0.240)	Data 0.007 (0.011)	Loss 1.5551 (1.4842)	Acc@1 35.938 (47.340)
Test: [130/157]	Time 0.233 (0.239)	Data 0.007 (0.011)	Loss 1.6048 (1.4907)	Acc@1 45.312 (47.006)
Test: [130/157]	Time 0.235 (0.239)	Data 0.007 (0.011)	Loss 1.6048 (1.4907)	Acc@1 45.312 (47.006)
Test: [140/157]	Time 0.252 (0.238)	Data 0.015 (0.011)	Loss 1.5207 (1.4904)	Acc@1 50.000 (47.163)
Test: [140/157]	Time 0.236 (0.239)	Data 0.008 (0.010)	Loss 1.5207 (1.4904)	Acc@1 50.000 (47.163)
Test: [150/157]	Time 0.250 (0.238)	Data 0.014 (0.011)	Loss 1.3747 (1.4897)	Acc@1 56.250 (47.175)
Test: [150/157]	Time 0.235 (0.239)	Data 0.006 (0.010)	Loss 1.3747 (1.4897)	Acc@1 56.250 (47.175)
New Best Found: 47.04%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 47.04%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run trim-donkey-8 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/d6h5yfpe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182620-d6h5yfpe/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run electric-plant-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/ki1xowa7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182620-ki1xowa7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1086 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1085) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:28:53
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1085)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182922-1y3rt535
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-elevator-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/1y3rt535
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_182922-elgb2a02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-wave-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/elgb2a02
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.351 (2.351)	Data 0.613 (0.613)	Loss 2.2918 (2.2918)	Acc@1 15.625 (15.625)
Epoch: [0][0/196]	Time 2.267 (2.267)	Data 0.515 (0.515)	Loss 2.3423 (2.3423)	Acc@1 3.125 (3.125)
Epoch: [0][10/196]	Time 0.456 (0.634)	Data 0.190 (0.236)	Loss 2.3102 (2.3144)	Acc@1 14.062 (10.795)
Epoch: [0][10/196]	Time 0.469 (0.625)	Data 0.200 (0.226)	Loss 2.2948 (2.3087)	Acc@1 15.625 (11.506)
Epoch: [0][20/196]	Time 0.460 (0.552)	Data 0.195 (0.216)	Loss 2.2632 (2.3120)	Acc@1 10.938 (10.119)
Epoch: [0][20/196]	Time 0.475 (0.547)	Data 0.195 (0.210)	Loss 2.3075 (2.3125)	Acc@1 7.812 (10.268)
Epoch: [0][30/196]	Time 0.455 (0.523)	Data 0.196 (0.210)	Loss 2.2852 (2.3055)	Acc@1 17.188 (10.685)
Epoch: [0][30/196]	Time 0.458 (0.520)	Data 0.196 (0.205)	Loss 2.2686 (2.3045)	Acc@1 12.500 (10.887)
Epoch: [0][40/196]	Time 0.464 (0.509)	Data 0.197 (0.207)	Loss 2.2854 (2.3000)	Acc@1 12.500 (11.090)
Epoch: [0][40/196]	Time 0.466 (0.506)	Data 0.196 (0.203)	Loss 2.2846 (2.2993)	Acc@1 12.500 (11.471)
Epoch: [0][50/196]	Time 0.463 (0.500)	Data 0.199 (0.206)	Loss 2.2735 (2.2956)	Acc@1 17.188 (11.918)
Epoch: [0][50/196]	Time 0.477 (0.499)	Data 0.200 (0.202)	Loss 2.2911 (2.2951)	Acc@1 18.750 (12.469)
Epoch: [0][60/196]	Time 0.463 (0.493)	Data 0.196 (0.201)	Loss 2.2799 (2.2912)	Acc@1 9.375 (13.064)
Epoch: [0][60/196]	Time 0.465 (0.495)	Data 0.197 (0.205)	Loss 2.2798 (2.2917)	Acc@1 12.500 (12.859)
Epoch: [0][70/196]	Time 0.464 (0.491)	Data 0.195 (0.204)	Loss 2.2637 (2.2880)	Acc@1 17.188 (13.358)
Epoch: [0][70/196]	Time 0.466 (0.489)	Data 0.198 (0.200)	Loss 2.2651 (2.2879)	Acc@1 18.750 (13.622)
Epoch: [0][80/196]	Time 0.460 (0.486)	Data 0.194 (0.199)	Loss 2.2552 (2.2862)	Acc@1 17.188 (14.178)
Epoch: [0][80/196]	Time 0.467 (0.487)	Data 0.201 (0.203)	Loss 2.2712 (2.2856)	Acc@1 14.062 (13.754)
Epoch: [0][90/196]	Time 0.457 (0.483)	Data 0.195 (0.199)	Loss 2.2750 (2.2826)	Acc@1 18.750 (14.870)
Epoch: [0][90/196]	Time 0.461 (0.485)	Data 0.197 (0.203)	Loss 2.2591 (2.2825)	Acc@1 20.312 (14.320)
Epoch: [0][100/196]	Time 0.467 (0.481)	Data 0.198 (0.199)	Loss 2.2425 (2.2800)	Acc@1 17.188 (15.377)Epoch: [0][100/196]	Time 0.463 (0.483)	Data 0.197 (0.202)	Loss 2.2572 (2.2798)	Acc@1 21.875 (14.960)

Epoch: [0][110/196]	Time 0.462 (0.480)	Data 0.195 (0.198)	Loss 2.2334 (2.2781)	Acc@1 21.875 (15.597)
Epoch: [0][110/196]	Time 0.465 (0.481)	Data 0.197 (0.202)	Loss 2.2454 (2.2773)	Acc@1 17.188 (15.456)
Epoch: [0][120/196]	Time 0.463 (0.479)	Data 0.197 (0.198)	Loss 2.2304 (2.2760)	Acc@1 26.562 (15.922)
Epoch: [0][120/196]	Time 0.464 (0.479)	Data 0.198 (0.201)	Loss 2.2343 (2.2751)	Acc@1 21.875 (15.638)
Epoch: [0][130/196]	Time 0.464 (0.478)	Data 0.198 (0.201)	Loss 2.2791 (2.2734)	Acc@1 14.062 (16.090)
Epoch: [0][130/196]	Time 0.467 (0.477)	Data 0.197 (0.198)	Loss 2.2476 (2.2736)	Acc@1 23.438 (16.341)
Epoch: [0][140/196]	Time 0.461 (0.477)	Data 0.194 (0.198)	Loss 2.2405 (2.2727)	Acc@1 18.750 (16.345)
Epoch: [0][140/196]	Time 0.466 (0.477)	Data 0.198 (0.201)	Loss 2.2427 (2.2716)	Acc@1 23.438 (16.390)
Epoch: [0][150/196]	Time 0.464 (0.476)	Data 0.195 (0.201)	Loss 2.2219 (2.2695)	Acc@1 17.188 (16.722)
Epoch: [0][150/196]	Time 0.472 (0.476)	Data 0.204 (0.198)	Loss 2.2118 (2.2703)	Acc@1 26.562 (16.505)
Epoch: [0][160/196]	Time 0.468 (0.476)	Data 0.199 (0.200)	Loss 2.2364 (2.2678)	Acc@1 23.438 (16.935)Epoch: [0][160/196]	Time 0.467 (0.475)	Data 0.198 (0.198)	Loss 2.2347 (2.2680)	Acc@1 20.312 (16.916)

Epoch: [0][170/196]	Time 0.466 (0.474)	Data 0.197 (0.198)	Loss 2.2255 (2.2662)	Acc@1 23.438 (17.114)
Epoch: [0][170/196]	Time 0.469 (0.475)	Data 0.201 (0.200)	Loss 2.2580 (2.2661)	Acc@1 9.375 (17.060)
Epoch: [0][180/196]	Time 0.463 (0.474)	Data 0.199 (0.200)	Loss 2.2369 (2.2638)	Acc@1 17.188 (17.239)
Epoch: [0][180/196]	Time 0.466 (0.474)	Data 0.201 (0.198)	Loss 2.2530 (2.2648)	Acc@1 14.062 (17.205)
Epoch: [0][190/196]	Time 0.464 (0.473)	Data 0.197 (0.198)	Loss 2.2483 (2.2626)	Acc@1 15.625 (17.441)Epoch: [0][190/196]	Time 0.462 (0.474)	Data 0.198 (0.200)	Loss 2.2400 (2.2623)	Acc@1 12.500 (17.351)

Test: [0/157]	Time 0.692 (0.692)	Data 0.446 (0.446)	Loss 2.1909 (2.1909)	Acc@1 31.250 (31.250)
Test: [0/157]	Time 0.698 (0.698)	Data 0.453 (0.453)	Loss 2.1909 (2.1909)	Acc@1 31.250 (31.250)
Test: [10/157]	Time 0.236 (0.279)	Data 0.006 (0.048)	Loss 2.2476 (2.2198)	Acc@1 18.750 (21.449)
Test: [10/157]	Time 0.237 (0.280)	Data 0.006 (0.049)	Loss 2.2476 (2.2198)	Acc@1 18.750 (21.449)
Test: [20/157]	Time 0.238 (0.259)	Data 0.008 (0.029)	Loss 2.2181 (2.2246)	Acc@1 18.750 (20.759)
Test: [20/157]	Time 0.238 (0.259)	Data 0.009 (0.029)	Loss 2.2181 (2.2246)	Acc@1 18.750 (20.759)
Test: [30/157]	Time 0.237 (0.253)	Data 0.009 (0.022)	Loss 2.2464 (2.2241)	Acc@1 18.750 (20.262)
Test: [30/157]	Time 0.239 (0.253)	Data 0.008 (0.022)	Loss 2.2464 (2.2241)	Acc@1 18.750 (20.262)
Test: [40/157]	Time 0.237 (0.249)	Data 0.008 (0.019)	Loss 2.2321 (2.2237)	Acc@1 23.438 (20.160)
Test: [40/157]	Time 0.238 (0.249)	Data 0.008 (0.019)	Loss 2.2321 (2.2237)	Acc@1 23.438 (20.160)
Test: [50/157]	Time 0.238 (0.247)	Data 0.007 (0.017)	Loss 2.2580 (2.2235)	Acc@1 15.625 (20.496)
Test: [50/157]	Time 0.235 (0.247)	Data 0.007 (0.017)	Loss 2.2580 (2.2235)	Acc@1 15.625 (20.496)
Test: [60/157]	Time 0.237 (0.246)	Data 0.008 (0.015)	Loss 2.2357 (2.2236)	Acc@1 20.312 (20.108)
Test: [60/157]	Time 0.241 (0.246)	Data 0.010 (0.016)	Loss 2.2357 (2.2236)	Acc@1 20.312 (20.108)
Test: [70/157]	Time 0.238 (0.244)	Data 0.007 (0.014)	Loss 2.1938 (2.2232)	Acc@1 21.875 (19.894)
Test: [70/157]	Time 0.239 (0.245)	Data 0.009 (0.015)	Loss 2.1938 (2.2232)	Acc@1 21.875 (19.894)
Test: [80/157]	Time 0.238 (0.244)	Data 0.007 (0.013)	Loss 2.1946 (2.2229)	Acc@1 31.250 (19.830)
Test: [80/157]	Time 0.240 (0.244)	Data 0.009 (0.014)	Loss 2.1946 (2.2229)	Acc@1 31.250 (19.830)
Test: [90/157]	Time 0.240 (0.243)	Data 0.009 (0.013)	Loss 2.2250 (2.2241)	Acc@1 14.062 (19.952)
Test: [90/157]	Time 0.240 (0.244)	Data 0.009 (0.014)	Loss 2.2250 (2.2241)	Acc@1 14.062 (19.952)
Test: [100/157]	Time 0.236 (0.242)	Data 0.008 (0.012)	Loss 2.2191 (2.2232)	Acc@1 23.438 (20.297)
Test: [100/157]	Time 0.239 (0.243)	Data 0.009 (0.013)	Loss 2.2191 (2.2232)	Acc@1 23.438 (20.297)
Test: [110/157]	Time 0.237 (0.242)	Data 0.007 (0.012)	Loss 2.2280 (2.2234)	Acc@1 15.625 (20.242)
Test: [110/157]	Time 0.238 (0.243)	Data 0.008 (0.013)	Loss 2.2280 (2.2234)	Acc@1 15.625 (20.242)
Test: [120/157]	Time 0.238 (0.242)	Data 0.008 (0.012)	Loss 2.2179 (2.2232)	Acc@1 23.438 (20.028)
Test: [120/157]	Time 0.241 (0.243)	Data 0.010 (0.012)	Loss 2.2179 (2.2232)	Acc@1 23.438 (20.028)
Test: [130/157]	Time 0.237 (0.241)	Data 0.007 (0.011)	Loss 2.1953 (2.2241)	Acc@1 21.875 (19.931)
Test: [130/157]	Time 0.239 (0.242)	Data 0.010 (0.012)	Loss 2.1953 (2.2241)	Acc@1 21.875 (19.931)
Test: [140/157]	Time 0.238 (0.241)	Data 0.007 (0.011)	Loss 2.2053 (2.2243)	Acc@1 29.688 (20.069)
Test: [140/157]	Time 0.239 (0.242)	Data 0.009 (0.012)	Loss 2.2053 (2.2243)	Acc@1 29.688 (20.069)
Test: [150/157]	Time 0.254 (0.241)	Data 0.015 (0.011)	Loss 2.2200 (2.2241)	Acc@1 20.312 (20.281)
Test: [150/157]	Time 0.238 (0.242)	Data 0.008 (0.012)	Loss 2.2200 (2.2241)	Acc@1 20.312 (20.281)
New Best Found: 20.41%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 20.41%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.014 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run stellar-elevator-9 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/1y3rt535
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182922-1y3rt535/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run copper-wave-10 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0/runs/elgb2a02
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_182922-elgb2a02/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2859 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2860) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:31:56
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2860)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183224-zq0048fm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-cherry-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/zq0048fm
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183224-1et5gfu0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-smoke-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/1et5gfu0
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.294 (2.294)	Data 0.520 (0.520)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][0/196]	Time 2.354 (2.354)	Data 0.532 (0.532)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.485 (0.665)	Data 0.196 (0.241)	Loss 2.3104 (2.3040)	Acc@1 3.125 (8.097)Epoch: [0][10/196]	Time 0.490 (0.660)	Data 0.202 (0.231)	Loss 2.3075 (2.3042)	Acc@1 9.375 (10.795)

Epoch: [0][20/196]	Time 0.486 (0.582)	Data 0.196 (0.222)	Loss 2.2975 (2.3051)	Acc@1 12.500 (9.301)
Epoch: [0][20/196]	Time 0.496 (0.580)	Data 0.203 (0.216)	Loss 2.2960 (2.3044)	Acc@1 14.062 (11.235)
Epoch: [0][30/196]	Time 0.485 (0.551)	Data 0.197 (0.211)	Loss 2.3079 (2.3042)	Acc@1 10.938 (10.988)
Epoch: [0][30/196]	Time 0.491 (0.553)	Data 0.202 (0.216)	Loss 2.3025 (2.3043)	Acc@1 7.812 (9.980)
Epoch: [0][40/196]	Time 0.491 (0.538)	Data 0.204 (0.213)	Loss 2.3170 (2.3041)	Acc@1 9.375 (9.985)
Epoch: [0][40/196]	Time 0.491 (0.536)	Data 0.201 (0.209)	Loss 2.3066 (2.3044)	Acc@1 3.125 (10.518)
Epoch: [0][50/196]	Time 0.493 (0.527)	Data 0.203 (0.207)	Loss 2.3177 (2.3051)	Acc@1 7.812 (10.018)Epoch: [0][50/196]	Time 0.496 (0.528)	Data 0.208 (0.211)	Loss 2.3063 (2.3044)	Acc@1 9.375 (9.926)

Epoch: [0][60/196]	Time 0.484 (0.521)	Data 0.198 (0.206)	Loss 2.3067 (2.3050)	Acc@1 7.812 (9.810)
Epoch: [0][60/196]	Time 0.488 (0.522)	Data 0.200 (0.209)	Loss 2.3058 (2.3047)	Acc@1 9.375 (9.836)
Epoch: [0][70/196]	Time 0.488 (0.518)	Data 0.200 (0.208)	Loss 2.2917 (2.3040)	Acc@1 4.688 (9.815)
Epoch: [0][70/196]	Time 0.490 (0.517)	Data 0.201 (0.205)	Loss 2.2874 (2.3047)	Acc@1 9.375 (10.189)
Epoch: [0][80/196]	Time 0.494 (0.514)	Data 0.206 (0.208)	Loss 2.3117 (2.3052)	Acc@1 7.812 (9.587)
Epoch: [0][80/196]	Time 0.493 (0.514)	Data 0.202 (0.205)	Loss 2.2964 (2.3051)	Acc@1 6.250 (10.031)
Epoch: [0][90/196]	Time 0.489 (0.512)	Data 0.203 (0.207)	Loss 2.3125 (2.3055)	Acc@1 10.938 (9.512)
Epoch: [0][90/196]	Time 0.493 (0.511)	Data 0.202 (0.204)	Loss 2.3070 (2.3054)	Acc@1 6.250 (9.976)
Epoch: [0][100/196]	Time 0.488 (0.509)	Data 0.199 (0.204)	Loss 2.3081 (2.3053)	Acc@1 6.250 (10.040)
Epoch: [0][100/196]	Time 0.490 (0.510)	Data 0.200 (0.207)	Loss 2.3091 (2.3057)	Acc@1 17.188 (9.344)
Epoch: [0][110/196]	Time 0.490 (0.508)	Data 0.202 (0.206)	Loss 2.3027 (2.3059)	Acc@1 14.062 (9.445)
Epoch: [0][110/196]	Time 0.489 (0.507)	Data 0.200 (0.204)	Loss 2.2969 (2.3051)	Acc@1 12.500 (10.008)
Epoch: [0][120/196]	Time 0.491 (0.507)	Data 0.203 (0.206)	Loss 2.3142 (2.3061)	Acc@1 9.375 (9.336)
Epoch: [0][120/196]	Time 0.492 (0.506)	Data 0.201 (0.204)	Loss 2.2990 (2.3055)	Acc@1 1.562 (9.788)
Epoch: [0][130/196]	Time 0.493 (0.505)	Data 0.203 (0.204)	Loss 2.3167 (2.3058)	Acc@1 3.125 (9.649)
Epoch: [0][130/196]	Time 0.493 (0.505)	Data 0.205 (0.206)	Loss 2.3225 (2.3061)	Acc@1 6.250 (9.423)
Epoch: [0][140/196]	Time 0.489 (0.504)	Data 0.200 (0.203)	Loss 2.2839 (2.3058)	Acc@1 12.500 (9.630)
Epoch: [0][140/196]	Time 0.492 (0.504)	Data 0.202 (0.206)	Loss 2.3131 (2.3061)	Acc@1 9.375 (9.497)
Epoch: [0][150/196]	Time 0.489 (0.503)	Data 0.199 (0.203)	Loss 2.3064 (2.3058)	Acc@1 6.250 (9.613)
Epoch: [0][150/196]	Time 0.490 (0.503)	Data 0.201 (0.205)	Loss 2.3148 (2.3062)	Acc@1 7.812 (9.489)
Epoch: [0][160/196]	Time 0.489 (0.503)	Data 0.202 (0.205)	Loss 2.2975 (2.3059)	Acc@1 7.812 (9.550)
Epoch: [0][160/196]	Time 0.500 (0.502)	Data 0.211 (0.203)	Loss 2.3082 (2.3058)	Acc@1 7.812 (9.491)
Epoch: [0][170/196]	Time 0.492 (0.502)	Data 0.205 (0.205)	Loss 2.3261 (2.3060)	Acc@1 7.812 (9.695)
Epoch: [0][170/196]	Time 0.497 (0.502)	Data 0.206 (0.203)	Loss 2.3150 (2.3058)	Acc@1 10.938 (9.512)
Epoch: [0][180/196]	Time 0.491 (0.502)	Data 0.203 (0.205)	Loss 2.3013 (2.3061)	Acc@1 9.375 (9.703)Epoch: [0][180/196]	Time 0.487 (0.501)	Data 0.199 (0.203)	Loss 2.3097 (2.3060)	Acc@1 9.375 (9.461)

Epoch: [0][190/196]	Time 0.491 (0.501)	Data 0.203 (0.205)	Loss 2.3053 (2.3063)	Acc@1 14.062 (9.612)
Epoch: [0][190/196]	Time 0.490 (0.501)	Data 0.201 (0.203)	Loss 2.3106 (2.3060)	Acc@1 10.938 (9.465)
Test: [0/157]	Time 0.680 (0.680)	Data 0.428 (0.428)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.714 (0.714)	Data 0.457 (0.457)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.264 (0.291)	Data 0.010 (0.046)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.246 (0.294)	Data 0.005 (0.049)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.248 (0.271)	Data 0.007 (0.027)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.249 (0.273)	Data 0.008 (0.029)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.249 (0.264)	Data 0.007 (0.021)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.247 (0.265)	Data 0.007 (0.022)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.251 (0.260)	Data 0.008 (0.017)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.249 (0.261)	Data 0.007 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.250 (0.258)	Data 0.007 (0.015)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.250 (0.259)	Data 0.007 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.249 (0.257)	Data 0.007 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.249 (0.257)	Data 0.007 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.248 (0.256)	Data 0.006 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.249 (0.256)	Data 0.007 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.248 (0.255)	Data 0.007 (0.012)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.250 (0.255)	Data 0.007 (0.013)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.250 (0.254)	Data 0.007 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.247 (0.255)	Data 0.006 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.250 (0.254)	Data 0.007 (0.011)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.248 (0.254)	Data 0.007 (0.011)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.249 (0.254)	Data 0.007 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.250 (0.254)	Data 0.008 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.250 (0.253)	Data 0.008 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.250 (0.253)	Data 0.008 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.249 (0.253)	Data 0.007 (0.011)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.250 (0.253)	Data 0.007 (0.010)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.250 (0.253)	Data 0.007 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.249 (0.253)	Data 0.006 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.249 (0.253)	Data 0.006 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run faithful-smoke-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/1et5gfu0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183224-1et5gfu0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run polished-cherry-1 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/zq0048fm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183224-zq0048fm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4720 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4719) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:35:08
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4719)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183537-6n6ctpkv
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183537-3scy117x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-wave-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/6n6ctpkv
wandb: Syncing run stellar-armadillo-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/3scy117x
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.115 (2.115)	Data 0.405 (0.405)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.301 (2.301)	Data 0.483 (0.483)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][10/196]	Time 0.486 (0.651)	Data 0.197 (0.223)	Loss 2.3026 (2.3029)	Acc@1 7.812 (9.659)Epoch: [0][10/196]	Time 0.487 (0.662)	Data 0.196 (0.242)	Loss 2.3012 (2.3025)	Acc@1 12.500 (10.227)

Epoch: [0][20/196]	Time 0.487 (0.573)	Data 0.202 (0.212)	Loss 2.3024 (2.3027)	Acc@1 12.500 (9.896)
Epoch: [0][20/196]	Time 0.491 (0.579)	Data 0.202 (0.221)	Loss 2.3039 (2.3027)	Acc@1 4.688 (9.673)
Epoch: [0][30/196]	Time 0.489 (0.547)	Data 0.203 (0.209)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.181)Epoch: [0][30/196]	Time 0.490 (0.550)	Data 0.201 (0.214)	Loss 2.3027 (2.3027)	Acc@1 12.500 (9.173)

Epoch: [0][40/196]	Time 0.490 (0.533)	Data 0.202 (0.208)	Loss 2.3024 (2.3027)	Acc@1 7.812 (9.794)
Epoch: [0][40/196]	Time 0.492 (0.536)	Data 0.200 (0.211)	Loss 2.3029 (2.3026)	Acc@1 9.375 (9.604)
Epoch: [0][50/196]	Time 0.492 (0.525)	Data 0.203 (0.207)	Loss 2.3026 (2.3027)	Acc@1 12.500 (9.804)
Epoch: [0][50/196]	Time 0.492 (0.527)	Data 0.202 (0.209)	Loss 2.3037 (2.3026)	Acc@1 9.375 (9.835)
Epoch: [0][60/196]	Time 0.491 (0.520)	Data 0.203 (0.206)	Loss 2.3038 (2.3027)	Acc@1 6.250 (9.836)
Epoch: [0][60/196]	Time 0.491 (0.521)	Data 0.204 (0.208)	Loss 2.3029 (2.3026)	Acc@1 12.500 (9.785)
Epoch: [0][70/196]	Time 0.491 (0.515)	Data 0.203 (0.206)	Loss 2.2995 (2.3026)	Acc@1 26.562 (10.299)
Epoch: [0][70/196]	Time 0.495 (0.517)	Data 0.202 (0.207)	Loss 2.3010 (2.3026)	Acc@1 12.500 (10.101)
Epoch: [0][80/196]	Time 0.486 (0.512)	Data 0.200 (0.206)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.012)
Epoch: [0][80/196]	Time 0.488 (0.514)	Data 0.197 (0.206)	Loss 2.3033 (2.3026)	Acc@1 6.250 (9.954)
Epoch: [0][90/196]	Time 0.489 (0.510)	Data 0.203 (0.205)	Loss 2.3025 (2.3027)	Acc@1 10.938 (10.130)
Epoch: [0][90/196]	Time 0.493 (0.511)	Data 0.202 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.027)
Epoch: [0][100/196]	Time 0.490 (0.510)	Data 0.199 (0.205)	Loss 2.3006 (2.3026)	Acc@1 14.062 (9.994)Epoch: [0][100/196]	Time 0.496 (0.508)	Data 0.208 (0.205)	Loss 2.3035 (2.3027)	Acc@1 3.125 (10.303)

Epoch: [0][110/196]	Time 0.486 (0.507)	Data 0.200 (0.205)	Loss 2.3023 (2.3027)	Acc@1 10.938 (10.248)
Epoch: [0][110/196]	Time 0.493 (0.508)	Data 0.202 (0.205)	Loss 2.3032 (2.3026)	Acc@1 10.938 (9.938)
Epoch: [0][120/196]	Time 0.488 (0.507)	Data 0.199 (0.204)	Loss 2.3015 (2.3026)	Acc@1 9.375 (10.085)
Epoch: [0][120/196]	Time 0.494 (0.506)	Data 0.204 (0.205)	Loss 2.3021 (2.3026)	Acc@1 12.500 (10.292)
Epoch: [0][130/196]	Time 0.493 (0.505)	Data 0.203 (0.205)	Loss 2.3045 (2.3026)	Acc@1 7.812 (10.222)
Epoch: [0][130/196]	Time 0.493 (0.506)	Data 0.200 (0.204)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.138)
Epoch: [0][140/196]	Time 0.491 (0.504)	Data 0.203 (0.205)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.073)
Epoch: [0][140/196]	Time 0.494 (0.505)	Data 0.204 (0.204)	Loss 2.3023 (2.3026)	Acc@1 12.500 (10.250)
Epoch: [0][150/196]	Time 0.493 (0.503)	Data 0.204 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.006)Epoch: [0][150/196]	Time 0.493 (0.504)	Data 0.203 (0.204)	Loss 2.3030 (2.3026)	Acc@1 9.375 (10.192)

Epoch: [0][160/196]	Time 0.492 (0.502)	Data 0.203 (0.205)	Loss 2.3037 (2.3027)	Acc@1 6.250 (9.812)
Epoch: [0][160/196]	Time 0.494 (0.503)	Data 0.204 (0.204)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.122)
Epoch: [0][170/196]	Time 0.489 (0.502)	Data 0.203 (0.205)	Loss 2.3024 (2.3027)	Acc@1 14.062 (9.905)
Epoch: [0][170/196]	Time 0.493 (0.503)	Data 0.204 (0.204)	Loss 2.3022 (2.3026)	Acc@1 14.062 (10.106)
Epoch: [0][180/196]	Time 0.492 (0.501)	Data 0.205 (0.205)	Loss 2.3029 (2.3026)	Acc@1 9.375 (10.031)
Epoch: [0][180/196]	Time 0.494 (0.502)	Data 0.204 (0.204)	Loss 2.3018 (2.3026)	Acc@1 9.375 (10.040)
Epoch: [0][190/196]	Time 0.493 (0.501)	Data 0.204 (0.205)	Loss 2.3035 (2.3027)	Acc@1 3.125 (9.858)
Epoch: [0][190/196]	Time 0.494 (0.502)	Data 0.203 (0.204)	Loss 2.3029 (2.3026)	Acc@1 10.938 (10.095)
Test: [0/157]	Time 0.669 (0.669)	Data 0.414 (0.414)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [0/157]	Time 0.704 (0.704)	Data 0.446 (0.446)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [10/157]	Time 0.248 (0.289)	Data 0.005 (0.045)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [10/157]	Time 0.246 (0.295)	Data 0.005 (0.049)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [20/157]	Time 0.252 (0.270)	Data 0.009 (0.027)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [20/157]	Time 0.247 (0.273)	Data 0.007 (0.028)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [30/157]	Time 0.249 (0.265)	Data 0.008 (0.021)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [30/157]	Time 0.250 (0.265)	Data 0.008 (0.021)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [40/157]	Time 0.252 (0.262)	Data 0.008 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [40/157]	Time 0.250 (0.262)	Data 0.008 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [50/157]	Time 0.249 (0.259)	Data 0.006 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [50/157]	Time 0.252 (0.260)	Data 0.007 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [60/157]	Time 0.249 (0.258)	Data 0.006 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [60/157]	Time 0.249 (0.259)	Data 0.007 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [70/157]	Time 0.249 (0.256)	Data 0.007 (0.013)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [70/157]	Time 0.251 (0.258)	Data 0.008 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [80/157]	Time 0.252 (0.256)	Data 0.008 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [80/157]	Time 0.248 (0.257)	Data 0.008 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [90/157]	Time 0.249 (0.255)	Data 0.007 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [90/157]	Time 0.248 (0.256)	Data 0.007 (0.013)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [100/157]	Time 0.255 (0.255)	Data 0.010 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [100/157]	Time 0.249 (0.255)	Data 0.007 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.250 (0.254)	Data 0.006 (0.011)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [110/157]	Time 0.250 (0.255)	Data 0.007 (0.012)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [120/157]	Time 0.251 (0.254)	Data 0.008 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [120/157]	Time 0.250 (0.254)	Data 0.008 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [130/157]	Time 0.248 (0.254)	Data 0.006 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [130/157]	Time 0.251 (0.254)	Data 0.007 (0.011)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [140/157]	Time 0.251 (0.254)	Data 0.007 (0.011)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [150/157]	Time 0.250 (0.253)	Data 0.007 (0.010)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
Test: [150/157]	Time 0.252 (0.254)	Data 0.008 (0.011)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run good-wave-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/6n6ctpkv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183537-6n6ctpkv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run stellar-armadillo-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/3scy117x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183537-3scy117x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 6446 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6445) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:38:22
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 6445)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization : Normalization :   TrueTrue

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183850-go8a9ja0
wandb: Run `wandb offline` to turn off syncing.
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_183850-cmveokrh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-vortex-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/cmveokrh
wandb: Syncing run rich-pyramid-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/go8a9ja0
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.194 (2.194)	Data 0.491 (0.491)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.282 (2.282)	Data 0.522 (0.522)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][10/196]	Time 0.485 (0.655)	Data 0.195 (0.235)	Loss 2.3012 (2.3025)	Acc@1 12.500 (10.227)
Epoch: [0][10/196]	Time 0.488 (0.652)	Data 0.199 (0.228)	Loss 2.3026 (2.3029)	Acc@1 7.812 (9.659)
Epoch: [0][20/196]	Time 0.498 (0.576)	Data 0.208 (0.216)	Loss 2.3024 (2.3027)	Acc@1 12.500 (9.896)
Epoch: [0][20/196]	Time 0.495 (0.578)	Data 0.202 (0.219)	Loss 2.3039 (2.3027)	Acc@1 4.688 (9.673)
Epoch: [0][30/196]	Time 0.490 (0.549)	Data 0.201 (0.212)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.181)
Epoch: [0][30/196]	Time 0.491 (0.550)	Data 0.200 (0.213)	Loss 2.3027 (2.3027)	Acc@1 12.500 (9.173)
Epoch: [0][40/196]	Time 0.494 (0.535)	Data 0.207 (0.210)	Loss 2.3024 (2.3027)	Acc@1 7.812 (9.794)
Epoch: [0][40/196]	Time 0.491 (0.536)	Data 0.201 (0.210)	Loss 2.3029 (2.3026)	Acc@1 9.375 (9.604)
Epoch: [0][50/196]	Time 0.494 (0.527)	Data 0.206 (0.209)	Loss 2.3026 (2.3027)	Acc@1 12.500 (9.804)
Epoch: [0][50/196]	Time 0.491 (0.527)	Data 0.202 (0.208)	Loss 2.3037 (2.3026)	Acc@1 9.375 (9.835)
Epoch: [0][60/196]	Time 0.493 (0.521)	Data 0.207 (0.208)	Loss 2.3038 (2.3027)	Acc@1 6.250 (9.836)
Epoch: [0][60/196]	Time 0.493 (0.522)	Data 0.203 (0.207)	Loss 2.3029 (2.3026)	Acc@1 12.500 (9.785)
Epoch: [0][70/196]	Time 0.492 (0.517)	Data 0.205 (0.207)	Loss 2.2995 (2.3026)	Acc@1 26.562 (10.299)
Epoch: [0][70/196]	Time 0.492 (0.517)	Data 0.200 (0.207)	Loss 2.3010 (2.3026)	Acc@1 12.500 (10.101)
Epoch: [0][80/196]	Time 0.492 (0.514)	Data 0.203 (0.207)	Loss 2.3027 (2.3027)	Acc@1 7.812 (10.012)Epoch: [0][80/196]	Time 0.492 (0.514)	Data 0.202 (0.206)	Loss 2.3033 (2.3026)	Acc@1 6.250 (9.954)

Epoch: [0][90/196]	Time 0.497 (0.512)	Data 0.207 (0.207)	Loss 2.3025 (2.3027)	Acc@1 10.938 (10.130)Epoch: [0][90/196]	Time 0.490 (0.512)	Data 0.201 (0.206)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.027)

Epoch: [0][100/196]	Time 0.497 (0.510)	Data 0.210 (0.206)	Loss 2.3035 (2.3027)	Acc@1 3.125 (10.303)
Epoch: [0][100/196]	Time 0.491 (0.510)	Data 0.200 (0.205)	Loss 2.3006 (2.3026)	Acc@1 14.062 (9.994)
Epoch: [0][110/196]	Time 0.491 (0.508)	Data 0.203 (0.206)	Loss 2.3023 (2.3027)	Acc@1 10.938 (10.248)
Epoch: [0][110/196]	Time 0.491 (0.508)	Data 0.202 (0.205)	Loss 2.3032 (2.3026)	Acc@1 10.938 (9.938)
Epoch: [0][120/196]	Time 0.495 (0.507)	Data 0.205 (0.206)	Loss 2.3021 (2.3026)	Acc@1 12.500 (10.292)
Epoch: [0][120/196]	Time 0.493 (0.507)	Data 0.202 (0.205)	Loss 2.3015 (2.3026)	Acc@1 9.375 (10.085)
Epoch: [0][130/196]	Time 0.493 (0.506)	Data 0.206 (0.206)	Loss 2.3045 (2.3026)	Acc@1 7.812 (10.222)
Epoch: [0][130/196]	Time 0.491 (0.506)	Data 0.202 (0.204)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.138)
Epoch: [0][140/196]	Time 0.491 (0.505)	Data 0.202 (0.204)	Loss 2.3023 (2.3026)	Acc@1 12.500 (10.250)
Epoch: [0][140/196]	Time 0.501 (0.505)	Data 0.211 (0.206)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.073)
Epoch: [0][150/196]	Time 0.493 (0.504)	Data 0.205 (0.205)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.006)
Epoch: [0][150/196]	Time 0.495 (0.504)	Data 0.203 (0.204)	Loss 2.3030 (2.3026)	Acc@1 9.375 (10.192)
Epoch: [0][160/196]	Time 0.497 (0.503)	Data 0.208 (0.205)	Loss 2.3037 (2.3027)	Acc@1 6.250 (9.812)Epoch: [0][160/196]	Time 0.494 (0.503)	Data 0.204 (0.204)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.122)

Epoch: [0][170/196]	Time 0.486 (0.502)	Data 0.199 (0.205)	Loss 2.3024 (2.3027)	Acc@1 14.062 (9.905)
Epoch: [0][170/196]	Time 0.488 (0.503)	Data 0.200 (0.204)	Loss 2.3022 (2.3026)	Acc@1 14.062 (10.106)
Epoch: [0][180/196]	Time 0.491 (0.502)	Data 0.202 (0.204)	Loss 2.3018 (2.3026)	Acc@1 9.375 (10.040)
Epoch: [0][180/196]	Time 0.492 (0.502)	Data 0.205 (0.205)	Loss 2.3029 (2.3026)	Acc@1 9.375 (10.031)
Epoch: [0][190/196]	Time 0.495 (0.501)	Data 0.207 (0.205)	Loss 2.3035 (2.3027)	Acc@1 3.125 (9.858)
Epoch: [0][190/196]	Time 0.497 (0.501)	Data 0.202 (0.203)	Loss 2.3029 (2.3026)	Acc@1 10.938 (10.095)
Test: [0/157]	Time 0.690 (0.690)	Data 0.433 (0.433)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [0/157]	Time 0.719 (0.719)	Data 0.459 (0.459)	Loss 2.3043 (2.3043)	Acc@1 3.125 (3.125)
Test: [10/157]	Time 0.249 (0.290)	Data 0.006 (0.046)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [10/157]	Time 0.247 (0.295)	Data 0.005 (0.049)	Loss 2.3023 (2.3029)	Acc@1 9.375 (9.659)
Test: [20/157]	Time 0.247 (0.270)	Data 0.007 (0.027)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [20/157]	Time 0.248 (0.274)	Data 0.007 (0.029)	Loss 2.3039 (2.3026)	Acc@1 7.812 (9.747)
Test: [30/157]	Time 0.249 (0.263)	Data 0.007 (0.021)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [30/157]	Time 0.250 (0.266)	Data 0.007 (0.022)	Loss 2.3023 (2.3026)	Acc@1 12.500 (9.778)
Test: [40/157]	Time 0.251 (0.260)	Data 0.007 (0.017)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [40/157]	Time 0.247 (0.262)	Data 0.007 (0.018)	Loss 2.3022 (2.3026)	Acc@1 10.938 (10.442)
Test: [50/157]	Time 0.250 (0.258)	Data 0.008 (0.015)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [50/157]	Time 0.248 (0.259)	Data 0.007 (0.016)	Loss 2.3033 (2.3026)	Acc@1 4.688 (10.355)
Test: [60/157]	Time 0.250 (0.256)	Data 0.007 (0.014)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [60/157]	Time 0.251 (0.258)	Data 0.007 (0.015)	Loss 2.3029 (2.3025)	Acc@1 7.812 (10.579)
Test: [70/157]	Time 0.251 (0.255)	Data 0.007 (0.013)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [70/157]	Time 0.251 (0.257)	Data 0.008 (0.014)	Loss 2.3015 (2.3025)	Acc@1 9.375 (10.497)
Test: [80/157]	Time 0.251 (0.255)	Data 0.008 (0.012)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [80/157]	Time 0.250 (0.256)	Data 0.007 (0.013)	Loss 2.3036 (2.3026)	Acc@1 10.938 (10.185)
Test: [90/157]	Time 0.251 (0.254)	Data 0.007 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [90/157]	Time 0.248 (0.255)	Data 0.006 (0.012)	Loss 2.3023 (2.3026)	Acc@1 10.938 (10.062)
Test: [100/157]	Time 0.248 (0.254)	Data 0.007 (0.011)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [100/157]	Time 0.249 (0.254)	Data 0.008 (0.012)	Loss 2.3018 (2.3026)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.248 (0.253)	Data 0.007 (0.011)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [110/157]	Time 0.248 (0.254)	Data 0.007 (0.011)	Loss 2.3023 (2.3026)	Acc@1 15.625 (10.065)
Test: [120/157]	Time 0.251 (0.253)	Data 0.007 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [120/157]	Time 0.247 (0.253)	Data 0.007 (0.011)	Loss 2.3033 (2.3026)	Acc@1 6.250 (10.085)
Test: [130/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [130/157]	Time 0.250 (0.253)	Data 0.007 (0.010)	Loss 2.3026 (2.3026)	Acc@1 6.250 (10.007)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [140/157]	Time 0.249 (0.253)	Data 0.007 (0.010)	Loss 2.3020 (2.3026)	Acc@1 12.500 (10.007)
Test: [150/157]	Time 0.250 (0.253)	Data 0.006 (0.010)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
Test: [150/157]	Time 0.250 (0.253)	Data 0.009 (0.010)	Loss 2.3031 (2.3026)	Acc@1 9.375 (9.975)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run rich-pyramid-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/go8a9ja0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183850-go8a9ja0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run wobbly-vortex-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/cmveokrh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_183850-cmveokrh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 8174 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 8173) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:41:34
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 8173)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184203-4mm9h43i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-rain-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/4mm9h43i
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184204-95mtbvo6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-moon-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/95mtbvo6
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.163 (2.163)	Data 0.467 (0.467)	Loss 2.3045 (2.3045)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.207 (2.207)	Data 0.394 (0.394)	Loss 2.3036 (2.3036)	Acc@1 7.812 (7.812)
Epoch: [0][10/196]	Time 0.490 (0.646)	Data 0.199 (0.216)	Loss 2.3104 (2.3040)	Acc@1 3.125 (8.097)Epoch: [0][10/196]	Time 0.495 (0.664)	Data 0.206 (0.246)	Loss 2.3075 (2.3042)	Acc@1 9.375 (10.795)

Epoch: [0][20/196]	Time 0.491 (0.572)	Data 0.202 (0.209)	Loss 2.2975 (2.3051)	Acc@1 12.500 (9.301)
Epoch: [0][20/196]	Time 0.495 (0.582)	Data 0.204 (0.224)	Loss 2.2960 (2.3044)	Acc@1 14.062 (11.235)
Epoch: [0][30/196]	Time 0.487 (0.546)	Data 0.200 (0.207)	Loss 2.3025 (2.3043)	Acc@1 7.812 (9.980)Epoch: [0][30/196]	Time 0.492 (0.552)	Data 0.203 (0.217)	Loss 2.3079 (2.3042)	Acc@1 10.938 (10.988)

Epoch: [0][40/196]	Time 0.490 (0.538)	Data 0.202 (0.214)	Loss 2.3066 (2.3044)	Acc@1 3.125 (10.518)
Epoch: [0][40/196]	Time 0.494 (0.533)	Data 0.204 (0.206)	Loss 2.3170 (2.3041)	Acc@1 9.375 (9.985)
Epoch: [0][50/196]	Time 0.490 (0.525)	Data 0.202 (0.205)	Loss 2.3063 (2.3044)	Acc@1 9.375 (9.926)
Epoch: [0][50/196]	Time 0.494 (0.529)	Data 0.204 (0.212)	Loss 2.3177 (2.3051)	Acc@1 7.812 (10.018)
Epoch: [0][60/196]	Time 0.490 (0.520)	Data 0.201 (0.205)	Loss 2.3058 (2.3047)	Acc@1 9.375 (9.836)
Epoch: [0][60/196]	Time 0.492 (0.523)	Data 0.203 (0.210)	Loss 2.3067 (2.3050)	Acc@1 7.812 (9.810)
Epoch: [0][70/196]	Time 0.487 (0.516)	Data 0.200 (0.205)	Loss 2.2917 (2.3040)	Acc@1 4.688 (9.815)
Epoch: [0][70/196]	Time 0.489 (0.519)	Data 0.201 (0.209)	Loss 2.2874 (2.3047)	Acc@1 9.375 (10.189)
Epoch: [0][80/196]	Time 0.493 (0.513)	Data 0.205 (0.205)	Loss 2.3117 (2.3052)	Acc@1 7.812 (9.587)
Epoch: [0][80/196]	Time 0.491 (0.515)	Data 0.204 (0.208)	Loss 2.2964 (2.3051)	Acc@1 6.250 (10.031)
Epoch: [0][90/196]	Time 0.493 (0.510)	Data 0.203 (0.205)	Loss 2.3125 (2.3055)	Acc@1 10.938 (9.512)
Epoch: [0][90/196]	Time 0.492 (0.513)	Data 0.202 (0.208)	Loss 2.3070 (2.3054)	Acc@1 6.250 (9.976)
Epoch: [0][100/196]	Time 0.487 (0.509)	Data 0.200 (0.204)	Loss 2.3091 (2.3057)	Acc@1 17.188 (9.344)
Epoch: [0][100/196]	Time 0.488 (0.511)	Data 0.201 (0.207)	Loss 2.3081 (2.3053)	Acc@1 6.250 (10.040)
Epoch: [0][110/196]	Time 0.490 (0.509)	Data 0.201 (0.207)	Loss 2.2969 (2.3051)	Acc@1 12.500 (10.008)
Epoch: [0][110/196]	Time 0.495 (0.507)	Data 0.206 (0.204)	Loss 2.3027 (2.3059)	Acc@1 14.062 (9.445)
Epoch: [0][120/196]	Time 0.493 (0.508)	Data 0.204 (0.207)	Loss 2.2990 (2.3055)	Acc@1 1.562 (9.788)
Epoch: [0][120/196]	Time 0.497 (0.506)	Data 0.207 (0.204)	Loss 2.3142 (2.3061)	Acc@1 9.375 (9.336)
Epoch: [0][130/196]	Time 0.488 (0.505)	Data 0.201 (0.204)	Loss 2.3225 (2.3061)	Acc@1 6.250 (9.423)
Epoch: [0][130/196]	Time 0.489 (0.506)	Data 0.203 (0.207)	Loss 2.3167 (2.3058)	Acc@1 3.125 (9.649)
Epoch: [0][140/196]	Time 0.492 (0.504)	Data 0.205 (0.204)	Loss 2.3131 (2.3061)	Acc@1 9.375 (9.497)
Epoch: [0][140/196]	Time 0.493 (0.506)	Data 0.204 (0.206)	Loss 2.2839 (2.3058)	Acc@1 12.500 (9.630)
Epoch: [0][150/196]	Time 0.491 (0.505)	Data 0.202 (0.206)	Loss 2.3064 (2.3058)	Acc@1 6.250 (9.613)
Epoch: [0][150/196]	Time 0.495 (0.503)	Data 0.207 (0.204)	Loss 2.3148 (2.3062)	Acc@1 7.812 (9.489)
Epoch: [0][160/196]	Time 0.492 (0.504)	Data 0.203 (0.206)	Loss 2.3082 (2.3058)	Acc@1 7.812 (9.491)
Epoch: [0][160/196]	Time 0.493 (0.503)	Data 0.203 (0.204)	Loss 2.2975 (2.3059)	Acc@1 7.812 (9.550)
Epoch: [0][170/196]	Time 0.490 (0.503)	Data 0.202 (0.206)	Loss 2.3150 (2.3058)	Acc@1 10.938 (9.512)
Epoch: [0][170/196]	Time 0.492 (0.502)	Data 0.204 (0.204)	Loss 2.3261 (2.3060)	Acc@1 7.812 (9.695)
Epoch: [0][180/196]	Time 0.491 (0.502)	Data 0.201 (0.204)	Loss 2.3013 (2.3061)	Acc@1 9.375 (9.703)
Epoch: [0][180/196]	Time 0.503 (0.503)	Data 0.214 (0.206)	Loss 2.3097 (2.3060)	Acc@1 9.375 (9.461)
Epoch: [0][190/196]	Time 0.492 (0.501)	Data 0.206 (0.204)	Loss 2.3053 (2.3063)	Acc@1 14.062 (9.612)
Epoch: [0][190/196]	Time 0.493 (0.502)	Data 0.203 (0.206)	Loss 2.3106 (2.3060)	Acc@1 10.938 (9.465)
Test: [0/157]	Time 0.673 (0.673)	Data 0.412 (0.412)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.719 (0.719)	Data 0.463 (0.463)	Loss 2.3060 (2.3060)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.248 (0.290)	Data 0.005 (0.045)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.258 (0.294)	Data 0.009 (0.049)	Loss 2.3110 (2.3052)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.251 (0.272)	Data 0.007 (0.027)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.248 (0.272)	Data 0.007 (0.029)	Loss 2.3025 (2.3061)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.249 (0.265)	Data 0.007 (0.022)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.251 (0.266)	Data 0.007 (0.021)	Loss 2.3102 (2.3067)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.248 (0.261)	Data 0.007 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.253 (0.263)	Data 0.008 (0.018)	Loss 2.2993 (2.3063)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.250 (0.259)	Data 0.008 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.252 (0.261)	Data 0.007 (0.016)	Loss 2.3035 (2.3062)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.250 (0.258)	Data 0.009 (0.015)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.250 (0.259)	Data 0.006 (0.014)	Loss 2.3166 (2.3065)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.252 (0.257)	Data 0.007 (0.014)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.249 (0.258)	Data 0.006 (0.013)	Loss 2.3112 (2.3062)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.249 (0.256)	Data 0.008 (0.013)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.251 (0.257)	Data 0.007 (0.013)	Loss 2.3028 (2.3064)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.250 (0.255)	Data 0.007 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.251 (0.256)	Data 0.007 (0.012)	Loss 2.3009 (2.3061)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.251 (0.255)	Data 0.009 (0.012)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.251 (0.256)	Data 0.007 (0.012)	Loss 2.3025 (2.3057)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.253 (0.254)	Data 0.009 (0.012)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.253 (0.255)	Data 0.007 (0.011)	Loss 2.3149 (2.3055)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.250 (0.254)	Data 0.008 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.252 (0.255)	Data 0.007 (0.011)	Loss 2.2827 (2.3055)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.252 (0.254)	Data 0.008 (0.011)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.251 (0.255)	Data 0.007 (0.011)	Loss 2.3197 (2.3058)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.250 (0.254)	Data 0.008 (0.011)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.251 (0.254)	Data 0.006 (0.010)	Loss 2.2965 (2.3060)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.251 (0.253)	Data 0.008 (0.011)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.250 (0.254)	Data 0.008 (0.010)	Loss 2.3044 (2.3059)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: üöÄ View run apricot-moon-8 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/95mtbvo6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184204-95mtbvo6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run pretty-rain-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention/runs/4mm9h43i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184203-4mm9h43i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 9888 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 9889) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:44:47
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 9889)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184516-svukeejf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-leaf-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/svukeejf
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184517-b72xg7c0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-jazz-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/b72xg7c0
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.082 (2.082)	Data 0.382 (0.382)	Loss 2.2946 (2.2946)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.215 (2.215)	Data 0.419 (0.419)	Loss 2.2964 (2.2964)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.543 (0.691)	Data 0.204 (0.224)	Loss 2.2867 (2.3281)	Acc@1 14.062 (9.091)
Epoch: [0][10/196]	Time 0.545 (0.716)	Data 0.214 (0.258)	Loss 2.3543 (2.3239)	Acc@1 12.500 (10.511)
Epoch: [0][20/196]	Time 0.550 (0.633)	Data 0.213 (0.233)	Loss 2.3459 (2.3479)	Acc@1 9.375 (10.417)
Epoch: [0][20/196]	Time 0.548 (0.621)	Data 0.213 (0.217)	Loss 2.3345 (2.3534)	Acc@1 4.688 (9.226)
Epoch: [0][30/196]	Time 0.543 (0.605)	Data 0.209 (0.226)	Loss 2.3809 (2.3564)	Acc@1 10.938 (10.887)
Epoch: [0][30/196]	Time 0.548 (0.596)	Data 0.215 (0.215)	Loss 2.3518 (2.3669)	Acc@1 7.812 (8.821)
Epoch: [0][40/196]	Time 0.556 (0.590)	Data 0.215 (0.222)	Loss 2.3750 (2.3567)	Acc@1 6.250 (10.366)
Epoch: [0][40/196]	Time 0.546 (0.584)	Data 0.208 (0.213)	Loss 2.4405 (2.3689)	Acc@1 4.688 (8.765)
Epoch: [0][50/196]	Time 0.544 (0.576)	Data 0.210 (0.213)	Loss 2.3949 (2.3696)	Acc@1 9.375 (8.640)
Epoch: [0][50/196]	Time 0.545 (0.581)	Data 0.210 (0.219)	Loss 2.4310 (2.3571)	Acc@1 7.812 (10.049)
Epoch: [0][60/196]	Time 0.550 (0.571)	Data 0.214 (0.212)	Loss 2.4033 (2.3767)	Acc@1 9.375 (8.991)Epoch: [0][60/196]	Time 0.544 (0.575)	Data 0.206 (0.218)	Loss 2.3826 (2.3625)	Acc@1 7.812 (10.015)

Epoch: [0][70/196]	Time 0.542 (0.567)	Data 0.208 (0.212)	Loss 2.3150 (2.3749)	Acc@1 4.688 (9.045)
Epoch: [0][70/196]	Time 0.542 (0.570)	Data 0.205 (0.216)	Loss 2.4304 (2.3674)	Acc@1 9.375 (9.837)
Epoch: [0][80/196]	Time 0.549 (0.564)	Data 0.214 (0.212)	Loss 2.3463 (2.3748)	Acc@1 14.062 (9.375)
Epoch: [0][80/196]	Time 0.545 (0.567)	Data 0.206 (0.215)	Loss 2.3289 (2.3684)	Acc@1 14.062 (9.645)
Epoch: [0][90/196]	Time 0.544 (0.562)	Data 0.208 (0.211)	Loss 2.4122 (2.3752)	Acc@1 15.625 (9.461)
Epoch: [0][90/196]	Time 0.546 (0.564)	Data 0.209 (0.214)	Loss 2.3779 (2.3698)	Acc@1 10.938 (9.684)
Epoch: [0][100/196]	Time 0.549 (0.560)	Data 0.214 (0.211)	Loss 2.5095 (2.3764)	Acc@1 9.375 (9.360)Epoch: [0][100/196]	Time 0.545 (0.563)	Data 0.208 (0.214)	Loss 2.5526 (2.3724)	Acc@1 10.938 (9.700)

Epoch: [0][110/196]	Time 0.544 (0.561)	Data 0.209 (0.213)	Loss 2.3400 (2.3704)	Acc@1 14.062 (9.727)
Epoch: [0][110/196]	Time 0.547 (0.559)	Data 0.208 (0.211)	Loss 2.3988 (2.3753)	Acc@1 12.500 (9.530)
Epoch: [0][120/196]	Time 0.546 (0.557)	Data 0.209 (0.211)	Loss 2.3002 (2.3731)	Acc@1 14.062 (9.879)Epoch: [0][120/196]	Time 0.552 (0.560)	Data 0.213 (0.213)	Loss 2.3102 (2.3696)	Acc@1 12.500 (10.034)

Epoch: [0][130/196]	Time 0.543 (0.556)	Data 0.208 (0.211)	Loss 2.3260 (2.3757)	Acc@1 6.250 (9.709)
Epoch: [0][130/196]	Time 0.544 (0.558)	Data 0.207 (0.212)	Loss 2.2987 (2.3704)	Acc@1 10.938 (9.983)
Epoch: [0][140/196]	Time 0.542 (0.555)	Data 0.208 (0.211)	Loss 2.3840 (2.3769)	Acc@1 7.812 (9.719)Epoch: [0][140/196]	Time 0.541 (0.557)	Data 0.207 (0.212)	Loss 2.2602 (2.3743)	Acc@1 20.312 (9.973)

Epoch: [0][150/196]	Time 0.553 (0.555)	Data 0.218 (0.211)	Loss 2.4600 (2.3778)	Acc@1 7.812 (9.768)
Epoch: [0][150/196]	Time 0.549 (0.556)	Data 0.212 (0.212)	Loss 2.5257 (2.3783)	Acc@1 6.250 (9.923)
Epoch: [0][160/196]	Time 0.547 (0.556)	Data 0.212 (0.212)	Loss 2.4087 (2.3799)	Acc@1 3.125 (9.821)
Epoch: [0][160/196]	Time 0.546 (0.554)	Data 0.209 (0.210)	Loss 2.3476 (2.3769)	Acc@1 9.375 (9.812)
Epoch: [0][170/196]	Time 0.546 (0.555)	Data 0.207 (0.211)	Loss 2.3137 (2.3789)	Acc@1 10.938 (9.814)
Epoch: [0][170/196]	Time 0.547 (0.553)	Data 0.208 (0.210)	Loss 2.3098 (2.3760)	Acc@1 7.812 (9.795)
Epoch: [0][180/196]	Time 0.542 (0.554)	Data 0.207 (0.211)	Loss 2.4153 (2.3787)	Acc@1 14.062 (9.893)Epoch: [0][180/196]	Time 0.543 (0.553)	Data 0.208 (0.210)	Loss 2.3449 (2.3746)	Acc@1 10.938 (9.919)

Epoch: [0][190/196]	Time 0.538 (0.552)	Data 0.207 (0.210)	Loss 2.3076 (2.3739)	Acc@1 14.062 (9.948)Epoch: [0][190/196]	Time 0.541 (0.553)	Data 0.207 (0.211)	Loss 2.3598 (2.3775)	Acc@1 10.938 (9.874)

Test: [0/157]	Time 0.728 (0.728)	Data 0.448 (0.448)	Loss 2.3430 (2.3430)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.721 (0.721)	Data 0.440 (0.440)	Loss 2.3430 (2.3430)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.274 (0.314)	Data 0.006 (0.047)	Loss 2.3557 (2.3562)	Acc@1 6.250 (10.227)
Test: [10/157]	Time 0.272 (0.316)	Data 0.006 (0.048)	Loss 2.3557 (2.3562)	Acc@1 6.250 (10.227)
Test: [20/157]	Time 0.275 (0.296)	Data 0.010 (0.028)	Loss 2.3960 (2.3660)	Acc@1 9.375 (10.342)
Test: [20/157]	Time 0.273 (0.296)	Data 0.009 (0.029)	Loss 2.3960 (2.3660)	Acc@1 9.375 (10.342)
Test: [30/157]	Time 0.273 (0.288)	Data 0.008 (0.022)	Loss 2.3776 (2.3723)	Acc@1 9.375 (9.829)
Test: [30/157]	Time 0.273 (0.289)	Data 0.008 (0.022)	Loss 2.3776 (2.3723)	Acc@1 9.375 (9.829)
Test: [40/157]	Time 0.272 (0.284)	Data 0.008 (0.018)	Loss 2.3069 (2.3679)	Acc@1 12.500 (9.870)
Test: [40/157]	Time 0.272 (0.285)	Data 0.008 (0.018)	Loss 2.3069 (2.3679)	Acc@1 12.500 (9.870)
Test: [50/157]	Time 0.272 (0.282)	Data 0.008 (0.016)	Loss 2.3945 (2.3727)	Acc@1 9.375 (9.865)
Test: [50/157]	Time 0.272 (0.283)	Data 0.008 (0.016)	Loss 2.3945 (2.3727)	Acc@1 9.375 (9.865)
Test: [60/157]	Time 0.271 (0.281)	Data 0.007 (0.015)	Loss 2.4057 (2.3743)	Acc@1 7.812 (9.734)
Test: [60/157]	Time 0.271 (0.281)	Data 0.008 (0.015)	Loss 2.4057 (2.3743)	Acc@1 7.812 (9.734)
Test: [70/157]	Time 0.277 (0.280)	Data 0.009 (0.014)	Loss 2.3199 (2.3750)	Acc@1 7.812 (9.837)
Test: [70/157]	Time 0.272 (0.280)	Data 0.009 (0.014)	Loss 2.3199 (2.3750)	Acc@1 7.812 (9.837)
Test: [80/157]	Time 0.273 (0.279)	Data 0.007 (0.013)	Loss 2.3400 (2.3794)	Acc@1 10.938 (9.703)
Test: [80/157]	Time 0.273 (0.279)	Data 0.009 (0.013)	Loss 2.3400 (2.3794)	Acc@1 10.938 (9.703)
Test: [90/157]	Time 0.274 (0.278)	Data 0.009 (0.013)	Loss 2.4200 (2.3806)	Acc@1 12.500 (9.787)
Test: [90/157]	Time 0.273 (0.278)	Data 0.007 (0.013)	Loss 2.4200 (2.3806)	Acc@1 12.500 (9.787)
Test: [100/157]	Time 0.275 (0.278)	Data 0.009 (0.012)	Loss 2.3482 (2.3792)	Acc@1 12.500 (9.886)
Test: [100/157]	Time 0.273 (0.278)	Data 0.008 (0.012)	Loss 2.3482 (2.3792)	Acc@1 12.500 (9.886)
Test: [110/157]	Time 0.275 (0.277)	Data 0.009 (0.012)	Loss 2.3193 (2.3787)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.273 (0.277)	Data 0.008 (0.012)	Loss 2.3193 (2.3787)	Acc@1 9.375 (9.994)
Test: [120/157]	Time 0.274 (0.277)	Data 0.008 (0.012)	Loss 2.3263 (2.3804)	Acc@1 18.750 (9.969)
Test: [120/157]	Time 0.293 (0.277)	Data 0.018 (0.012)	Loss 2.3263 (2.3804)	Acc@1 18.750 (9.969)
Test: [130/157]	Time 0.272 (0.277)	Data 0.007 (0.011)	Loss 2.3830 (2.3810)	Acc@1 4.688 (9.900)
Test: [130/157]	Time 0.272 (0.277)	Data 0.007 (0.011)	Loss 2.3830 (2.3810)	Acc@1 4.688 (9.900)
Test: [140/157]	Time 0.277 (0.277)	Data 0.009 (0.011)	Loss 2.3125 (2.3802)	Acc@1 15.625 (9.885)
Test: [140/157]	Time 0.274 (0.277)	Data 0.007 (0.011)	Loss 2.3125 (2.3802)	Acc@1 15.625 (9.885)
Test: [150/157]	Time 0.271 (0.277)	Data 0.009 (0.011)	Loss 2.4512 (2.3807)	Acc@1 9.375 (9.882)
Test: [150/157]	Time 0.277 (0.277)	Data 0.008 (0.011)	Loss 2.4512 (2.3807)	Acc@1 9.375 (9.882)
New Best Found: 9.96%
New Best Found: 9.96%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run vague-leaf-1 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/svukeejf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184516-svukeejf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run ethereal-jazz-2 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/b72xg7c0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184517-b72xg7c0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 11647 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 11648) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:48:13
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 11648)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184841-taf3pszw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-spaceship-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/taf3pszw
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_184841-oyzykzfi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-fog-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/oyzykzfi
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.154 (2.154)	Data 0.437 (0.437)	Loss 2.2946 (2.2946)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.130 (2.130)	Data 0.391 (0.391)	Loss 2.2964 (2.2964)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.542 (0.683)	Data 0.211 (0.223)	Loss 2.3255 (2.3075)	Acc@1 3.125 (8.381)
Epoch: [0][10/196]	Time 0.546 (0.698)	Data 0.208 (0.238)	Loss 2.3268 (2.3080)	Acc@1 9.375 (11.222)
Epoch: [0][20/196]	Time 0.545 (0.624)	Data 0.211 (0.223)	Loss 2.3015 (2.3077)	Acc@1 20.312 (10.565)Epoch: [0][20/196]	Time 0.542 (0.616)	Data 0.209 (0.216)	Loss 2.2914 (2.3078)	Acc@1 15.625 (9.226)

Epoch: [0][30/196]	Time 0.544 (0.592)	Data 0.207 (0.214)	Loss 2.3049 (2.3059)	Acc@1 10.938 (9.173)Epoch: [0][30/196]	Time 0.545 (0.597)	Data 0.209 (0.218)	Loss 2.3028 (2.3073)	Acc@1 15.625 (10.534)

Epoch: [0][40/196]	Time 0.542 (0.593)	Data 0.207 (0.216)	Loss 2.3024 (2.3066)	Acc@1 6.250 (10.137)Epoch: [0][40/196]	Time 0.545 (0.589)	Data 0.210 (0.213)	Loss 2.3131 (2.3052)	Acc@1 7.812 (9.489)

Epoch: [0][50/196]	Time 0.545 (0.580)	Data 0.212 (0.212)	Loss 2.3001 (2.3047)	Acc@1 9.375 (9.957)
Epoch: [0][50/196]	Time 0.546 (0.583)	Data 0.210 (0.214)	Loss 2.3113 (2.3061)	Acc@1 7.812 (10.294)
Epoch: [0][60/196]	Time 0.548 (0.576)	Data 0.211 (0.214)	Loss 2.3066 (2.3058)	Acc@1 7.812 (9.939)Epoch: [0][60/196]	Time 0.544 (0.574)	Data 0.207 (0.212)	Loss 2.3100 (2.3048)	Acc@1 3.125 (9.708)

Epoch: [0][70/196]	Time 0.545 (0.569)	Data 0.213 (0.211)	Loss 2.3010 (2.3045)	Acc@1 7.812 (9.375)Epoch: [0][70/196]	Time 0.542 (0.572)	Data 0.208 (0.213)	Loss 2.3008 (2.3054)	Acc@1 12.500 (9.903)

Epoch: [0][80/196]	Time 0.541 (0.568)	Data 0.207 (0.212)	Loss 2.3025 (2.3053)	Acc@1 6.250 (9.857)Epoch: [0][80/196]	Time 0.543 (0.566)	Data 0.210 (0.211)	Loss 2.3007 (2.3045)	Acc@1 7.812 (9.317)

Epoch: [0][90/196]	Time 0.545 (0.565)	Data 0.212 (0.212)	Loss 2.3004 (2.3050)	Acc@1 6.250 (9.701)
Epoch: [0][90/196]	Time 0.545 (0.563)	Data 0.211 (0.211)	Loss 2.3013 (2.3043)	Acc@1 10.938 (9.289)
Epoch: [0][100/196]	Time 0.542 (0.563)	Data 0.206 (0.211)	Loss 2.2945 (2.3048)	Acc@1 14.062 (9.824)Epoch: [0][100/196]	Time 0.543 (0.561)	Data 0.209 (0.211)	Loss 2.3094 (2.3042)	Acc@1 3.125 (9.514)

Epoch: [0][110/196]	Time 0.542 (0.560)	Data 0.208 (0.210)	Loss 2.2996 (2.3041)	Acc@1 9.375 (9.530)
Epoch: [0][110/196]	Time 0.544 (0.561)	Data 0.207 (0.211)	Loss 2.3025 (2.3047)	Acc@1 10.938 (9.797)
Epoch: [0][120/196]	Time 0.546 (0.560)	Data 0.209 (0.211)	Loss 2.2988 (2.3046)	Acc@1 9.375 (9.917)Epoch: [0][120/196]	Time 0.546 (0.558)	Data 0.209 (0.210)	Loss 2.3042 (2.3039)	Acc@1 10.938 (9.620)

Epoch: [0][130/196]	Time 0.544 (0.557)	Data 0.209 (0.210)	Loss 2.3112 (2.3039)	Acc@1 3.125 (9.637)Epoch: [0][130/196]	Time 0.546 (0.558)	Data 0.212 (0.211)	Loss 2.3113 (2.3047)	Acc@1 10.938 (9.769)

Epoch: [0][140/196]	Time 0.543 (0.556)	Data 0.208 (0.210)	Loss 2.3027 (2.3039)	Acc@1 9.375 (9.641)
Epoch: [0][140/196]	Time 0.551 (0.557)	Data 0.213 (0.210)	Loss 2.2989 (2.3045)	Acc@1 10.938 (9.896)
Epoch: [0][150/196]	Time 0.542 (0.556)	Data 0.208 (0.210)	Loss 2.3036 (2.3044)	Acc@1 15.625 (9.872)
Epoch: [0][150/196]	Time 0.548 (0.555)	Data 0.213 (0.210)	Loss 2.3049 (2.3038)	Acc@1 9.375 (9.727)
Epoch: [0][160/196]	Time 0.545 (0.554)	Data 0.211 (0.210)	Loss 2.3041 (2.3038)	Acc@1 10.938 (9.831)
Epoch: [0][160/196]	Time 0.543 (0.556)	Data 0.208 (0.210)	Loss 2.3057 (2.3043)	Acc@1 3.125 (9.783)
Epoch: [0][170/196]	Time 0.546 (0.554)	Data 0.213 (0.210)	Loss 2.3076 (2.3038)	Acc@1 15.625 (9.914)
Epoch: [0][170/196]	Time 0.546 (0.555)	Data 0.213 (0.210)	Loss 2.3078 (2.3042)	Acc@1 10.938 (9.722)
Epoch: [0][180/196]	Time 0.539 (0.553)	Data 0.206 (0.210)	Loss 2.3034 (2.3038)	Acc@1 9.375 (9.936)
Epoch: [0][180/196]	Time 0.540 (0.554)	Data 0.207 (0.210)	Loss 2.3033 (2.3042)	Acc@1 10.938 (9.660)
Epoch: [0][190/196]	Time 0.543 (0.553)	Data 0.210 (0.210)	Loss 2.3003 (2.3037)	Acc@1 12.500 (10.062)
Epoch: [0][190/196]	Time 0.542 (0.554)	Data 0.209 (0.210)	Loss 2.3023 (2.3041)	Acc@1 10.938 (9.661)
Test: [0/157]	Time 0.867 (0.867)	Data 0.591 (0.591)	Loss 2.3003 (2.3003)	Acc@1 9.375 (9.375)Test: [0/157]	Time 0.865 (0.865)	Data 0.584 (0.584)	Loss 2.3003 (2.3003)	Acc@1 9.375 (9.375)

Test: [10/157]	Time 0.271 (0.328)	Data 0.006 (0.061)	Loss 2.3041 (2.3025)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.270 (0.328)	Data 0.005 (0.061)	Loss 2.3041 (2.3025)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.270 (0.301)	Data 0.007 (0.035)	Loss 2.3046 (2.3032)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.291 (0.304)	Data 0.013 (0.036)	Loss 2.3046 (2.3032)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.268 (0.292)	Data 0.006 (0.026)	Loss 2.3043 (2.3034)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.275 (0.294)	Data 0.008 (0.027)	Loss 2.3043 (2.3034)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.271 (0.287)	Data 0.007 (0.022)	Loss 2.3016 (2.3035)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.276 (0.289)	Data 0.008 (0.022)	Loss 2.3016 (2.3035)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.274 (0.284)	Data 0.008 (0.019)	Loss 2.2993 (2.3032)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.272 (0.286)	Data 0.007 (0.019)	Loss 2.2993 (2.3032)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.270 (0.283)	Data 0.006 (0.017)	Loss 2.3022 (2.3033)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.274 (0.284)	Data 0.007 (0.017)	Loss 2.3022 (2.3033)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.271 (0.281)	Data 0.007 (0.016)	Loss 2.3039 (2.3032)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.274 (0.283)	Data 0.008 (0.016)	Loss 2.3039 (2.3032)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.271 (0.280)	Data 0.006 (0.015)	Loss 2.3034 (2.3032)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.274 (0.282)	Data 0.008 (0.015)	Loss 2.3034 (2.3032)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.271 (0.279)	Data 0.008 (0.014)	Loss 2.2973 (2.3029)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.271 (0.281)	Data 0.007 (0.014)	Loss 2.2973 (2.3029)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.278 (0.278)	Data 0.009 (0.013)	Loss 2.3041 (2.3028)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.272 (0.281)	Data 0.007 (0.014)	Loss 2.3041 (2.3028)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.269 (0.278)	Data 0.006 (0.013)	Loss 2.3044 (2.3028)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.273 (0.281)	Data 0.008 (0.013)	Loss 2.3044 (2.3028)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.271 (0.278)	Data 0.007 (0.012)	Loss 2.2948 (2.3028)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.277 (0.280)	Data 0.008 (0.013)	Loss 2.2948 (2.3028)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.270 (0.277)	Data 0.007 (0.012)	Loss 2.3035 (2.3029)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.273 (0.280)	Data 0.007 (0.013)	Loss 2.3035 (2.3029)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.269 (0.277)	Data 0.006 (0.012)	Loss 2.3003 (2.3030)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.274 (0.279)	Data 0.008 (0.012)	Loss 2.3003 (2.3030)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.270 (0.276)	Data 0.006 (0.011)	Loss 2.3065 (2.3030)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.273 (0.279)	Data 0.008 (0.012)	Loss 2.3065 (2.3030)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.015 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run expert-spaceship-3 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/taf3pszw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184841-taf3pszw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run driven-fog-4 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/oyzykzfi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_184841-oyzykzfi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 13380 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 13379) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:51:41
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13379)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  Normalization : True 
True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_185213-5ukg68gp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-smoke-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/5ukg68gp
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_185213-j2951qob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-cosmos-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/j2951qob
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.565 (2.565)	Data 0.818 (0.818)	Loss 2.2964 (2.2964)	Acc@1 9.375 (9.375)
Epoch: [0][0/196]	Time 2.238 (2.238)	Data 0.498 (0.498)	Loss 2.2946 (2.2946)	Acc@1 14.062 (14.062)
Epoch: [0][10/196]	Time 0.547 (0.730)	Data 0.215 (0.266)	Loss 2.3255 (2.3075)	Acc@1 3.125 (8.381)
Epoch: [0][10/196]	Time 0.548 (0.696)	Data 0.214 (0.234)	Loss 2.3268 (2.3080)	Acc@1 9.375 (11.222)
Epoch: [0][20/196]	Time 0.545 (0.641)	Data 0.209 (0.238)	Loss 2.2914 (2.3078)	Acc@1 15.625 (9.226)Epoch: [0][20/196]	Time 0.543 (0.623)	Data 0.207 (0.221)	Loss 2.3015 (2.3077)	Acc@1 20.312 (10.565)

Epoch: [0][30/196]	Time 0.544 (0.610)	Data 0.210 (0.229)	Loss 2.3049 (2.3059)	Acc@1 10.938 (9.173)Epoch: [0][30/196]	Time 0.545 (0.598)	Data 0.210 (0.217)	Loss 2.3028 (2.3073)	Acc@1 15.625 (10.534)

Epoch: [0][40/196]	Time 0.551 (0.594)	Data 0.213 (0.224)	Loss 2.3131 (2.3052)	Acc@1 7.812 (9.489)
Epoch: [0][40/196]	Time 0.547 (0.584)	Data 0.208 (0.215)	Loss 2.3024 (2.3066)	Acc@1 6.250 (10.137)
Epoch: [0][50/196]	Time 0.550 (0.577)	Data 0.215 (0.214)	Loss 2.3113 (2.3061)	Acc@1 7.812 (10.294)Epoch: [0][50/196]	Time 0.546 (0.584)	Data 0.207 (0.221)	Loss 2.3001 (2.3047)	Acc@1 9.375 (9.957)

Epoch: [0][60/196]	Time 0.544 (0.571)	Data 0.208 (0.213)	Loss 2.3066 (2.3058)	Acc@1 7.812 (9.939)
Epoch: [0][60/196]	Time 0.548 (0.577)	Data 0.211 (0.219)	Loss 2.3100 (2.3048)	Acc@1 3.125 (9.708)
Epoch: [0][70/196]	Time 0.543 (0.567)	Data 0.207 (0.212)	Loss 2.3008 (2.3054)	Acc@1 12.500 (9.903)Epoch: [0][70/196]	Time 0.544 (0.573)	Data 0.207 (0.218)	Loss 2.3010 (2.3045)	Acc@1 7.812 (9.375)

Epoch: [0][80/196]	Time 0.548 (0.564)	Data 0.211 (0.212)	Loss 2.3025 (2.3053)	Acc@1 6.250 (9.857)
Epoch: [0][80/196]	Time 0.554 (0.569)	Data 0.217 (0.217)	Loss 2.3007 (2.3045)	Acc@1 7.812 (9.317)
Epoch: [0][90/196]	Time 0.546 (0.566)	Data 0.210 (0.216)	Loss 2.3013 (2.3043)	Acc@1 10.938 (9.289)
Epoch: [0][90/196]	Time 0.548 (0.562)	Data 0.212 (0.212)	Loss 2.3004 (2.3050)	Acc@1 6.250 (9.701)
Epoch: [0][100/196]	Time 0.560 (0.560)	Data 0.222 (0.211)	Loss 2.2945 (2.3048)	Acc@1 14.062 (9.824)
Epoch: [0][100/196]	Time 0.546 (0.564)	Data 0.206 (0.215)	Loss 2.3094 (2.3042)	Acc@1 3.125 (9.514)
Epoch: [0][110/196]	Time 0.546 (0.559)	Data 0.210 (0.211)	Loss 2.3025 (2.3047)	Acc@1 10.938 (9.797)
Epoch: [0][110/196]	Time 0.554 (0.562)	Data 0.213 (0.215)	Loss 2.2996 (2.3041)	Acc@1 9.375 (9.530)
Epoch: [0][120/196]	Time 0.556 (0.557)	Data 0.218 (0.211)	Loss 2.2988 (2.3046)	Acc@1 9.375 (9.917)
Epoch: [0][120/196]	Time 0.548 (0.561)	Data 0.209 (0.214)	Loss 2.3042 (2.3039)	Acc@1 10.938 (9.620)
Epoch: [0][130/196]	Time 0.545 (0.556)	Data 0.209 (0.211)	Loss 2.3113 (2.3047)	Acc@1 10.938 (9.769)
Epoch: [0][130/196]	Time 0.545 (0.559)	Data 0.208 (0.214)	Loss 2.3112 (2.3039)	Acc@1 3.125 (9.637)
Epoch: [0][140/196]	Time 0.540 (0.558)	Data 0.206 (0.213)	Loss 2.3027 (2.3039)	Acc@1 9.375 (9.641)
Epoch: [0][140/196]	Time 0.542 (0.555)	Data 0.209 (0.211)	Loss 2.2989 (2.3045)	Acc@1 10.938 (9.896)
Epoch: [0][150/196]	Time 0.546 (0.555)	Data 0.210 (0.211)	Loss 2.3036 (2.3044)	Acc@1 15.625 (9.872)Epoch: [0][150/196]	Time 0.557 (0.557)	Data 0.221 (0.213)	Loss 2.3049 (2.3038)	Acc@1 9.375 (9.727)

Epoch: [0][160/196]	Time 0.545 (0.554)	Data 0.210 (0.211)	Loss 2.3057 (2.3043)	Acc@1 3.125 (9.783)Epoch: [0][160/196]	Time 0.547 (0.557)	Data 0.212 (0.213)	Loss 2.3041 (2.3038)	Acc@1 10.938 (9.831)

Epoch: [0][170/196]	Time 0.548 (0.554)	Data 0.211 (0.211)	Loss 2.3078 (2.3042)	Acc@1 10.938 (9.722)
Epoch: [0][170/196]	Time 0.547 (0.556)	Data 0.208 (0.213)	Loss 2.3076 (2.3038)	Acc@1 15.625 (9.914)
Epoch: [0][180/196]	Time 0.545 (0.553)	Data 0.207 (0.211)	Loss 2.3033 (2.3042)	Acc@1 10.938 (9.660)Epoch: [0][180/196]	Time 0.551 (0.555)	Data 0.218 (0.213)	Loss 2.3034 (2.3038)	Acc@1 9.375 (9.936)

Epoch: [0][190/196]	Time 0.547 (0.555)	Data 0.209 (0.213)	Loss 2.3003 (2.3037)	Acc@1 12.500 (10.062)
Epoch: [0][190/196]	Time 0.553 (0.553)	Data 0.213 (0.211)	Loss 2.3023 (2.3041)	Acc@1 10.938 (9.661)
Test: [0/157]	Time 0.702 (0.702)	Data 0.421 (0.421)	Loss 2.3003 (2.3003)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.701 (0.701)	Data 0.420 (0.420)	Loss 2.3003 (2.3003)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.272 (0.315)	Data 0.008 (0.047)	Loss 2.3041 (2.3025)	Acc@1 7.812 (10.369)
Test: [10/157]	Time 0.273 (0.315)	Data 0.008 (0.047)	Loss 2.3041 (2.3025)	Acc@1 7.812 (10.369)
Test: [20/157]	Time 0.274 (0.295)	Data 0.008 (0.028)	Loss 2.3046 (2.3032)	Acc@1 9.375 (10.417)
Test: [20/157]	Time 0.274 (0.295)	Data 0.007 (0.028)	Loss 2.3046 (2.3032)	Acc@1 9.375 (10.417)
Test: [30/157]	Time 0.272 (0.287)	Data 0.007 (0.021)	Loss 2.3043 (2.3034)	Acc@1 9.375 (9.879)
Test: [30/157]	Time 0.275 (0.289)	Data 0.007 (0.021)	Loss 2.3043 (2.3034)	Acc@1 9.375 (9.879)
Test: [40/157]	Time 0.271 (0.284)	Data 0.007 (0.018)	Loss 2.3016 (2.3035)	Acc@1 12.500 (9.985)
Test: [40/157]	Time 0.273 (0.285)	Data 0.008 (0.018)	Loss 2.3016 (2.3035)	Acc@1 12.500 (9.985)
Test: [50/157]	Time 0.272 (0.281)	Data 0.007 (0.016)	Loss 2.2993 (2.3032)	Acc@1 9.375 (9.957)
Test: [50/157]	Time 0.274 (0.282)	Data 0.006 (0.016)	Loss 2.2993 (2.3032)	Acc@1 9.375 (9.957)
Test: [60/157]	Time 0.275 (0.280)	Data 0.008 (0.015)	Loss 2.3022 (2.3033)	Acc@1 7.812 (9.810)
Test: [60/157]	Time 0.276 (0.281)	Data 0.008 (0.014)	Loss 2.3022 (2.3033)	Acc@1 7.812 (9.810)
Test: [70/157]	Time 0.271 (0.279)	Data 0.008 (0.014)	Loss 2.3039 (2.3032)	Acc@1 7.812 (9.925)
Test: [70/157]	Time 0.272 (0.280)	Data 0.007 (0.013)	Loss 2.3039 (2.3032)	Acc@1 7.812 (9.925)
Test: [80/157]	Time 0.272 (0.278)	Data 0.007 (0.013)	Loss 2.3034 (2.3032)	Acc@1 10.938 (9.780)
Test: [80/157]	Time 0.270 (0.279)	Data 0.007 (0.013)	Loss 2.3034 (2.3032)	Acc@1 10.938 (9.780)
Test: [90/157]	Time 0.270 (0.277)	Data 0.007 (0.012)	Loss 2.2973 (2.3029)	Acc@1 12.500 (9.873)
Test: [90/157]	Time 0.270 (0.278)	Data 0.007 (0.012)	Loss 2.2973 (2.3029)	Acc@1 12.500 (9.873)
Test: [100/157]	Time 0.271 (0.277)	Data 0.007 (0.012)	Loss 2.3041 (2.3028)	Acc@1 12.500 (9.963)
Test: [100/157]	Time 0.273 (0.278)	Data 0.006 (0.011)	Loss 2.3041 (2.3028)	Acc@1 12.500 (9.963)
Test: [110/157]	Time 0.272 (0.276)	Data 0.008 (0.011)	Loss 2.3044 (2.3028)	Acc@1 7.812 (10.051)
Test: [110/157]	Time 0.274 (0.277)	Data 0.007 (0.011)	Loss 2.3044 (2.3028)	Acc@1 7.812 (10.051)
Test: [120/157]	Time 0.271 (0.276)	Data 0.008 (0.011)	Loss 2.2948 (2.3028)	Acc@1 18.750 (10.021)
Test: [120/157]	Time 0.272 (0.277)	Data 0.006 (0.011)	Loss 2.2948 (2.3028)	Acc@1 18.750 (10.021)
Test: [130/157]	Time 0.273 (0.276)	Data 0.008 (0.011)	Loss 2.3035 (2.3029)	Acc@1 4.688 (9.948)
Test: [130/157]	Time 0.273 (0.276)	Data 0.008 (0.010)	Loss 2.3035 (2.3029)	Acc@1 4.688 (9.948)
Test: [140/157]	Time 0.272 (0.275)	Data 0.008 (0.011)	Loss 2.3003 (2.3030)	Acc@1 15.625 (9.929)
Test: [140/157]	Time 0.273 (0.276)	Data 0.007 (0.010)	Loss 2.3003 (2.3030)	Acc@1 15.625 (9.929)
Test: [150/157]	Time 0.273 (0.275)	Data 0.007 (0.010)	Loss 2.3065 (2.3030)	Acc@1 9.375 (9.923)
Test: [150/157]	Time 0.272 (0.276)	Data 0.006 (0.010)	Loss 2.3065 (2.3030)	Acc@1 9.375 (9.923)
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 10.0%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.009 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run fancy-cosmos-6 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/j2951qob
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_185213-j2951qob/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run wise-smoke-5 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/5ukg68gp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_185213-5ukg68gp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 15106 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 15107) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:55:10
  host      : gpu008.iitmandi.ac.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 15107)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Normalization :  True
Normalization :  True
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: b21234. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_185539-1bvkn8be
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-pond-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/1bvkn8be
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/ravihm.scee.iitmandi/Certified-Robustness/wandb/run-20240704_185539-xbdh7jj9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-haze-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: üöÄ View run at https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/xbdh7jj9
Training 0.07544305763498814% of the parameters
starting training
Training 0.07544305763498814% of the parameters
starting training
Epoch: [0][0/196]	Time 2.181 (2.181)	Data 0.439 (0.439)	Loss 2.2946 (2.2946)	Acc@1 14.062 (14.062)
Epoch: [0][0/196]	Time 2.401 (2.401)	Data 0.683 (0.683)	Loss 2.2964 (2.2964)	Acc@1 9.375 (9.375)
Epoch: [0][10/196]	Time 0.539 (0.708)	Data 0.207 (0.250)	Loss 2.2867 (2.3281)	Acc@1 14.062 (9.091)
Epoch: [0][10/196]	Time 0.540 (0.691)	Data 0.204 (0.229)	Loss 2.3543 (2.3239)	Acc@1 12.500 (10.511)
Epoch: [0][20/196]	Time 0.542 (0.621)	Data 0.207 (0.220)	Loss 2.3459 (2.3479)	Acc@1 9.375 (10.417)
Epoch: [0][20/196]	Time 0.542 (0.631)	Data 0.207 (0.231)	Loss 2.3345 (2.3534)	Acc@1 4.688 (9.226)
Epoch: [0][30/196]	Time 0.540 (0.603)	Data 0.209 (0.224)	Loss 2.3518 (2.3669)	Acc@1 7.812 (8.821)Epoch: [0][30/196]	Time 0.541 (0.596)	Data 0.208 (0.217)	Loss 2.3809 (2.3564)	Acc@1 10.938 (10.887)

Epoch: [0][40/196]	Time 0.542 (0.588)	Data 0.207 (0.220)	Loss 2.4405 (2.3689)	Acc@1 4.688 (8.765)
Epoch: [0][40/196]	Time 0.546 (0.583)	Data 0.211 (0.215)	Loss 2.3750 (2.3567)	Acc@1 6.250 (10.366)
Epoch: [0][50/196]	Time 0.542 (0.579)	Data 0.210 (0.218)	Loss 2.3949 (2.3696)	Acc@1 9.375 (8.640)
Epoch: [0][50/196]	Time 0.544 (0.576)	Data 0.211 (0.214)	Loss 2.4310 (2.3571)	Acc@1 7.812 (10.049)
Epoch: [0][60/196]	Time 0.541 (0.570)	Data 0.205 (0.213)	Loss 2.3826 (2.3625)	Acc@1 7.812 (10.015)
Epoch: [0][60/196]	Time 0.552 (0.574)	Data 0.214 (0.217)	Loss 2.4033 (2.3767)	Acc@1 9.375 (8.991)
Epoch: [0][70/196]	Time 0.547 (0.569)	Data 0.213 (0.216)	Loss 2.3150 (2.3749)	Acc@1 4.688 (9.045)
Epoch: [0][70/196]	Time 0.543 (0.567)	Data 0.209 (0.213)	Loss 2.4304 (2.3674)	Acc@1 9.375 (9.837)
Epoch: [0][80/196]	Time 0.551 (0.566)	Data 0.217 (0.215)	Loss 2.3463 (2.3748)	Acc@1 14.062 (9.375)
Epoch: [0][80/196]	Time 0.547 (0.564)	Data 0.209 (0.212)	Loss 2.3289 (2.3684)	Acc@1 14.062 (9.645)
Epoch: [0][90/196]	Time 0.550 (0.562)	Data 0.218 (0.212)	Loss 2.3779 (2.3698)	Acc@1 10.938 (9.684)
Epoch: [0][90/196]	Time 0.545 (0.564)	Data 0.209 (0.215)	Loss 2.4122 (2.3752)	Acc@1 15.625 (9.461)
Epoch: [0][100/196]	Time 0.544 (0.560)	Data 0.210 (0.212)	Loss 2.5526 (2.3724)	Acc@1 10.938 (9.700)
Epoch: [0][100/196]	Time 0.546 (0.562)	Data 0.212 (0.214)	Loss 2.5095 (2.3764)	Acc@1 9.375 (9.360)
Epoch: [0][110/196]	Time 0.540 (0.560)	Data 0.207 (0.214)	Loss 2.3988 (2.3753)	Acc@1 12.500 (9.530)
Epoch: [0][110/196]	Time 0.542 (0.559)	Data 0.205 (0.212)	Loss 2.3400 (2.3704)	Acc@1 14.062 (9.727)
Epoch: [0][120/196]	Time 0.539 (0.557)	Data 0.206 (0.211)	Loss 2.3102 (2.3696)	Acc@1 12.500 (10.034)
Epoch: [0][120/196]	Time 0.543 (0.559)	Data 0.207 (0.213)	Loss 2.3002 (2.3731)	Acc@1 14.062 (9.879)
Epoch: [0][130/196]	Time 0.540 (0.556)	Data 0.209 (0.211)	Loss 2.2987 (2.3704)	Acc@1 10.938 (9.983)
Epoch: [0][130/196]	Time 0.545 (0.558)	Data 0.207 (0.213)	Loss 2.3260 (2.3757)	Acc@1 6.250 (9.709)
Epoch: [0][140/196]	Time 0.542 (0.555)	Data 0.208 (0.211)	Loss 2.2602 (2.3743)	Acc@1 20.312 (9.973)
Epoch: [0][140/196]	Time 0.542 (0.557)	Data 0.208 (0.213)	Loss 2.3840 (2.3769)	Acc@1 7.812 (9.719)
Epoch: [0][150/196]	Time 0.541 (0.556)	Data 0.209 (0.212)	Loss 2.4600 (2.3778)	Acc@1 7.812 (9.768)
Epoch: [0][150/196]	Time 0.543 (0.554)	Data 0.208 (0.211)	Loss 2.5257 (2.3783)	Acc@1 6.250 (9.923)
Epoch: [0][160/196]	Time 0.540 (0.555)	Data 0.209 (0.212)	Loss 2.3476 (2.3769)	Acc@1 9.375 (9.812)
Epoch: [0][160/196]	Time 0.545 (0.554)	Data 0.209 (0.211)	Loss 2.4087 (2.3799)	Acc@1 3.125 (9.821)
Epoch: [0][170/196]	Time 0.542 (0.554)	Data 0.207 (0.212)	Loss 2.3098 (2.3760)	Acc@1 7.812 (9.795)
Epoch: [0][170/196]	Time 0.553 (0.553)	Data 0.212 (0.210)	Loss 2.3137 (2.3789)	Acc@1 10.938 (9.814)
Epoch: [0][180/196]	Time 0.537 (0.554)	Data 0.206 (0.212)	Loss 2.3449 (2.3746)	Acc@1 10.938 (9.919)
Epoch: [0][180/196]	Time 0.541 (0.553)	Data 0.204 (0.210)	Loss 2.4153 (2.3787)	Acc@1 14.062 (9.893)
Epoch: [0][190/196]	Time 0.540 (0.553)	Data 0.209 (0.212)	Loss 2.3076 (2.3739)	Acc@1 14.062 (9.948)
Epoch: [0][190/196]	Time 0.542 (0.552)	Data 0.208 (0.210)	Loss 2.3598 (2.3775)	Acc@1 10.938 (9.874)
Test: [0/157]	Time 0.715 (0.715)	Data 0.438 (0.438)	Loss 2.3430 (2.3430)	Acc@1 9.375 (9.375)
Test: [0/157]	Time 0.714 (0.714)	Data 0.437 (0.437)	Loss 2.3430 (2.3430)	Acc@1 9.375 (9.375)
Test: [10/157]	Time 0.270 (0.312)	Data 0.006 (0.046)	Loss 2.3557 (2.3562)	Acc@1 6.250 (10.227)
Test: [10/157]	Time 0.272 (0.315)	Data 0.005 (0.048)	Loss 2.3557 (2.3562)	Acc@1 6.250 (10.227)
Test: [20/157]	Time 0.272 (0.292)	Data 0.007 (0.027)	Loss 2.3960 (2.3660)	Acc@1 9.375 (10.342)
Test: [20/157]	Time 0.273 (0.294)	Data 0.007 (0.028)	Loss 2.3960 (2.3660)	Acc@1 9.375 (10.342)
Test: [30/157]	Time 0.268 (0.286)	Data 0.007 (0.021)	Loss 2.3776 (2.3723)	Acc@1 9.375 (9.829)
Test: [30/157]	Time 0.270 (0.286)	Data 0.006 (0.021)	Loss 2.3776 (2.3723)	Acc@1 9.375 (9.829)
Test: [40/157]	Time 0.271 (0.283)	Data 0.008 (0.018)	Loss 2.3069 (2.3679)	Acc@1 12.500 (9.870)
Test: [40/157]	Time 0.275 (0.283)	Data 0.009 (0.018)	Loss 2.3069 (2.3679)	Acc@1 12.500 (9.870)
Test: [50/157]	Time 0.267 (0.280)	Data 0.007 (0.016)	Loss 2.3945 (2.3727)	Acc@1 9.375 (9.865)
Test: [50/157]	Time 0.274 (0.281)	Data 0.008 (0.016)	Loss 2.3945 (2.3727)	Acc@1 9.375 (9.865)
Test: [60/157]	Time 0.269 (0.278)	Data 0.007 (0.014)	Loss 2.4057 (2.3743)	Acc@1 7.812 (9.734)
Test: [60/157]	Time 0.270 (0.279)	Data 0.008 (0.015)	Loss 2.4057 (2.3743)	Acc@1 7.812 (9.734)
Test: [70/157]	Time 0.271 (0.277)	Data 0.007 (0.013)	Loss 2.3199 (2.3750)	Acc@1 7.812 (9.837)
Test: [70/157]	Time 0.273 (0.278)	Data 0.007 (0.014)	Loss 2.3199 (2.3750)	Acc@1 7.812 (9.837)
Test: [80/157]	Time 0.271 (0.277)	Data 0.006 (0.013)	Loss 2.3400 (2.3794)	Acc@1 10.938 (9.703)
Test: [80/157]	Time 0.271 (0.278)	Data 0.007 (0.013)	Loss 2.3400 (2.3794)	Acc@1 10.938 (9.703)
Test: [90/157]	Time 0.272 (0.276)	Data 0.007 (0.012)	Loss 2.4200 (2.3806)	Acc@1 12.500 (9.787)
Test: [90/157]	Time 0.271 (0.277)	Data 0.007 (0.012)	Loss 2.4200 (2.3806)	Acc@1 12.500 (9.787)
Test: [100/157]	Time 0.269 (0.276)	Data 0.007 (0.012)	Loss 2.3482 (2.3792)	Acc@1 12.500 (9.886)
Test: [100/157]	Time 0.272 (0.276)	Data 0.008 (0.012)	Loss 2.3482 (2.3792)	Acc@1 12.500 (9.886)
Test: [110/157]	Time 0.270 (0.275)	Data 0.007 (0.011)	Loss 2.3193 (2.3787)	Acc@1 9.375 (9.994)
Test: [110/157]	Time 0.269 (0.276)	Data 0.007 (0.011)	Loss 2.3193 (2.3787)	Acc@1 9.375 (9.994)
Test: [120/157]	Time 0.270 (0.275)	Data 0.007 (0.011)	Loss 2.3263 (2.3804)	Acc@1 18.750 (9.969)
Test: [120/157]	Time 0.272 (0.276)	Data 0.007 (0.011)	Loss 2.3263 (2.3804)	Acc@1 18.750 (9.969)
Test: [130/157]	Time 0.270 (0.275)	Data 0.007 (0.011)	Loss 2.3830 (2.3810)	Acc@1 4.688 (9.900)
Test: [130/157]	Time 0.273 (0.275)	Data 0.008 (0.011)	Loss 2.3830 (2.3810)	Acc@1 4.688 (9.900)
Test: [140/157]	Time 0.272 (0.274)	Data 0.007 (0.010)	Loss 2.3125 (2.3802)	Acc@1 15.625 (9.885)
Test: [140/157]	Time 0.272 (0.275)	Data 0.008 (0.010)	Loss 2.3125 (2.3802)	Acc@1 15.625 (9.885)
Test: [150/157]	Time 0.271 (0.274)	Data 0.007 (0.010)	Loss 2.4512 (2.3807)	Acc@1 9.375 (9.882)
Test: [150/157]	Time 0.275 (0.275)	Data 0.007 (0.010)	Loss 2.4512 (2.3807)	Acc@1 9.375 (9.882)
New Best Found: 9.96%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
New Best Found: 9.96%
Traceback (most recent call last):
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 633, in <module>
    main()
  File "/home/ravihm.scee.iitmandi/Certified-Robustness/train_denoiser_adapter.py", line 383, in main
    model.save_adapter( args.outdir+folder+"/"+str(args.noise_sd), "denoising-adapter")
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DistributedDataParallel' object has no attribute 'save_adapter'
wandb: - 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.008 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run vague-pond-7 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/1bvkn8be
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_185539-1bvkn8be/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: üöÄ View run glowing-haze-8 at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert/runs/xbdh7jj9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/b21234/vit_cifar10_compacter_0.5_1.0_fourier_attention_invert
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240704_185539-xbdh7jj9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16911 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16910) of binary: /scratch/ravihm.scee.iitmandi/pytorch/bin/python
Traceback (most recent call last):
  File "/scratch/ravihm.scee.iitmandi/pytorch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/ravihm.scee.iitmandi/pytorch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_denoiser_adapter.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-04_18:58:36
  host      : gpu008.iitmandi.ac.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 16910)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
